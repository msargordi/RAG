{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RtawaERNORl"
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_fireworks langchain_community beautifulsoup4 google-search-results chromadb langchainhub sentence-transformers langchain-chroma gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4749,
     "status": "ok",
     "timestamp": 1722689116499,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "s5FdjAFiNKGx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_fireworks import FireworksEmbeddings, ChatFireworks\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1722689079085,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "9aK_SqnCNZEW",
    "outputId": "6c661acd-422c-4f15-a6f1-e876a52a1c92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up API clients\n",
    "os.environ['FIREWORKS_API_KEY'] = 'API_KEY'\n",
    "os.environ[\"SERPER_API_KEY\"] = 'API_KEY'\n",
    "\n",
    "\n",
    "# Download NLTK data for sentence tokenization\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101845,
     "status": "ok",
     "timestamp": 1722636622581,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "Z6fg_wjVFcrK",
    "outputId": "6eaf3c70-cc1d-4bdf-df86-12d612cd37c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: what are the best dogs?\n",
      "Generated hypothetical document:\n",
      "**The Ultimate Guide to the Best Dog Breeds for Every Lifestyle**\n",
      "\n",
      "Are you thinking of bringing a new furry friend into your family? With over 340 recognized breeds, choosing the right dog can be a da...\n",
      "Expanded queries: ['what are the best dogs?', 'What are the most popular dog breeds for families with small children?', 'Which dog breeds are best suited for apartment living due to their low energy levels?', 'What are the top dog breeds for people with allergies, considering hypoallergenic characteristics?']\n",
      "Scraped https://www.akc.org/expert-advice/dog-breeds/best-family-dogs/: 20832 characters\n",
      "Scraped https://www.forbes.com/advisor/pet-insurance/pet-care/popular-dog-breeds/: 43632 characters\n",
      "Error scraping https://be.chewy.com/best-family-dogs/: HTTPSConnectionPool(host='be.chewy.com', port=443): Read timed out. (read timeout=10)\n",
      "Scraped https://be.chewy.com/best-family-dogs/: 0 characters\n",
      "Content length for query 'what are the best dogs?': 64466 characters\n",
      "Scraped https://www.akc.org/dog-breeds/best-dogs-for-kids/: 28002 characters\n",
      "Error scraping https://be.chewy.com/best-family-dogs/: HTTPSConnectionPool(host='be.chewy.com', port=443): Read timed out. (read timeout=10)\n",
      "Scraped https://be.chewy.com/best-family-dogs/: 0 characters\n",
      "Scraped https://www.akc.org/expert-advice/dog-breeds/best-family-dogs/: 20832 characters\n",
      "Content length for query 'What are the most popular dog breeds for families with small children?': 48836 characters\n",
      "Scraped https://www.reddit.com/r/dogs/comments/43hwu7/breeds_low_energy_apartment_dog_recommendations/: 221 characters\n",
      "Scraped https://www.rover.com/blog/lazy-dogs/: 58 characters\n",
      "Scraped https://www.apartmentlist.com/renter-life/top-10-apartment-friendly-dog-breeds: 147 characters\n",
      "Content length for query 'Which dog breeds are best suited for apartment living due to their low energy levels?': 428 characters\n",
      "Scraped https://www.akc.org/expert-advice/dog-breeds/hypoallergenic-dog-breeds/: 23378 characters\n",
      "Scraped https://health.clevelandclinic.org/are-any-dogs-hypoallergenic: 9041 characters\n",
      "Scraped https://www.everydayhealth.com/allergy-pictures/best-and-worst-dog-breeds-for-people-with-allergies.aspx: 13680 characters\n",
      "Content length for query 'What are the top dog breeds for people with allergies, considering hypoallergenic characteristics?': 46101 characters\n",
      "Combined text length: 159834 characters\n",
      "Number of sentence windows: 773\n",
      "Number of index documents: 773\n",
      "Number of document chunks: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 60 documents using HyDE\n",
      "Number of unique documents: 39\n",
      "Number of retrieved and reranked documents: 10\n",
      "Retrieved document 1: Back To Dog Breeds Best Family Dogs: Which Breed Is Right for You? By AKC Staff Updated: May 07, 2024 | 3 Minutes Updated: May 07, 2024 | 3 Minutes acquiring a puppy kids family children When adding a...\n",
      "Retrieved document 2: When choosing a dog for your family, consider your lifestyle and the ages of your children. Some breeds have infinite patience, others can play endlessly and still others are naturally protective of c...\n",
      "Retrieved document 3: We’ve compiled a list of dog breeds to help you find the perfect fit for you and your family: Labrador Retriever ©WavebreakMediaMicro - stock.adobe.com The Labrador Retriever is one of the most popula...\n",
      "Retrieved document 4: They’re strong, loyal, affectionate, responsive and fast. A Collie would be best suited for an active family as they’re high-energy and love to move around. While they need daily exercise, they’re als...\n",
      "Retrieved document 5: Best Family Dogs: Which Breed Is Right for You? – American Kennel Club Event Search Find a Puppy Register Shop AKC TV AKC Rx Sign In Breeds A-Z Expert Advice Products & Services Sports & Events Clubs ...\n",
      "Retrieved document 6: Some breeds have infinite patience, others can play endlessly and still others are naturally protective of children. While most dogs thrive on training … Dogs can be great for kids: Not only are they ...\n",
      "Retrieved document 7: For example, some breeds are better suited for suburban and farm families than households living in the city. When adding any dog to your family, it’s also important that everyone is included in the d...\n",
      "Retrieved document 8: When choosing a dog for your family, consider your lifestyle and the ages of your children. Some breeds have infinite patience, others can play endlessly and still others are naturally protective of c...\n",
      "Retrieved document 9: By AKC Staff Updated: May 07, 2024 | 3 Minutes Updated: May 07, 2024 | 3 Minutes acquiring a puppy kids family children When adding a new puppy or dog to your family, it can be difficult to decide whi...\n",
      "Retrieved document 10: They get along well with children and other dogs and are a great pick for new families. Collie © 2014 Charles Mann via Getty Images Collies are legendary for their herding skills. They’re strong, loya...\n",
      "Answer: The best dogs are a matter of personal preference and lifestyle. However, some popular breeds that are often considered great family dogs include Labrador Retrievers, Collies, and Australian Terriers. Ultimately, the best dog for you will depend on your individual circumstances and what you're looking for in a pet.\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "search = GoogleSerperAPIWrapper(k=3)\n",
    "embeddings = FireworksEmbeddings(model=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "def scrape_webpage(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text[:50000], len(text[:50000])\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\", 0\n",
    "\n",
    "def search_and_scrape(query):\n",
    "    search_results = search.results(query)\n",
    "    scraped_urls = set()\n",
    "    full_texts = []\n",
    "    if 'organic' in search_results:\n",
    "        for result in search_results['organic']:\n",
    "            url = result.get('link')\n",
    "            if url and url not in scraped_urls and len(full_texts) < 3:\n",
    "                text, length = scrape_webpage(url)\n",
    "                print(f\"Scraped {url}: {length} characters\")\n",
    "                full_texts.append(text)\n",
    "                scraped_urls.add(url)\n",
    "    return \" \".join(full_texts)\n",
    "\n",
    "def query_expansion(query):\n",
    "    expansion_prompt = f\"\"\"\n",
    "    Given the following search query, generate 3 additional related queries that could help find more comprehensive information on the topic. The queries should be different from each other and explore various aspects of the main query. Provide only the additional queries, numbered 1-3.\n",
    "\n",
    "    Main query: {query}\n",
    "\n",
    "    Additional queries:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(expansion_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    expanded_queries = [query]\n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.strip() and line[0].isdigit():\n",
    "            expanded_queries.append(line.split('. ', 1)[1].strip())\n",
    "\n",
    "    return expanded_queries[:4]  # Limit to 4 queries (original + 3 expansions)\n",
    "\n",
    "def create_sentence_windows(text, window_size=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    windows = []\n",
    "    for i in range(len(sentences)):\n",
    "        window = \" \".join(sentences[max(0, i-window_size):min(len(sentences), i+window_size+1)])\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def chunk_documents(documents, chunk_size=250):\n",
    "    chunked_docs = []\n",
    "    for i in range(0, len(documents), chunk_size):\n",
    "        chunk = documents[i:i+chunk_size]\n",
    "        chunked_docs.append(chunk)\n",
    "    return chunked_docs\n",
    "\n",
    "def generate_hypothetical_document(query):\n",
    "    hyde_prompt = f\"\"\"\n",
    "    Given the search query below, generate a hypothetical document that would be a perfect match for this query. The document should be detailed and contain relevant information that directly addresses the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Document:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def llm_rerank(query, documents, k=5):\n",
    "    rerank_prompt = \"\"\"\n",
    "    Given the following query and a list of document excerpts, rank the documents based on their relevance to the query. Provide the rankings as a list of numbers from 1 to {}, where 1 is the most relevant.\n",
    "\n",
    "    Query: {}\n",
    "\n",
    "    Documents:\n",
    "    {}\n",
    "\n",
    "    Rankings (1 to {}):\n",
    "    \"\"\".format(len(documents), query, \"\\n\".join([f\"{i+1}. {doc.page_content[:200]}...\" for i, doc in enumerate(documents)]), len(documents))\n",
    "\n",
    "    response = llm.invoke(rerank_prompt)\n",
    "    rankings = [int(x) for x in response.content.split() if x.isdigit()]\n",
    "\n",
    "    # Sort documents based on rankings and return top k\n",
    "    return sorted(zip(documents, rankings), key=lambda x: x[1])[:k]\n",
    "\n",
    "def process_query(query):\n",
    "    try:\n",
    "        # Generate hypothetical document\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        print(\"Generated hypothetical document:\")\n",
    "        print(hypothetical_doc[:200] + \"...\")  # Print first 200 characters\n",
    "\n",
    "        # Embed the hypothetical document\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "\n",
    "        expanded_queries = query_expansion(query)\n",
    "        print(\"Expanded queries:\", expanded_queries)\n",
    "\n",
    "        all_texts = []\n",
    "        for expanded_query in expanded_queries:\n",
    "            content = search_and_scrape(expanded_query)\n",
    "            all_texts.append(content)\n",
    "            print(f\"Content length for query '{expanded_query}': {len(content)} characters\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        sentence_windows = create_sentence_windows(combined_text)\n",
    "        print(f\"Number of sentence windows: {len(sentence_windows)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=window, metadata={\"window\": window}) for window in sentence_windows]\n",
    "        print(f\"Number of index documents: {len(index_documents)}\")\n",
    "\n",
    "        chunked_docs = chunk_documents(index_documents)\n",
    "        print(f\"Number of document chunks: {len(chunked_docs)}\")\n",
    "\n",
    "        vectorstore = Chroma(embedding_function=embeddings, collection_name=\"rag-chroma\")\n",
    "\n",
    "        for i, chunk in enumerate(chunked_docs):\n",
    "            vectorstore.add_documents(chunk)\n",
    "            # print(f\"Processed chunk {i+1}/{len(chunked_docs)}\")\n",
    "\n",
    "        def get_hyde_retriever(vectorstore, hyde_embedding):\n",
    "            base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 60})  # Increase k to account for potential duplicates\n",
    "\n",
    "            def retriever(query):\n",
    "                # Use HyDE embedding for retrieval\n",
    "                docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=60)\n",
    "                print(f\"Retrieved {len(docs)} documents using HyDE\")\n",
    "\n",
    "                # Remove duplicates\n",
    "                unique_docs = []\n",
    "                seen_content = set()\n",
    "                for doc in docs:\n",
    "                    content = doc.metadata.get(\"window\", doc.page_content)\n",
    "                    if content not in seen_content:\n",
    "                        unique_docs.append(Document(page_content=content))\n",
    "                        seen_content.add(content)\n",
    "\n",
    "                print(f\"Number of unique documents: {len(unique_docs)}\")\n",
    "\n",
    "                # Apply LLM reranking\n",
    "                reranked_docs = llm_rerank(query, unique_docs, k=10)\n",
    "\n",
    "                return [doc for doc, _ in reranked_docs]\n",
    "\n",
    "            return retriever\n",
    "\n",
    "        retriever = get_hyde_retriever(vectorstore, hyde_embedding)\n",
    "\n",
    "        retrieved_docs = retriever(query)\n",
    "        print(f\"Number of retrieved and reranked documents: {len(retrieved_docs)}\")\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            print(f\"Retrieved document {i + 1}: {doc.page_content[:200]}...\")\n",
    "\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        prompt_template = \"\"\"\n",
    "        Use the following context to answer the question. If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "        answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "        # print(f\"Raw answer from LLM: {answer}\")\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\"\n",
    "\n",
    "def main():\n",
    "    query = input(\"Enter your query: \")\n",
    "    answer = process_query(query)\n",
    "    print(\"Answer:\", answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPBK3iJqGsND"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize components\n",
    "search = GoogleSerperAPIWrapper(k=3)\n",
    "embeddings = FireworksEmbeddings(model=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "def scrape_webpage(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text[:50000], len(text[:50000])\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\", 0\n",
    "\n",
    "def search_and_scrape(query):\n",
    "    search_results = search.results(query)\n",
    "    scraped_urls = set()\n",
    "    full_texts = []\n",
    "    if 'organic' in search_results:\n",
    "        for result in search_results['organic']:\n",
    "            url = result.get('link')\n",
    "            if url and url not in scraped_urls and len(full_texts) < 3:\n",
    "                text, length = scrape_webpage(url)\n",
    "                print(f\"Scraped {url}: {length} characters\")\n",
    "                full_texts.append(text)\n",
    "                scraped_urls.add(url)\n",
    "    return \" \".join(full_texts)\n",
    "\n",
    "def query_expansion(query):\n",
    "    expansion_prompt = f\"\"\"\n",
    "    Given the following search query, generate 3 additional related queries that could help find more comprehensive information on the topic. The queries should be different from each other and explore various aspects of the main query. Provide only the additional queries, numbered 1-3.\n",
    "\n",
    "    Main query: {query}\n",
    "\n",
    "    Additional queries:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(expansion_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    expanded_queries = [query]\n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.strip() and line[0].isdigit():\n",
    "            expanded_queries.append(line.split('. ', 1)[1].strip())\n",
    "\n",
    "    return expanded_queries[:4]  # Limit to 4 queries (original + 3 expansions)\n",
    "\n",
    "def create_sentence_windows(text, window_size=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    windows = []\n",
    "    for i in range(len(sentences)):\n",
    "        window = \" \".join(sentences[max(0, i-window_size):min(len(sentences), i+window_size+1)])\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def chunk_documents(documents, chunk_size=250):\n",
    "    chunked_docs = []\n",
    "    for i in range(0, len(documents), chunk_size):\n",
    "        chunk = documents[i:i+chunk_size]\n",
    "        chunked_docs.append(chunk)\n",
    "    return chunked_docs\n",
    "\n",
    "def generate_hypothetical_document(query):\n",
    "    hyde_prompt = f\"\"\"\n",
    "    Given the search query below, generate a hypothetical document that would be a perfect match for this query. The document should be detailed and contain relevant information that directly addresses the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Document:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def llm_rerank(query, documents, k=5):\n",
    "    rerank_prompt = \"\"\"\n",
    "    Given the following query and a list of document excerpts, rank the documents based on their relevance to the query. Provide the rankings as a list of numbers from 1 to {}, where 1 is the most relevant.\n",
    "\n",
    "    Query: {}\n",
    "\n",
    "    Documents:\n",
    "    {}\n",
    "\n",
    "    Rankings (1 to {}):\n",
    "    \"\"\".format(len(documents), query, \"\\n\".join([f\"{i+1}. {doc.page_content[:200]}...\" for i, doc in enumerate(documents)]), len(documents))\n",
    "\n",
    "    response = llm.invoke(rerank_prompt)\n",
    "    rankings = [int(x) for x in response.content.split() if x.isdigit()]\n",
    "\n",
    "    # Sort documents based on rankings and return top k\n",
    "    return sorted(zip(documents, rankings), key=lambda x: x[1])[:k]\n",
    "\n",
    "def process_query(query, num_expansions, num_urls, num_docs, num_rerank):\n",
    "    try:\n",
    "        # Generate hypothetical document\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        print(\"Generated hypothetical document:\")\n",
    "        print(hypothetical_doc[:200] + \"...\")  # Print first 200 characters\n",
    "\n",
    "        # Embed the hypothetical document\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "\n",
    "        expanded_queries = query_expansion(query)[:num_expansions]\n",
    "        print(\"Expanded queries:\", expanded_queries)\n",
    "\n",
    "        all_texts = []\n",
    "        for expanded_query in expanded_queries:\n",
    "            content = search_and_scrape(expanded_query)\n",
    "            all_texts.append(content)\n",
    "            print(f\"Content length for query '{expanded_query}': {len(content)} characters\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        sentence_windows = create_sentence_windows(combined_text)\n",
    "        print(f\"Number of sentence windows: {len(sentence_windows)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=window, metadata={\"window\": window}) for window in sentence_windows]\n",
    "        print(f\"Number of index documents: {len(index_documents)}\")\n",
    "\n",
    "        chunked_docs = chunk_documents(index_documents)\n",
    "        print(f\"Number of document chunks: {len(chunked_docs)}\")\n",
    "\n",
    "        vectorstore = Chroma(embedding_function=embeddings, collection_name=\"rag-chroma\")\n",
    "\n",
    "        for i, chunk in enumerate(chunked_docs):\n",
    "            vectorstore.add_documents(chunk)\n",
    "\n",
    "        def get_hyde_retriever(vectorstore, hyde_embedding):\n",
    "            base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": num_docs})\n",
    "\n",
    "            def retriever(query):\n",
    "                docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs)\n",
    "                print(f\"Retrieved {len(docs)} documents using HyDE\")\n",
    "\n",
    "                unique_docs = []\n",
    "                seen_content = set()\n",
    "                for doc in docs:\n",
    "                    content = doc.metadata.get(\"window\", doc.page_content)\n",
    "                    if content not in seen_content:\n",
    "                        unique_docs.append(Document(page_content=content))\n",
    "                        seen_content.add(content)\n",
    "\n",
    "                print(f\"Number of unique documents: {len(unique_docs)}\")\n",
    "\n",
    "                reranked_docs = llm_rerank(query, unique_docs, k=num_rerank)\n",
    "\n",
    "                return [doc for doc, _ in reranked_docs]\n",
    "\n",
    "            return retriever\n",
    "\n",
    "        retriever = get_hyde_retriever(vectorstore, hyde_embedding)\n",
    "\n",
    "        retrieved_docs = retriever(query)\n",
    "        print(f\"Number of retrieved and reranked documents: {len(retrieved_docs)}\")\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            print(f\"Retrieved document {i + 1}: {doc.page_content[:200]}...\")\n",
    "\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        prompt_template = \"\"\"\n",
    "        Use the following context to answer the question. If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "        answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\"\n",
    "\n",
    "def gradio_interface(query, num_expansions, num_urls, num_docs, num_rerank):\n",
    "    # Capture print statements\n",
    "    import io\n",
    "    import sys\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "    # Process the query\n",
    "    answer = process_query(query, num_expansions, num_urls, num_docs, num_rerank)\n",
    "\n",
    "    # Restore stdout and get the captured output\n",
    "    sys.stdout = old_stdout\n",
    "    captured_output = buffer.getvalue()\n",
    "\n",
    "    # Combine the captured output and the answer\n",
    "    full_output = f\"{captured_output}\\n\\nFinal Answer: {answer}\"\n",
    "    return full_output\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your query\"),\n",
    "        gr.Slider(minimum=1, maximum=5, value=3, step=1, label=\"Number of query expansions\"),\n",
    "        gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of URLs to scrape per query\"),\n",
    "        gr.Slider(minimum=20, maximum=200, value=60, step=1, label=\"Number of documents to retrieve\"),\n",
    "        gr.Slider(minimum=10, maximum=100, value=30, step=1, label=\"Number of documents to keep after reranking\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Advanced RAG Query Processing\",\n",
    "    description=\"Enter a query and adjust parameters to get a detailed answer based on web search and document analysis.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qTxhqvrgL2_N",
    "outputId": "5d66f9a6-488e-48da-86e0-7f7735adb283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "Running on public URL: https://1f587107c8448812de.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1f587107c8448812de.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-8-8df6523e76e3>\", line 128, in process_query\n",
      "    vectorstore.add_documents(chunk)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores/base.py\", line 491, in add_documents\n",
      "    return self.add_texts(texts, metadatas, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\", line 277, in add_texts\n",
      "    embeddings = self._embedding_function.embed_documents(texts)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_fireworks/embeddings.py\", line 47, in embed_documents\n",
      "    for i in self._client.embeddings.create(input=texts, model=self.model).data\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.InternalServerError: Error code: 500 - {'detail': 'embeding failed'}\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-8-8df6523e76e3>\", line 128, in process_query\n",
      "    vectorstore.add_documents(chunk)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores/base.py\", line 491, in add_documents\n",
      "    return self.add_texts(texts, metadatas, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\", line 277, in add_texts\n",
      "    embeddings = self._embedding_function.embed_documents(texts)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_fireworks/embeddings.py\", line 47, in embed_documents\n",
      "    for i in self._client.embeddings.create(input=texts, model=self.model).data\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.InternalServerError: Error code: 500 - {'detail': 'embeding failed'}\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-8-8df6523e76e3>\", line 128, in process_query\n",
      "    vectorstore.add_documents(chunk)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores/base.py\", line 491, in add_documents\n",
      "    return self.add_texts(texts, metadatas, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\", line 277, in add_texts\n",
      "    embeddings = self._embedding_function.embed_documents(texts)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_fireworks/embeddings.py\", line 47, in embed_documents\n",
      "    for i in self._client.embeddings.create(input=texts, model=self.model).data\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/embeddings.py\", line 114, in create\n",
      "    return self._post(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1266, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 942, in request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1031, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1079, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1046, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.InternalServerError: Error code: 500 - {'detail': 'embeding failed'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "search = GoogleSerperAPIWrapper(k=3)\n",
    "embeddings = FireworksEmbeddings(model=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "def scrape_webpage(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text[:50000], len(text[:50000])\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\", 0\n",
    "\n",
    "def search_and_scrape(query, num_urls):\n",
    "    search_results = search.results(query)\n",
    "    scraped_urls = set()\n",
    "    full_texts = []\n",
    "    if 'organic' in search_results:\n",
    "        for result in search_results['organic']:\n",
    "            url = result.get('link')\n",
    "            if url and url not in scraped_urls and len(full_texts) < num_urls:\n",
    "                text, length = scrape_webpage(url)\n",
    "                print(f\"Scraped {url}: {length} characters\")\n",
    "                full_texts.append(text)\n",
    "                scraped_urls.add(url)\n",
    "    return \" \".join(full_texts)\n",
    "\n",
    "def query_expansion(query):\n",
    "    expansion_prompt = f\"\"\"\n",
    "    Given the following search query, generate 3 additional related queries that could help find more comprehensive information on the topic. The queries should be different from each other and explore various aspects of the main query. Provide only the additional queries, numbered 1-3.\n",
    "\n",
    "    Main query: {query}\n",
    "\n",
    "    Additional queries:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(expansion_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    expanded_queries = [query]\n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.strip() and line[0].isdigit():\n",
    "            expanded_queries.append(line.split('. ', 1)[1].strip())\n",
    "\n",
    "    return expanded_queries[:4]  # Limit to 4 queries (original + 3 expansions)\n",
    "\n",
    "def create_sentence_windows(text, window_size=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    windows = []\n",
    "    for i in range(len(sentences)):\n",
    "        window = \" \".join(sentences[max(0, i-window_size):min(len(sentences), i+window_size+1)])\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def chunk_documents(documents, chunk_size=250):\n",
    "    chunked_docs = []\n",
    "    for i in range(0, len(documents), chunk_size):\n",
    "        chunk = documents[i:i+chunk_size]\n",
    "        chunked_docs.append(chunk)\n",
    "    return chunked_docs\n",
    "\n",
    "def generate_hypothetical_document(query):\n",
    "    hyde_prompt = f\"\"\"\n",
    "    Given the search query below, generate a hypothetical document that would be a perfect match for this query. The document should be detailed and contain relevant information that directly addresses the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Document:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def llm_rerank(query, documents, k=5):\n",
    "    rerank_prompt = \"\"\"\n",
    "    Given the following query and a list of document excerpts, rank the documents based on their relevance to the query. Provide the rankings as a list of numbers from 1 to {}, where 1 is the most relevant.\n",
    "\n",
    "    Query: {}\n",
    "\n",
    "    Documents:\n",
    "    {}\n",
    "\n",
    "    Rankings (1 to {}):\n",
    "    \"\"\".format(len(documents), query, \"\\n\".join([f\"{i+1}. {doc.page_content[:200]}...\" for i, doc in enumerate(documents)]), len(documents))\n",
    "\n",
    "    response = llm.invoke(rerank_prompt)\n",
    "    rankings = [int(x) for x in response.content.split() if x.isdigit()]\n",
    "\n",
    "    # Sort documents based on rankings and return top k\n",
    "    return sorted(zip(documents, rankings), key=lambda x: x[1])[:k]\n",
    "\n",
    "def process_query(query, num_expansions, num_urls, num_docs, num_rerank):\n",
    "    try:\n",
    "        # Generate hypothetical document\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        print(\"Generated hypothetical document:\")\n",
    "        print(hypothetical_doc[:200] + \"...\")  # Print first 200 characters\n",
    "\n",
    "        # Embed the hypothetical document\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "\n",
    "        expanded_queries = query_expansion(query)[:num_expansions]\n",
    "        print(f\"Expanded queries ({num_expansions}):\", expanded_queries)\n",
    "\n",
    "        all_texts = []\n",
    "        for expanded_query in expanded_queries:\n",
    "            content = search_and_scrape(expanded_query, num_urls)\n",
    "            all_texts.append(content)\n",
    "            print(f\"Content length for query '{expanded_query}': {len(content)} characters\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        sentence_windows = create_sentence_windows(combined_text)\n",
    "        print(f\"Number of sentence windows: {len(sentence_windows)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=window, metadata={\"window\": window}) for window in sentence_windows]\n",
    "        print(f\"Number of index documents: {len(index_documents)}\")\n",
    "\n",
    "        chunked_docs = chunk_documents(index_documents)\n",
    "        print(f\"Number of document chunks: {len(chunked_docs)}\")\n",
    "\n",
    "        vectorstore = Chroma(embedding_function=embeddings, collection_name=\"rag-chroma\")\n",
    "\n",
    "        for i, chunk in enumerate(chunked_docs):\n",
    "            vectorstore.add_documents(chunk)\n",
    "            # print(f\"Processed chunk {i+1}/{len(chunked_docs)}\")\n",
    "\n",
    "        def get_hyde_retriever(vectorstore, hyde_embedding):\n",
    "            base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": num_docs})\n",
    "\n",
    "            def retriever(query):\n",
    "                # Use HyDE embedding for retrieval\n",
    "                docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs)\n",
    "                print(f\"Retrieved {len(docs)} documents using HyDE\")\n",
    "\n",
    "                # Remove duplicates\n",
    "                unique_docs = []\n",
    "                seen_content = set()\n",
    "                for doc in docs:\n",
    "                    content = doc.metadata.get(\"window\", doc.page_content)\n",
    "                    if content not in seen_content:\n",
    "                        unique_docs.append(Document(page_content=content))\n",
    "                        seen_content.add(content)\n",
    "\n",
    "                print(f\"Number of unique documents: {len(unique_docs)}\")\n",
    "\n",
    "                # Apply LLM reranking\n",
    "                reranked_docs = llm_rerank(query, unique_docs, k=num_rerank)\n",
    "\n",
    "                return [doc for doc, _ in reranked_docs]\n",
    "\n",
    "            return retriever\n",
    "\n",
    "        retriever = get_hyde_retriever(vectorstore, hyde_embedding)\n",
    "\n",
    "        retrieved_docs = retriever(query)\n",
    "        print(f\"Number of retrieved and reranked documents: {len(retrieved_docs)}\")\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            print(f\"Retrieved document {i + 1}: {doc.page_content[:80]}...\")\n",
    "\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        prompt_template = \"\"\"\n",
    "        Use the following context to answer the question. If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "        answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "        print(\"\\n\")\n",
    "        print(\"\\-\"*50)\n",
    "        print(\"Final Answer:\\n\", answer)\n",
    "        return answer\n",
    "        print(\"-\"*50)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\"\n",
    "\n",
    "def gradio_interface(query, num_expansions, num_urls, num_docs, num_rerank):\n",
    "    # Set up capturing of print statements\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "    # Process the query\n",
    "    answer = process_query(query, num_expansions, num_urls, num_docs, num_rerank)\n",
    "\n",
    "    # Restore stdout and get the captured output\n",
    "    sys.stdout = old_stdout\n",
    "    captured_output = buffer.getvalue()\n",
    "\n",
    "    # Return the captured output\n",
    "    return captured_output\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your query\"),\n",
    "        gr.Slider(minimum=1, maximum=5, value=2, step=1, label=\"Number of query expansions\"),\n",
    "        gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of URLs to scrape per extended query\"),\n",
    "        gr.Slider(minimum=20, maximum=200, value=60, step=1, label=\"Number of documents to retrieve with HyDe\"),\n",
    "        gr.Slider(minimum=10, maximum=100, value=30, step=1, label=\"Number of documents to keep after reranking\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Advanced RAG Query Processing\",\n",
    "    description=\"Enter a query and adjust parameters to get a detailed answer based on web search and document analysis.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0fKJf6DOMrD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqKbpHTWuTwGZZoXbULNW6",
   "provenance": [
    {
     "file_id": "125wkisw8alSzlySbwhQ44VYiuY0CJHf2",
     "timestamp": 1722636201777
    },
    {
     "file_id": "16Ygm33u7K9_YHvuOIHxJYllSJqqAk0SH",
     "timestamp": 1722635210899
    },
    {
     "file_id": "1JZGn8IigITIj7-Eq4OF2zRxGc3v_qrYo",
     "timestamp": 1722629409325
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
