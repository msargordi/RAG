{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8uS3H8y-DKdu",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langchain_fireworks google-search-results requests gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZOJPeq3DUgR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1722437892430,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "pO8Bp6MTDVLf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['FIREWORKS_API_KEY'] = 'API_KEY'\n",
    "os.environ[\"SERPER_API_KEY\"] = 'API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4872,
     "status": "ok",
     "timestamp": 1722437898566,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "MWNbV0RDQ_EG",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os, bs4, re, time, json\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_fireworks import FireworksEmbeddings, ChatFireworks\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from collections import defaultdict\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1722437898566,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "haT8ApGDvM7K"
   },
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"You are an AI assistant tasked with answering questions based solely on the provided context. Here is how you should approach the task:\n",
    "\n",
    "1. ALWAYS analyze the provided context carefully. This context consists of search result snippets.\n",
    "2. ALWAYS ask yourself this key question and reason about it:\n",
    "   - Does the context explicitly provide complete information to answer the question, or are there any ambiguities or missing pieces of information? Explain your reasoning very concise.\n",
    "3. Based on your analysis:\n",
    "   - If the context has complete information or compelte but inconsistent from different part of the context:\n",
    "     - Provide a clear and concise answer based strictly on the information within the context.\n",
    "     - Conclude your response with \"Sufficient: Yes.\"\n",
    "\n",
    "   - If the context does not contain complete information to answer the question:\n",
    "     - Explain why you cannot answer the question.\n",
    "     - Conclude your response with \"Sufficient: No.\"\n",
    "\n",
    "4. Finally, check if you have added either \"Sufficient: Yes.\" or \"Sufficient: No.\" because you should end with one of the two.\n",
    "\n",
    "Example Demonstrations:\n",
    "\n",
    "1. Context contains complete information:\n",
    "\n",
    "Context: \"The Great Wall of China is an ancient series of walls and fortifications located in northern China, built around 500 years ago. Its total length is approximately 21,196 kilometers (13,171 miles), making it the longest wall in the world. The primary purpose of the Great Wall was to protect the Chinese states and empires against nomadic invasions from the Eurasian Steppe.\"\n",
    "\n",
    "Question: \"How long is the Great Wall of China and what was its main purpose?\"\n",
    "\n",
    "Analysis and Reasoning:\n",
    "Does the context explicitly provide complete information to answer the question, or are there any ambiguities or missing pieces of information? The context provides complete information to answer the question without ambiguities. It explicitly states the length of the Great Wall as approximately 21,196 kilometers (13,171 miles) and clearly mentions its primary purpose of protecting Chinese states and empires against nomadic invasions from the Eurasian Steppe.\n",
    "\n",
    "Answer:\n",
    "The Great Wall of China is approximately 21,196 kilometers (13,171 miles) long. Its main purpose was to protect Chinese states and empires against nomadic invasions from the Eurasian Steppe.\n",
    "\n",
    "Sufficient: Yes\n",
    "\n",
    "2. Context does not contain complete information:\n",
    "\n",
    "Context: \"The Great Wall of China is an ancient series of walls and fortifications located in northern China. Its primary purpose was to protect the Chinese states and empires against nomadic invasions from the Eurasian Steppe. The wall was built over many centuries, with the most famous sections dating back to the Ming Dynasty.\"\n",
    "\n",
    "Question: \"How long is the Great Wall of China and what was its main purpose?\"\n",
    "\n",
    "Analysis and Reasoning:\n",
    "Does the context explicitly provide complete information to answer the question, or are there any ambiguities or missing pieces of information? The context does not provide complete information to answer the question. While it clearly states the main purpose of the Great Wall, which is to protect Chinese states and empires against nomadic invasions from the Eurasian Steppe, it lacks any information about the length of the wall. This is a critical piece of missing information required to fully answer the question.\n",
    "\n",
    "Answer:\n",
    "I cannot provide a complete answer based on the provided context. The context explains the main purpose of the Great Wall of China but does not provide any information about its length, which is explicitly asked for in the question.\n",
    "\n",
    "Sufficient: No\n",
    "\n",
    "Now, please analyze the following context and answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt_2 = \"\"\"You are an AI assistant tasked with answering questions based solely on the provided context. Here is how you should approach the task:\n",
    "\n",
    "1. ALWAYS analyze the provided context carefully. This context consists of search result snippets.\n",
    "2. ALWAYS ask yourself this key question and reason about it:\n",
    "   - Does the context explicitly provide complete information to answer the question, or are there any ambiguities or missing pieces of information? Explain your reasoning very concise. It should NOT be more than 30 words.\n",
    "3. Based on your analysis:\n",
    "   - If the context has complete information or compelte but inconsistent from different part of the context:\n",
    "     - Provide a clear and concise answer based strictly on the information within the context.\n",
    "     - Conclude your response with \"Sufficient: Yes.\"\n",
    "\n",
    "   - If the context does not contain complete information to answer the question:\n",
    "     - Explain why you cannot answer the question.\n",
    "     - Conclude your response with \"Sufficient: No.\"\n",
    "\n",
    "4. Finally, check if you have added either \"Sufficient: Yes.\" or \"Sufficient: No.\" because you should end with one of the two.\n",
    "\n",
    "Example Demonstrations:\n",
    "\n",
    "1. Context contains complete information:\n",
    "\n",
    "Context: \"The Great Wall of China is an ancient series of walls and fortifications located in northern China, built around 500 years ago. Its total length is approximately 21,196 kilometers (13,171 miles), making it the longest wall in the world. The primary purpose of the Great Wall was to protect the Chinese states and empires against nomadic invasions from the Eurasian Steppe.\"\n",
    "\n",
    "Question: \"How long is the Great Wall of China and what was its main purpose?\"\n",
    "\n",
    "Reasoning:\n",
    "Context provides both length and purpose of the Great Wall, answering the question completely.\n",
    "\n",
    "Answer:\n",
    "The Great Wall of China is approximately 21,196 kilometers (13,171 miles) long. Its main purpose was to protect Chinese states and empires against nomadic invasions from the Eurasian Steppe.\n",
    "\n",
    "Sufficient: Yes\n",
    "\n",
    "2. Context does not contain complete information:\n",
    "\n",
    "Context: \"The Great Wall of China is an ancient series of walls and fortifications located in northern China. Its primary purpose was to protect the Chinese states and empires against nomadic invasions from the Eurasian Steppe. The wall was built over many centuries, with the most famous sections dating back to the Ming Dynasty.\"\n",
    "\n",
    "Question: \"How long is the Great Wall of China and what was its main purpose?\"\n",
    "\n",
    "Reasoning:\n",
    "Context provides the purpose but lacks information about the length of the Great Wall.\n",
    "\n",
    "Answer:\n",
    "I cannot provide a complete answer based on the provided context. The context explains the main purpose of the Great Wall of China but does not provide any information about its length, which is explicitly asked for in the question.\n",
    "\n",
    "Sufficient: No\n",
    "\n",
    "Now, please analyze the following context and answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt_3 = \"\"\"You are an AI assistant answering questions based solely on the provided context. Follow these steps:\n",
    "\n",
    "1. Analyze the context (search result snippets) and determine if it contains complete information to answer the question. Explain your reasoning in 30 words or less.\n",
    "\n",
    "2. If the context is sufficient:\n",
    "   - Provide a clear, concise answer based strictly on the context.\n",
    "   - End with \"Sufficient: Yes.\"\n",
    "\n",
    "3. If the context is insufficient:\n",
    "   - Briefly explain why you cannot answer the question.\n",
    "   - End with \"Sufficient: No.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2438,
     "status": "ok",
     "timestamp": 1722437902403,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "PthIqww7XbuZ",
    "outputId": "f39744de-ba24-473e-e13f-704849e56b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Apple M2 chip was unveiled at Apple's WWDC 2022 keynote on June 6, and it's the inaugural chip in Apple's second generation of bespoke silicon.\n",
      "\n",
      "Sufficient: Yes.\n",
      "0.55 Seconds\n"
     ]
    }
   ],
   "source": [
    "num_search_docs = 10\n",
    "search = GoogleSerperAPIWrapper(k=num_search_docs)\n",
    "\n",
    "def extract_snippets(data):\n",
    "    snippets = []\n",
    "    if 'organic' in data:\n",
    "        for result in data['organic']:\n",
    "            if 'snippet' in result:\n",
    "                snippets.append(result['snippet'])\n",
    "    if 'peopleAlsoAsk' in data:\n",
    "        for item in data['peopleAlsoAsk']:\n",
    "            if 'snippet' in item:\n",
    "                snippets.append(item['snippet'])\n",
    "    return snippets\n",
    "\n",
    "def get_context(question):\n",
    "    results = search.results(question)\n",
    "    snippets = extract_snippets(results)\n",
    "    return \"\\n\\n\".join(snippets[:num_search_docs])\n",
    "\n",
    "def answer_question(question, model_choice, prompt_choice):\n",
    "    context = get_context(question)\n",
    "\n",
    "    # Select the model\n",
    "    if model_choice == \"LLaMA-3.1-8B\":\n",
    "        llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "    elif model_choice == \"Gemma2-9B\":\n",
    "        llm = ChatFireworks(model_name=\"accounts/fireworks/models/gemma2-9b-it\", temperature=0)\n",
    "\n",
    "    # Select the prompt\n",
    "    if prompt_choice == \"Prompt 1\":\n",
    "        system_template = prompt_1\n",
    "    elif prompt_choice == \"Prompt 2\":\n",
    "        system_template = prompt_2\n",
    "    elif prompt_choice == \"Prompt 3\":\n",
    "        system_template = prompt_3\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt choice. Choose 'Prompt 1', 'Prompt 2', or 'Prompt 3'.\")\n",
    "\n",
    "    prompt = PromptTemplate.from_template(system_template)\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    input_dict = {\"context\": context, \"question\": question}\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = chain.invoke(input_dict)\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    return {\n",
    "        \"model\": model_choice,\n",
    "        \"prompt\": prompt_choice,\n",
    "        \"context\": context,\n",
    "        \"response\": response,\n",
    "        \"response_time\": response_time\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "question = \"When was Apple M2 released?\"\n",
    "out = answer_question(question, \"LLaMA-3.1-8B\", \"Prompt 3\")\n",
    "\n",
    "print(f\"{out['response']}\")\n",
    "print(f\"{out['response_time']:.2f} Seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2024,
     "status": "ok",
     "timestamp": 1722437907000,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "35kePFyjvoVb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e509fbec-0990-4972-c8c9-d919e3695336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://a0fa09fcf006481589.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a0fa09fcf006481589.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_output(result):\n",
    "    return f\"\"\"\n",
    "### Model: {result['model']}\n",
    "### Prompt: {result['prompt']}\n",
    "\n",
    "## Question:\n",
    "{result['question']}\n",
    "\n",
    "## Context:\n",
    "{result['context']}\n",
    "\n",
    "## Answer:\n",
    "{result['response']}\n",
    "\n",
    "**Response Time:** {result['response_time']}\n",
    "    \"\"\"\n",
    "\n",
    "def answer_question_wrapper(question, model_choice, prompt_choice):\n",
    "    result = answer_question(question, model_choice, prompt_choice)\n",
    "    result['question'] = question  # Add the question to the result dictionary\n",
    "    return format_output(result)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=answer_question_wrapper,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Question\"),\n",
    "        gr.Radio([\"LLaMA-3.1-8B\", \"Gemma2-9B\"], label=\"Model Choice\", value=\"LLaMA-3.1-8B\"),\n",
    "        gr.Radio([\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"], label=\"Prompt Choice\", value=\"Prompt 1\")\n",
    "    ],\n",
    "    outputs=gr.Markdown(),\n",
    "    title=\"Question Answering Model\",\n",
    "    description=\"Enter a question, choose a model and a prompt, and the system will provide an answer based on web search, along with the context and response time.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 19367,
     "status": "ok",
     "timestamp": 1722374932174,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "wNyam-OQ88TB",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0142940d-8a0f-4e42-9377-0742cbfa0935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 100 questions:\n",
      "1. What is the average airspeed velocity of an unladen swallow?\n",
      "2. Which ancient civilization built the first known sundial?\n",
      "3. What is the chemical composition of the pigment Tyrian purple?\n",
      "4. Who is the author of the first computer bug?\n",
      "5. What is the name of the largest living organism in the world?\n",
      "6. In what year did the first human settle in North America?\n",
      "7. What is the name of the algorithm used in the first GPS system?\n",
      "8. Who painted the ceiling of the Sistine Chapel?\n",
      "9. What is the deepest part of the ocean?\n",
      "10. What is the name of the first computer virus?\n",
      "11. Who is the founder of the field of psychoanalysis?\n",
      "12. What is the name of the largest star known to science?\n",
      "13. What is the chemical formula for the compound responsible for the smell of skunk spray?\n",
      "14. Who is the author of the book \"The Origin of Species\"?\n",
      "15. What is the name of the first successful polio vaccine?\n",
      "16. In what year did the first human walk on the moon?\n",
      "17. What is the name of the largest known prime number?\n",
      "18. Who is the inventor of the first successful light bulb?\n",
      "19. What is the name of the famous prehistoric monument in England?\n",
      "20. What is the chemical composition of the human nose's olfactory receptors?\n",
      "21. Who is the author of the famous poem \"The Road Not Taken\"?\n",
      "22. What is the name of the largest living species of lizard?\n",
      "23. In what year did the first commercial jet airliner take flight?\n",
      "24. What is the name of the first computer programming language?\n",
      "25. Who is the founder of the field of anthropology?\n",
      "26. What is the name of the largest known black hole?\n",
      "27. What is the chemical formula for the compound responsible for the smell of freshly cut grass?\n",
      "28. Who is the author of the book \"The Interpretation of Dreams\"?\n",
      "29. What is the name of the first successful kidney transplant?\n",
      "30. In what year did the first human settle in Australia?\n",
      "31. What is the name of the largest known crystal cave?\n",
      "32. Who is the inventor of the first successful parachute?\n",
      "33. What is the chemical composition of the human eye's lens?\n",
      "34. Who is the author of the famous play \"Romeo and Juliet\"?\n",
      "35. What is the name of the largest known species of shark?\n",
      "36. In what year did the first commercial computer go on sale?\n",
      "37. What is the name of the first computer network?\n",
      "38. Who is the founder of the field of sociology?\n",
      "39. What is the name of the largest known comet?\n",
      "40. What is the chemical formula for the compound responsible for the smell of coffee?\n",
      "41. Who is the author of the book \"The Wealth of Nations\"?\n",
      "42. What is the name of the largest known species of squid?\n",
      "43. In what year did the first human walk on the bottom of the ocean?\n",
      "44. What is the name of the first computer mouse?\n",
      "45. Who is the inventor of the first successful telephone?\n",
      "46. What is the chemical composition of the human brain's neurons?\n",
      "47. Who is the author of the famous poem \"The Love Song of J. Alfred Prufrock\"?\n",
      "48. What is the name of the largest known species of octopus?\n",
      "49. In what year did the first commercial email service launch?\n",
      "50. What is the name of the first computer virus that affected the internet?\n",
      "51. Who is the founder of the field of psychology?\n",
      "52. What is the name of the largest known species of starfish?\n",
      "53. What is the chemical formula for the compound responsible for the smell of vanilla?\n",
      "54. Who is the author of the book \"The Republic\"?\n",
      "55. What is the name of the largest known species of jellyfish?\n",
      "56. In what year did the first human settle in South America?\n",
      "57. What is the name of the first computer operating system?\n",
      "58. Who is the inventor of the first successful laser?\n",
      "59. What is the chemical composition of the human skin's melanin?\n",
      "60. Who is the author of the famous play \"Hamlet\"?\n",
      "61. What is the name of the largest known species of butterfly?\n",
      "62. In what year did the first commercial satellite launch?\n",
      "63. What is the name of the first computer programming language for the web?\n",
      "64. Who is the founder of the field of economics?\n",
      "65. What is the name of the largest known species of frog?\n",
      "66. What is the chemical formula for the compound responsible for the smell of garlic?\n",
      "67. Who is the author of the book \"The Picture of Dorian Gray\"?\n",
      "68. What is the name of the largest known species of snake?\n",
      "69. In what year did the first human settle in New Zealand?\n",
      "70. What is the name of the first computer graphics software?\n",
      "71. Who is the inventor of the first successful robot?\n",
      "72. What is the chemical composition of the human eye's retina?\n",
      "73. Who is the author of the famous poem \"The Waste Land\"?\n",
      "74. What is the name of the largest known species of turtle?\n",
      "75. In what year did the first commercial video game console launch?\n",
      "76. What is the name of the first computer network protocol?\n",
      "77. Who is the founder of the field of philosophy?\n",
      "78. What is the name of the largest known species of crocodile?\n",
      "79. What is the chemical formula for the compound responsible for the smell of citrus?\n",
      "80. Who is the author of the book \"The Count of Monte Cristo\"?\n",
      "81. What is the name of the largest known species of fish?\n",
      "82. In what year did the first human settle in Antarctica?\n",
      "83. What is the name of the first computer spreadsheet software?\n",
      "84. Who is the inventor of the first successful microwave oven?\n",
      "85. What is the chemical composition of the human brain's synapses?\n",
      "86. Who is the author of the famous play \"Macbeth\"?\n",
      "87. What is the name of the largest known species of spider?\n",
      "88. In what year did the first commercial mobile phone launch?\n",
      "89. What is the name of the first computer graphics hardware?\n",
      "90. Who is the founder of the field of biology?\n",
      "91. What is the name of the largest known species of ant?\n",
      "92. What is the chemical formula for the compound responsible for the smell of honey?\n",
      "93. Who is the author of the book \"The Adventures of Huckleberry Finn\"?\n",
      "94. What is the name of the largest known species of bat?\n",
      "95. In what year did the first human settle in Hawaii?\n",
      "96. What is the name of the first computer antivirus software?\n",
      "97. Who is the inventor of the first successful solar panel?\n",
      "98. What is the chemical composition of the human skin's collagen?\n",
      "99. Who is the author of the famous poem \"The Raven\"?\n",
      "100. What is the name of the largest known species of whale?\n"
     ]
    }
   ],
   "source": [
    "# LLM for generating questions\n",
    "llm_generator = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0.6)\n",
    "\n",
    "# Question generation prompt\n",
    "question_gen_template = \"\"\"Generate exactly {num_questions} diverse and challenging questions that would require complex web searches to answer. The questions should:\n",
    "\n",
    "1. Cover a wide range of topics (e.g., science, history, current events, technology, arts, code)\n",
    "2. Include some questions that are easy to search and find solutions for\n",
    "3. Avoid long questions\n",
    "4. Include some easy factual questions in the list\n",
    "5. Ensure there is only one question per query. Query should NOT be multiple questions\n",
    "\n",
    "Please provide the questions as a numbered list, starting from 1 and ending at {num_questions}.\n",
    "\n",
    "Generated Questions:\"\"\"\n",
    "\n",
    "question_gen_prompt = PromptTemplate.from_template(question_gen_template)\n",
    "\n",
    "def generate_questions(num_questions, max_attempts=3):\n",
    "    for attempt in range(max_attempts):\n",
    "        question_gen_chain = question_gen_prompt | llm_generator | StrOutputParser()\n",
    "        questions_text = question_gen_chain.invoke({\"num_questions\": num_questions})\n",
    "\n",
    "        questions = []\n",
    "        for line in questions_text.split('\\n'):\n",
    "            match = re.match(r'^\\s*\\d+\\.\\s*(.+)$', line)\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                questions.append(question)\n",
    "\n",
    "        if len(questions) == num_questions:\n",
    "            return questions\n",
    "\n",
    "        print(f\"Attempt {attempt + 1}: Generated {len(questions)} questions instead of {num_questions}. Retrying...\")\n",
    "\n",
    "    raise ValueError(f\"Failed to generate exactly {num_questions} questions after {max_attempts} attempts.\")\n",
    "\n",
    "# Generate questions\n",
    "num_questions = 100\n",
    "try:\n",
    "    evaluation_questions = generate_questions(num_questions)\n",
    "    print(f\"Successfully generated {len(evaluation_questions)} questions:\")\n",
    "    for i, question in enumerate(evaluation_questions, 1):\n",
    "        print(f\"{i}. {question}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297866,
     "status": "ok",
     "timestamp": 1722365045347,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "2CJwp5wFP4Hs",
    "outputId": "61ca5d61-e867-477b-a170-4761fb78a54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 100\n",
      "Valid evaluations: 94\n",
      "Correct answers: 87\n",
      "Answer Accuracy: 92.55% (of 94 valid evaluations)\n",
      "Correct sufficiency assessments: 90\n",
      "Sufficiency Assessment Accuracy: 95.74% (of 94 valid evaluations)\n",
      "Averge answer time of the model is: 0.5932064151763916\n"
     ]
    }
   ],
   "source": [
    "# LLM for answering questions\n",
    "llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "\n",
    "# LLM for judging (70B model)\n",
    "judge_llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "# Chain for answering questions\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Judge prompt template\n",
    "judge_template = \"\"\"You are an expert AI evaluator tasked with assessing the performance of a smaller AI model. Your job is to evaluate the model's response based on the given context and question. Focus on two main aspects:\n",
    "\n",
    "1. Answer Correctness: Determine if the model's answer is correct and fully addresses the question based solely on the provided context.\n",
    "2. Sufficiency Assessment: Evaluate if the model's \"Sufficient: Yes/No\" conclusion is correct.\n",
    "\n",
    "Guidelines:\n",
    "- The model should only answer when it can COMPLETELY address the question using the context. It is ok if the model mentions it can not answer based on context.\n",
    "- Partial answers should be considered incorrect.\n",
    "- The model should not use any external knowledge not present in the context.\n",
    "- \"Sufficient: Yes\" should only be used when the context contains ALL necessary information to fully answer the question.\n",
    "- There should be always either \"Sufficient: Yes.\" or \"Sufficient: No.\" at the end.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Model's Response:\n",
    "{response}\n",
    "\n",
    "Provide your evaluation in the following JSON format:\n",
    "{{\n",
    "  \"answer_correctness\": \"Correct\" or \"Incorrect\",\n",
    "  \"sufficiency_assessment\": \"Correct\" or \"Incorrect\",\n",
    "  \"explanation\": \"Brief explanation of your evaluation\"\n",
    "}}\n",
    "\n",
    "Your Evaluation:\"\"\"\n",
    "\n",
    "judge_prompt = PromptTemplate.from_template(judge_template)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_response(context, question, response):\n",
    "    judge_chain = judge_prompt | judge_llm | StrOutputParser()\n",
    "    evaluation = judge_chain.invoke({\"context\": context, \"question\": question, \"response\": response})\n",
    "\n",
    "    try:\n",
    "        # Try to parse the JSON output\n",
    "        eval_dict = json.loads(evaluation)\n",
    "\n",
    "        # Ensure all required keys are present\n",
    "        required_keys = [\"answer_correctness\", \"sufficiency_assessment\", \"explanation\"]\n",
    "        if all(key in eval_dict for key in required_keys):\n",
    "            return eval_dict\n",
    "        else:\n",
    "            raise ValueError(\"Missing required keys in evaluation output\")\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        # If JSON parsing fails or required keys are missing, return an error dict\n",
    "        return {\n",
    "            \"answer_correctness\": \"Error\",\n",
    "            \"sufficiency_assessment\": \"Error\",\n",
    "            \"explanation\": f\"Failed to parse evaluation: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Run evaluation\n",
    "results = []\n",
    "total_answer_time = 0\n",
    "total_judge_time = 0\n",
    "\n",
    "for question in evaluation_questions:\n",
    "    context = get_context(question)\n",
    "\n",
    "    # Time the smaller model (answering)\n",
    "    start_answer = time.time()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    end_answer = time.time()\n",
    "    answer_time = end_answer - start_answer\n",
    "    total_answer_time += answer_time\n",
    "\n",
    "    # Time the larger model (judging)\n",
    "    start_judge = time.time()\n",
    "    evaluation = evaluate_response(context, question, answer)\n",
    "    end_judge = time.time()\n",
    "    judge_time = end_judge - start_judge\n",
    "    total_judge_time += judge_time\n",
    "\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"context\": context,\n",
    "        \"response\": answer,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"answer_time\": answer_time,\n",
    "        \"judge_time\": judge_time\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"evaluation_results_1.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "def parse_evaluation(evaluation):\n",
    "    lines = evaluation.strip().split('\\n')\n",
    "    answer_correctness = lines[0].split(': ')[1]\n",
    "    sufficiency_assessment = lines[1].split(': ')[1]\n",
    "    return answer_correctness, sufficiency_assessment\n",
    "\n",
    "correct_answers = 0\n",
    "correct_sufficiency = 0\n",
    "total_questions = len(evaluation_questions)\n",
    "valid_evaluations = 0\n",
    "\n",
    "for result in results:\n",
    "    evaluation = result[\"evaluation\"]\n",
    "    if evaluation[\"answer_correctness\"] != \"Error\":\n",
    "        valid_evaluations += 1\n",
    "        if evaluation[\"answer_correctness\"] == \"Correct\":\n",
    "            correct_answers += 1\n",
    "        if evaluation[\"sufficiency_assessment\"] == \"Correct\":\n",
    "            correct_sufficiency += 1\n",
    "\n",
    "print(f\"Total questions: {total_questions}\")\n",
    "print(f\"Valid evaluations: {valid_evaluations}\")\n",
    "print(f\"Correct answers: {correct_answers}\")\n",
    "print(f\"Answer Accuracy: {correct_answers / valid_evaluations * 100:.2f}% (of {valid_evaluations} valid evaluations)\")\n",
    "print(f\"Correct sufficiency assessments: {correct_sufficiency}\")\n",
    "print(f\"Sufficiency Assessment Accuracy: {correct_sufficiency / valid_evaluations * 100:.2f}% (of {valid_evaluations} valid evaluations)\")\n",
    "print(f\"Averge answer time of the model is: {total_answer_time/len(evaluation_questions):.2f} second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544368,
     "status": "ok",
     "timestamp": 1722365780659,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "OtsopaL0ulWN",
    "outputId": "dc48f721-8216-4273-e70c-8e4cf8bb9b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 100\n",
      "Valid evaluations: 100\n",
      "Correct answers: 95\n",
      "Answer Accuracy: 95.00% (of 100 valid evaluations)\n",
      "Correct sufficiency assessments: 99\n",
      "Sufficiency Assessment Accuracy: 99.00% (of 100 valid evaluations)\n",
      "Averge answer time of the model is: 0.6739742994308472\n"
     ]
    }
   ],
   "source": [
    "# LLM for answering questions\n",
    "llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "\n",
    "# LLM for judging (405B model)\n",
    "judge_llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
    "\n",
    "# Chain for answering questions\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "judge_prompt = PromptTemplate.from_template(judge_template)\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "results = []\n",
    "total_answer_time = 0\n",
    "total_judge_time = 0\n",
    "\n",
    "for question in evaluation_questions:\n",
    "    context = get_context(question)\n",
    "\n",
    "    # Time the smaller model (answering)\n",
    "    start_answer = time.time()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    end_answer = time.time()\n",
    "    answer_time = end_answer - start_answer\n",
    "    total_answer_time += answer_time\n",
    "\n",
    "    # Time the larger model (judging)\n",
    "    start_judge = time.time()\n",
    "    evaluation = evaluate_response(context, question, answer)\n",
    "    end_judge = time.time()\n",
    "    judge_time = end_judge - start_judge\n",
    "    total_judge_time += judge_time\n",
    "\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"answer_time\": answer_time,\n",
    "        \"judge_time\": judge_time\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"evaluation_results_2.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "def parse_evaluation(evaluation):\n",
    "    lines = evaluation.strip().split('\\n')\n",
    "    answer_correctness = lines[0].split(': ')[1]\n",
    "    sufficiency_assessment = lines[1].split(': ')[1]\n",
    "    return answer_correctness, sufficiency_assessment\n",
    "\n",
    "correct_answers = 0\n",
    "correct_sufficiency = 0\n",
    "total_questions = len(evaluation_questions)\n",
    "valid_evaluations = 0\n",
    "\n",
    "for result in results:\n",
    "    evaluation = result[\"evaluation\"]\n",
    "    if evaluation[\"answer_correctness\"] != \"Error\":\n",
    "        valid_evaluations += 1\n",
    "        if evaluation[\"answer_correctness\"] == \"Correct\":\n",
    "            correct_answers += 1\n",
    "        if evaluation[\"sufficiency_assessment\"] == \"Correct\":\n",
    "            correct_sufficiency += 1\n",
    "\n",
    "print(f\"Total questions: {total_questions}\")\n",
    "print(f\"Valid evaluations: {valid_evaluations}\")\n",
    "print(f\"Correct answers: {correct_answers}\")\n",
    "print(f\"Answer Accuracy: {correct_answers / valid_evaluations * 100:.2f}% (of {valid_evaluations} valid evaluations)\")\n",
    "print(f\"Correct sufficiency assessments: {correct_sufficiency}\")\n",
    "print(f\"Sufficiency Assessment Accuracy: {correct_sufficiency / valid_evaluations * 100:.2f}% (of {valid_evaluations} valid evaluations)\")\n",
    "print(f\"Averge answer time of the model is: {total_answer_time/len(evaluation_questions):.2f} second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0w5s528Q99H5"
   },
   "outputs": [],
   "source": [
    "def evaluate_models(evaluation_questions, models, prompts, judges):\n",
    "    results = []\n",
    "    response_times = defaultdict(list)\n",
    "\n",
    "    for model in models:\n",
    "        for prompt_name in prompts:\n",
    "            for judge_name in judges:\n",
    "                config_results = []\n",
    "                config_times = []\n",
    "\n",
    "                for question in evaluation_questions:\n",
    "                    context = get_context(question)\n",
    "\n",
    "                    # Select the prompt\n",
    "                    if prompt_name == \"Prompt 1\":\n",
    "                        system_template = prompt_1\n",
    "                    elif prompt_name == \"Prompt 2\":\n",
    "                        system_template = prompt_2\n",
    "                    elif prompt_name == \"Prompt 3\":\n",
    "                        system_template = prompt_3\n",
    "                    else:\n",
    "                        raise ValueError(f\"Invalid prompt choice: {prompt_name}\")\n",
    "\n",
    "                    prompt = PromptTemplate.from_template(system_template)\n",
    "\n",
    "                    # Answer the question\n",
    "                    llm = ChatFireworks(model_name=model_name_map[model], temperature=0)\n",
    "                    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "                    end_time = time.time()\n",
    "                    response_time = end_time - start_time\n",
    "\n",
    "                    config_times.append(response_time)\n",
    "\n",
    "                    judge = ChatFireworks(model_name=model_name_map[judge_name], temperature=0)\n",
    "\n",
    "                    judge_prompt = PromptTemplate.from_template(\"\"\"You are an expert AI evaluator tasked with assessing the performance of a smaller AI model. Your job is to evaluate the model's response based on the given context and question. Focus on two main aspects:\n",
    "\n",
    "                    1. Answer Correctness: Determine if the model's answer is correct and fully addresses the question based solely on the provided context.\n",
    "                    2. Sufficiency Assessment: Evaluate if the model's \"Sufficient: Yes/No\" conclusion is correct.\n",
    "\n",
    "                    Guidelines:\n",
    "                    - The model should only answer when it can COMPLETELY address the question using the context. It is ok if the model mentions it can not answer based on context.\n",
    "                    - Partial answers should be considered incorrect.\n",
    "                    - The model should not use any external knowledge not present in the context.\n",
    "                    - \"Sufficient: Yes\" should only be used when the context contains ALL necessary information to fully answer the question.\n",
    "                    - There should be always either \"Sufficient: Yes.\" or \"Sufficient: No.\" at the end.\n",
    "\n",
    "                    Context:\n",
    "                    {context}\n",
    "\n",
    "                    Question: {question}\n",
    "\n",
    "                    Model's Response:\n",
    "                    {response}\n",
    "\n",
    "                    Provide your evaluation in the following JSON format:\n",
    "                    {{\n",
    "                      \"answer_correctness\": \"Correct\" or \"Incorrect\",\n",
    "                      \"sufficiency_assessment\": \"Correct\" or \"Incorrect\",\n",
    "                      \"explanation\": \"Brief explanation of your evaluation\"\n",
    "                    }}\n",
    "\n",
    "                    Your Evaluation:\"\"\")\n",
    "\n",
    "                    judge_chain = judge_prompt | judge | StrOutputParser()\n",
    "\n",
    "                    evaluation = judge_chain.invoke({\n",
    "                        \"question\": question,\n",
    "                        \"context\": context,\n",
    "                        \"response\": answer\n",
    "                    })\n",
    "\n",
    "                    result = {\n",
    "                        \"question\": question,\n",
    "                        \"model\": model,\n",
    "                        \"prompt\": prompt_name,\n",
    "                        \"judge\": judge_name,\n",
    "                        \"response\": answer,\n",
    "                        \"response_time\": response_time,\n",
    "                        \"evaluation\": evaluation\n",
    "                    }\n",
    "\n",
    "                    config_results.append(result)\n",
    "\n",
    "                # Calculate metrics for this configuration\n",
    "                avg_response_time = sum(config_times) / len(config_times)\n",
    "                correct_answers = sum(1 for r in config_results if '\"answer_correctness\": \"Correct\"' in r['evaluation'])\n",
    "                correct_sufficiency = sum(1 for r in config_results if '\"sufficiency_assessment\": \"Correct\"' in r['evaluation'])\n",
    "\n",
    "                print(f\"\\nMetrics for configuration:\")\n",
    "                print(f\"Model: {model}\")\n",
    "                print(f\"Prompt: {prompt_name}\")\n",
    "                print(f\"Judge: {judge_name}\")\n",
    "                print(f\"Average Response Time: {avg_response_time:.2f} seconds\")\n",
    "                print(f\"Correct Answers: {correct_answers}/{len(evaluation_questions)}\")\n",
    "                print(f\"Correct Sufficiency Assessments: {correct_sufficiency}/{len(evaluation_questions)}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "                results.extend(config_results)\n",
    "                response_times[(model, prompt_name)].extend(config_times)\n",
    "\n",
    "    # Calculate and print overall average response times\n",
    "    print(\"\\nOverall Average Response Times:\")\n",
    "    for (model, prompt), times in response_times.items():\n",
    "        avg_time = sum(times) / len(times)\n",
    "        print(f\"Model: {model}, Prompt: {prompt} - Average Time: {avg_time:.2f} seconds\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6829180,
     "status": "ok",
     "timestamp": 1722381779051,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "IFxsfGWhYPYZ",
    "outputId": "c3acf779-8504-4507-d2bc-0ec6f6b0e994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 1\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.71 seconds\n",
      "Correct Answers: 96/100\n",
      "Correct Sufficiency Assessments: 96/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 1\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 0.85 seconds\n",
      "Correct Answers: 96/100\n",
      "Correct Sufficiency Assessments: 98/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'502 Server Error: Bad Gateway for url: https://api.smith.langchain.com/runs/batch\\', \\'\\\\n<html><head>\\\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\\\n<title>502 Server Error</title>\\\\n</head>\\\\n<body text=#000000 bgcolor=#ffffff>\\\\n<h1>Error: Server Error</h1>\\\\n<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\\\\n<h2></h2>\\\\n</body></html>\\\\n\\')')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 2\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.63 seconds\n",
      "Correct Answers: 95/100\n",
      "Correct Sufficiency Assessments: 96/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 2\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 0.56 seconds\n",
      "Correct Answers: 94/100\n",
      "Correct Sufficiency Assessments: 98/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 3\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.42 seconds\n",
      "Correct Answers: 80/100\n",
      "Correct Sufficiency Assessments: 89/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 3\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 0.40 seconds\n",
      "Correct Answers: 86/100\n",
      "Correct Sufficiency Assessments: 93/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 1\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.99 seconds\n",
      "Correct Answers: 86/100\n",
      "Correct Sufficiency Assessments: 86/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 1\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 1.04 seconds\n",
      "Correct Answers: 85/100\n",
      "Correct Sufficiency Assessments: 86/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 2\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.83 seconds\n",
      "Correct Answers: 91/100\n",
      "Correct Sufficiency Assessments: 92/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 2\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 0.82 seconds\n",
      "Correct Answers: 89/100\n",
      "Correct Sufficiency Assessments: 90/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 3\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.61 seconds\n",
      "Correct Answers: 73/100\n",
      "Correct Sufficiency Assessments: 79/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 3\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 0.62 seconds\n",
      "Correct Answers: 75/100\n",
      "Correct Sufficiency Assessments: 83/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Overall Average Response Times:\n",
      "Model: LLaMA-3.1-8B, Prompt: Prompt 1 - Average Time: 0.78 seconds\n",
      "Model: LLaMA-3.1-8B, Prompt: Prompt 2 - Average Time: 0.60 seconds\n",
      "Model: LLaMA-3.1-8B, Prompt: Prompt 3 - Average Time: 0.41 seconds\n",
      "Model: Gemma2-9B, Prompt: Prompt 1 - Average Time: 1.02 seconds\n",
      "Model: Gemma2-9B, Prompt: Prompt 2 - Average Time: 0.82 seconds\n",
      "Model: Gemma2-9B, Prompt: Prompt 3 - Average Time: 0.61 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping for model names\n",
    "model_name_map = {\n",
    "    \"LLaMA-3.1-8B\": \"accounts/fireworks/models/llama-v3p1-8b-instruct\",\n",
    "    \"Gemma2-9B\": \"accounts/fireworks/models/gemma2-9b-it\",\n",
    "    \"LLaMA-70B\": \"accounts/fireworks/models/llama-v3p1-70b-instruct\",\n",
    "    \"LLaMA-405B\": \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n",
    "}\n",
    "\n",
    "models = [\"LLaMA-3.1-8B\", \"Gemma2-9B\"]\n",
    "prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\n",
    "judges = [\"LLaMA-70B\", \"LLaMA-405B\"]\n",
    "\n",
    "evaluation_results = evaluate_models(evaluation_questions, models, prompts, judges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AKt07zga7fo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPzo8pgVt16Lwe41gcB6xJj",
   "provenance": [
    {
     "file_id": "16cbyPDOSqUShgvYeiK1jbtYBkzbK15us",
     "timestamp": 1722366099137
    },
    {
     "file_id": "1rjo4NNhfvSdBMPoGROW-lvwT9oLbR-PM",
     "timestamp": 1722357676502
    },
    {
     "file_id": "1pvUJLSQMwmwe-wo6JsZHt-Kh0tHp8JeM",
     "timestamp": 1722294345788
    },
    {
     "file_id": "1xMS4sulBD56oy7VHih-bdwaD3TN2l1tH",
     "timestamp": 1722288913723
    },
    {
     "file_id": "1aRrGzH8tc8b2cibEzz3RobyA4HOVgQLj",
     "timestamp": 1722283659675
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
