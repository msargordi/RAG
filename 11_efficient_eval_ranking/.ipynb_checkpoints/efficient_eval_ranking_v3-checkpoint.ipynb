{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930968b4-02cf-4f40-8360-0e07c67c0c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/maziar/eval_ranking/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_fireworks import FireworksEmbeddings, ChatFireworks\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "from typing import List, Tuple, Any\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import faiss\n",
    "import httpx\n",
    "from urllib.parse import urlparse\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from typing import List, Dict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae80fd5e-0346-4d85-8373-12da58b452f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU will be used automatically by FlashRank.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/ubuntu/.flashrank_cache/rank_zephyr_7b_v1_full/rank_zephyr_7b_v1_full.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = hub\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    98.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'hub', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set up API clients\n",
    "os.environ['FIREWORKS_API_KEY'] = 'API'\n",
    "# os.environ[\"SERPER_API_KEY\"] = 'API'\n",
    "os.environ[\"SERPER_API_KEY\"] = 'API'\n",
    "\n",
    "# Initialize components\n",
    "search = GoogleSerperAPIWrapper(k=3)\n",
    "embeddings = FireworksEmbeddings(model=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_8b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_70b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "# Create a directory for caching in the user's home folder\n",
    "cache_dir = Path.home() / \".flashrank_cache\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU will be used automatically by FlashRank.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. CPU will be used.\")\n",
    "\n",
    "# Initialize FlashRank rerankers\n",
    "ranker_nano = Ranker(cache_dir=str(cache_dir))\n",
    "ranker_small = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=str(cache_dir))\n",
    "ranker_medium_t5 = Ranker(model_name=\"rank-T5-flan\", cache_dir=str(cache_dir))\n",
    "ranker_medium_multilang = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=str(cache_dir))\n",
    "ranker_large = Ranker(model_name=\"rank_zephyr_7b_v1_full\", max_length=1024, cache_dir=str(cache_dir))\n",
    "\n",
    "# Ensure models are on GPU if available\n",
    "for ranker in [ranker_nano, ranker_small, ranker_medium_t5, ranker_medium_multilang, ranker_large]:\n",
    "    if hasattr(ranker, 'model') and hasattr(ranker.model, 'to'):\n",
    "        ranker.model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Download NLTK data\n",
    "# nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dec19805-7363-4f1d-b12f-13fd280b33bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n"
     ]
    }
   ],
   "source": [
    "async def scrape_webpage(client, url):\n",
    "    try:\n",
    "        response = await client.get(url, timeout=3.0)\n",
    "        response.raise_for_status()\n",
    "        text = response.text\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        content = ' '.join(soup.stripped_strings)\n",
    "        return content[:5000], len(content[:5000])\n",
    "    except (httpx.RequestError, httpx.TimeoutException) as exc:\n",
    "        print(f\"An error occurred while requesting {url}: {exc}\")\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(f\"Error response {exc.response.status_code} while requesting {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return \"\", 0\n",
    "\n",
    "async def search_and_scrape(query, num_urls):\n",
    "    search_results = search.results(query)\n",
    "    scraped_urls = set()\n",
    "    full_texts = []\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=httpx.Timeout(10.0, connect=3.0)) as client:\n",
    "        tasks = []\n",
    "        if 'organic' in search_results:\n",
    "            for result in search_results['organic']:\n",
    "                url = result.get('link')\n",
    "                domain = urlparse(url).netloc if url else None\n",
    "                if url and domain not in scraped_urls and len(tasks) < num_urls:\n",
    "                    tasks.append(scrape_webpage(client, url))\n",
    "                    scraped_urls.add(domain)\n",
    "\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        for result in results:\n",
    "            if isinstance(result, tuple) and result[1] > 0:\n",
    "                full_texts.append(result[0])\n",
    "\n",
    "    return \" \".join(full_texts)\n",
    "\n",
    "def query_expansion(query, num_expansions):\n",
    "    expansion_prompt = f\"\"\"\n",
    "    Given the following search query, generate {num_expansions} additional related queries that could help find more comprehensive information on the topic. The queries should be different from each other and explore various aspects of the main query. Provide only the additional queries, numbered 1-{num_expansions}.\n",
    "\n",
    "    Main query: {query}\n",
    "\n",
    "    Additional queries:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(expansion_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    expanded_queries = [query]\n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.strip() and line[0].isdigit():\n",
    "            expanded_queries.append(line.split('. ', 1)[1].strip())\n",
    "\n",
    "    return expanded_queries[:num_expansions + 1]\n",
    "\n",
    "def create_sentence_windows(text, window_size=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    windows = []\n",
    "    for i in range(len(sentences)):\n",
    "        window = \" \".join(sentences[max(0, i-window_size):min(len(sentences), i+window_size+1)])\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def generate_hypothetical_document(query):\n",
    "    hyde_prompt = f\"\"\"\n",
    "    Given the search query below, generate a hypothetical document that would be a perfect match for this query. The document should be concise, containing only 3 sentences of relevant information that directly addresses the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Document (3 sentences):\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def llm_rerank(query, documents):\n",
    "    rerank_prompt = \"\"\"\n",
    "    Given the following query and a list of document excerpts, rank the documents based on their relevance to the query. Provide the rankings as a list of numbers from 1 to {}, where 1 is the most relevant. Ensure you provide a ranking for every document.\n",
    "\n",
    "    Query: {}\n",
    "\n",
    "    Documents:\n",
    "    {}\n",
    "\n",
    "    Rankings (1 to {}):\n",
    "    \"\"\".format(len(documents), query, \"\\n\".join([f\"{i+1}. {doc.page_content[:200]}...\" for i, doc in enumerate(documents)]), len(documents))\n",
    "\n",
    "    response = llm.invoke(rerank_prompt)\n",
    "    rankings = [int(x) for x in response.content.split() if x.isdigit()]\n",
    "\n",
    "    if len(rankings) < len(documents):\n",
    "        remaining = set(range(1, len(documents) + 1)) - set(rankings)\n",
    "        rankings.extend(remaining)\n",
    "\n",
    "    sorted_docs = sorted(zip(documents, rankings), key=lambda x: x[1])\n",
    "    return sorted_docs\n",
    "\n",
    "def flashrank_rerank(query, documents, ranker):\n",
    "    rerank_request = RerankRequest(\n",
    "        query=query,\n",
    "        passages=[{\"text\": doc.page_content} for doc in documents]\n",
    "    )\n",
    "    reranked = ranker.rerank(rerank_request)\n",
    "    \n",
    "    if isinstance(reranked, list) and isinstance(reranked[0], dict):\n",
    "        sorted_results = sorted(reranked, key=lambda x: x.get('score', 0), reverse=True)\n",
    "        return [(documents[i], result.get('score', 0)) for i, result in enumerate(sorted_results)]\n",
    "    \n",
    "    elif isinstance(reranked, list) and hasattr(reranked[0], 'score'):\n",
    "        sorted_results = sorted(reranked, key=lambda x: x.score, reverse=True)\n",
    "        return [(documents[i], result.score) for i, result in enumerate(sorted_results)]\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unexpected reranked result type. Using original document order.\")\n",
    "        return [(doc, 1.0) for doc in documents]\n",
    "\n",
    "def get_hyde_retriever(vectorstores, hyde_embedding, num_docs, num_rerank, rerank_method):\n",
    "    def retriever(query):\n",
    "        all_docs = []\n",
    "        for vectorstore in vectorstores:\n",
    "            docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs)\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        unique_docs = []\n",
    "        seen_content = set()\n",
    "        for doc in all_docs:\n",
    "            content = doc.page_content\n",
    "            if content not in seen_content:\n",
    "                unique_docs.append(Document(page_content=content))\n",
    "                seen_content.add(content)\n",
    "\n",
    "        try:\n",
    "            if rerank_method == \"none\":\n",
    "                return unique_docs[:num_rerank]\n",
    "            elif rerank_method == \"llm\":\n",
    "                reranked_docs = llm_rerank(query, unique_docs)\n",
    "            elif rerank_method in [\"nano\", \"small\", \"medium_t5\", \"medium_multilang\", \"large\"]:\n",
    "                ranker = globals()[f\"ranker_{rerank_method}\"]\n",
    "                reranked_docs = flashrank_rerank(query, unique_docs, ranker)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown rerank method: {rerank_method}\")\n",
    "\n",
    "            return [doc for doc, _ in reranked_docs[:num_rerank]]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reranking with method {rerank_method}: {str(e)}\")\n",
    "            print(\"Traceback:\", traceback.format_exc())\n",
    "            print(\"Falling back to no reranking.\")\n",
    "            return unique_docs[:num_rerank]\n",
    "\n",
    "    return retriever\n",
    "\n",
    "def batch_embed_documents(documents, batch_size=512):\n",
    "    batched_embeddings = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        texts = [doc.page_content for doc in batch]\n",
    "        embeddings_batch = embeddings.embed_documents(texts)\n",
    "        batched_embeddings.extend(embeddings_batch)\n",
    "    return batched_embeddings\n",
    "\n",
    "async def process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        hyde_start = time.time()\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        hyde_time = time.time() - hyde_start\n",
    "        print(f\"hypothetical_doc length: {len(hypothetical_doc)}\")\n",
    "        print(f\"-----HyDE generation time: {hyde_time:.2f} seconds\")\n",
    "\n",
    "        embed_start = time.time()\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "        embed_time = time.time() - embed_start\n",
    "        print(f\"-----Embedding time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        ext_start = time.time()\n",
    "        expanded_queries = query_expansion(query, num_expansions)\n",
    "        ext_time = time.time() - embed_start\n",
    "        print(f\"-----Query expansion time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        scrape_start = time.time()\n",
    "        all_texts = await asyncio.gather(*[search_and_scrape(eq, num_urls) for eq in expanded_queries])\n",
    "        scrape_time = time.time() - scrape_start\n",
    "        print(f\"-----Web scraping time: {scrape_time:.2f} seconds\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        sentence_windows = create_sentence_windows(combined_text)\n",
    "        print(f\"Number of sentence windows: {len(sentence_windows)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=window) for window in sentence_windows]\n",
    "\n",
    "        vectorstore_start = time.time()\n",
    "        vectorstores = []\n",
    "        for i in range(0, len(index_documents), 256):\n",
    "            batch = index_documents[i:i + 256]\n",
    "\n",
    "            batch_embeddings = batch_embed_documents(batch)\n",
    "\n",
    "            texts = [doc.page_content for doc in batch]\n",
    "\n",
    "            vectorstore = FAISS.from_embeddings(\n",
    "                embedding=embeddings,\n",
    "                text_embeddings=list(zip(texts, batch_embeddings))\n",
    "            )\n",
    "            vectorstores.append(vectorstore)\n",
    "\n",
    "        vectorstore_time = time.time() - vectorstore_start\n",
    "        print(f\"-----Vectorstore creation time: {vectorstore_time:.2f} seconds\")\n",
    "\n",
    "        retrieval_start = time.time()\n",
    "        retriever = get_hyde_retriever(vectorstores, hyde_embedding, num_docs, num_rerank, rerank_method)\n",
    "        retrieved_docs = retriever(query)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        print(f\"-----Retrieval and reranking time: {retrieval_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"Number of retrieved and reranked documents: {len(retrieved_docs)}\")\n",
    "\n",
    "        context_docs = [doc.page_content for doc in retrieved_docs]\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "\n",
    "        total_processing_time = hyde_time + embed_time + scrape_time + vectorstore_time + retrieval_time\n",
    "        print(f\"-----Total processing time before answer generation: {total_processing_time:.2f} seconds\")\n",
    "\n",
    "        answer_start = time.time()\n",
    "        prompt_template = \"\"\"\n",
    "        Use the following context to answer the question. Before answering the question generate a reasoning step. then answer.\n",
    "        If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        chosen_llm = llm_70b if use_70b_model else llm_8b\n",
    "\n",
    "        rag_chain = prompt | chosen_llm | StrOutputParser()\n",
    "        answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "        answer_time = time.time() - answer_start\n",
    "        print(f\"-----Answer generation time: {answer_time:.2f} seconds\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*120)\n",
    "        print(\"Final Answer:\\n\", answer)\n",
    "        print(\"-\"*120)\n",
    "\n",
    "        return answer, context_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\", []\n",
    "\n",
    "def gradio_interface(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "    answer, context_docs = asyncio.run(process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model))\n",
    "\n",
    "    sys.stdout = old_stdout\n",
    "    captured_output = buffer.getvalue()\n",
    "\n",
    "    truncated_docs = [f\"Document {i+1}: {doc[:150]}...\" for i, doc in enumerate(context_docs)]\n",
    "    truncated_context = \"\\n\\n\".join(truncated_docs)\n",
    "\n",
    "    captured_output += f\"\\n\\nContext used for answer generation (first 150 characters of each document, {len(context_docs)} documents in total):\\n\" + truncated_context\n",
    "\n",
    "    return captured_output\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your query\"),\n",
    "        gr.Slider(minimum=0, maximum=3, value=1, step=1, label=\"Number of query expansions\"),\n",
    "        gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of URLs to scrape per extended query\"),\n",
    "        gr.Slider(minimum=20, maximum=80, value=80, step=1, label=\"Number of documents to retrieve with HyDE\"),\n",
    "        gr.Slider(minimum=10, maximum=80, value=50, step=1, label=\"Number of documents to keep after retrieval/reranking\"),\n",
    "        gr.Radio([\"none\", \"llm\", \"nano\", \"small\", \"medium_t5\", \"medium_multilang\"], label=\"Reranking method\", value=\"none\"),\n",
    "        gr.Checkbox(label=\"Use 70B model for QA (unchecked uses 8B)\", value=False)\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Advanced RAG Query Processing\",\n",
    "    description=\"Enter a query and adjust parameters to get a detailed answer based on web search and document analysis.\",\n",
    "    examples=[\n",
    "        [\"How can I take care of my eyes?\", 1, 3, 80, 50, \"llm\", False],\n",
    "        [\"How can I take care of my eyes?\", 1, 3, 80, 50, \"nano\", False]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     iface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "918a8247-ee12-4ca6-a588-20978b2ac68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 100 questions:\n",
      "1. What is the average airspeed velocity of an unladen swallow?\n",
      "2. Who is the oldest known living person with a verified birth certificate?\n",
      "3. What is the chemical composition of the Great Wall of China?\n",
      "4. Which country has the highest number of languages spoken?\n",
      "5. What is the largest living organism in the world?\n",
      "6. Who was the first person to walk on the moon?\n",
      "7. What is the most widely spoken language in the world?\n",
      "8. What is the highest mountain peak in the solar system?\n",
      "9. Who is the author of the oldest known surviving work of literature?\n",
      "10. What is the deepest part of the ocean?\n",
      "11. Which city has the most museums in the world?\n",
      "12. What is the largest waterfall in the world by volume?\n",
      "13. Who was the first woman to win a Nobel Prize?\n",
      "14. What is the longest word in the English language?\n",
      "15. What is the smallest country in the world?\n",
      "16. Who was the first person to fly solo around the world?\n",
      "17. What is the largest desert in the world?\n",
      "18. Which river is the longest in South America?\n",
      "19. Who was the first African American to win a Pulitzer Prize?\n",
      "20. What is the highest temperature ever recorded on Earth?\n",
      "21. What is the largest island in the Mediterranean Sea?\n",
      "22. Who was the first person to climb Mount Everest?\n",
      "23. What is the most widely used programming language?\n",
      "24. What is the largest living species of lizard?\n",
      "25. Who was the first woman to serve in the US Congress?\n",
      "26. What is the deepest cave in the world?\n",
      "27. Which city has the most skyscrapers in the world?\n",
      "28. What is the largest snowflake ever recorded?\n",
      "29. Who was the first person to win a Grammy, Emmy, and Oscar?\n",
      "30. What is the longest river in Asia?\n",
      "31. Who was the first person to walk across the United States?\n",
      "32. What is the largest coral reef system in the world?\n",
      "33. Who was the first woman to win a Fields Medal?\n",
      "34. What is the highest mountain peak in North America?\n",
      "35. Which city has the most bridges in the world?\n",
      "36. What is the largest planet in our solar system?\n",
      "37. Who was the first person to fly across the Atlantic Ocean?\n",
      "38. What is the largest waterfall in the world by height?\n",
      "39. Who was the first African American to serve in the US Senate?\n",
      "40. What is the longest word that can be typed using only the left hand on a standard keyboard?\n",
      "41. What is the largest island in the world?\n",
      "42. Who was the first person to win a Nobel Prize in Economics?\n",
      "43. What is the largest living species of shark?\n",
      "44. Who was the first woman to serve as a head of state?\n",
      "45. What is the deepest part of the Mariana Trench?\n",
      "46. Which city has the most universities in the world?\n",
      "47. What is the largest snowman ever built?\n",
      "48. Who was the first person to win a Pulitzer Prize and a Nobel Prize?\n",
      "49. What is the longest river in Europe?\n",
      "50. Who was the first person to walk on the sun?\n",
      "51. What is the largest living species of turtle?\n",
      "52. Who was the first woman to win a Tony Award?\n",
      "53. What is the highest mountain peak in South America?\n",
      "54. Which city has the most theaters in the world?\n",
      "55. What is the largest waterfall in the world by width?\n",
      "56. Who was the first person to fly around the world in a hot air balloon?\n",
      "57. What is the largest living species of frog?\n",
      "58. Who was the first African American to win a Nobel Prize in Literature?\n",
      "59. What is the longest word that can be typed using only the right hand on a standard keyboard?\n",
      "60. What is the largest island in the Pacific Ocean?\n",
      "61. Who was the first person to win a Nobel Prize in Physics?\n",
      "62. What is the largest living species of snake?\n",
      "63. Who was the first woman to serve as a Supreme Court Justice?\n",
      "64. What is the deepest part of the ocean in the Southern Hemisphere?\n",
      "65. Which city has the most libraries in the world?\n",
      "66. What is the largest snowflake ever recorded in the United States?\n",
      "67. Who was the first person to win a Grammy Award and a Pulitzer Prize?\n",
      "68. What is the longest river in Africa?\n",
      "69. Who was the first person to walk on the moon's surface?\n",
      "70. What is the largest living species of bird?\n",
      "71. Who was the first woman to win a Fields Medal and a Nobel Prize?\n",
      "72. What is the highest mountain peak in Europe?\n",
      "73. Which city has the most parks in the world?\n",
      "74. What is the largest waterfall in the world by volume in the Southern Hemisphere?\n",
      "75. Who was the first person to fly solo across the Atlantic Ocean?\n",
      "76. What is the largest living species of fish?\n",
      "77. Who was the first African American to serve as a governor in the United States?\n",
      "78. What is the longest word that can be typed using only the home row keys on a standard keyboard?\n",
      "79. What is the largest island in the Indian Ocean?\n",
      "80. Who was the first person to win a Nobel Prize in Chemistry?\n",
      "81. What is the largest living species of mammal?\n",
      "82. Who was the first woman to serve as a head of government in the United States?\n",
      "83. What is the deepest part of the ocean in the Northern Hemisphere?\n",
      "84. Which city has the most hospitals in the world?\n",
      "85. What is the largest snowman ever built in the United States?\n",
      "86. Who was the first person to win a Pulitzer Prize and a Grammy Award?\n",
      "87. What is the longest river in North America?\n",
      "88. Who was the first person to walk on the surface of Mars?\n",
      "89. What is the largest living species of insect?\n",
      "90. Who was the first woman to win a Nobel Prize in Physiology or Medicine?\n",
      "91. What is the highest mountain peak in Asia?\n",
      "92. Which city has the most restaurants in the world?\n",
      "93. What is the largest waterfall in the world by height in the Northern Hemisphere?\n",
      "94. Who was the first person to fly around the world in a single-engine plane?\n",
      "95. What is the largest living species of reptile?\n",
      "96. Who was the first African American to serve as a mayor of a major city in the United States?\n",
      "97. What is the longest word that can be typed using only the top row keys on a standard keyboard?\n",
      "98. What is the largest island in the Atlantic Ocean?\n",
      "99. Who was the first person to win a Nobel Prize in Literature and a Pulitzer Prize?\n",
      "100. What is the largest living species of amphibian?\n"
     ]
    }
   ],
   "source": [
    "#### evaluation \n",
    "\n",
    "# LLM for generating questions\n",
    "llm_generator = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0.6)\n",
    "\n",
    "# Question generation prompt\n",
    "question_gen_template = \"\"\"Generate exactly {num_questions} diverse and challenging questions that would require complex web searches to answer. The questions should:\n",
    "\n",
    "1. Cover a wide range of topics (e.g., science, history, current events, technology, arts)\n",
    "2. Avoid long questions\n",
    "3. Ensure there is only one question per query. Query should NOT be multiple questions\n",
    "\n",
    "Please provide the questions as a numbered list, starting from 1 and ending at {num_questions}.\n",
    "\n",
    "Generated Questions:\"\"\"\n",
    "\n",
    "question_gen_prompt = PromptTemplate.from_template(question_gen_template)\n",
    "\n",
    "def generate_questions(num_questions, max_attempts=3):\n",
    "    for attempt in range(max_attempts):\n",
    "        question_gen_chain = question_gen_prompt | llm_generator | StrOutputParser()\n",
    "        questions_text = question_gen_chain.invoke({\"num_questions\": num_questions})\n",
    "\n",
    "        questions = []\n",
    "        for line in questions_text.split('\\n'):\n",
    "            match = re.match(r'^\\s*\\d+\\.\\s*(.+)$', line)\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                questions.append(question)\n",
    "\n",
    "        if len(questions) == num_questions:\n",
    "            return questions\n",
    "\n",
    "        print(f\"Attempt {attempt + 1}: Generated {len(questions)} questions instead of {num_questions}. Retrying...\")\n",
    "\n",
    "    raise ValueError(f\"Failed to generate exactly {num_questions} questions after {max_attempts} attempts.\")\n",
    "\n",
    "# Generate questions\n",
    "num_questions = 100\n",
    "\n",
    "evaluation_questions = generate_questions(num_questions)\n",
    "print(f\"Successfully generated {len(evaluation_questions)} questions:\")\n",
    "for i, question in enumerate(evaluation_questions, 1):\n",
    "    print(f\"{i}. {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbd89660-138d-444c-b4b3-b2bfa2b019e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the average airspeed velocity of an unladen swallow?',\n",
       " 'Who is the oldest known living person with a verified birth certificate?',\n",
       " 'What is the chemical composition of the Great Wall of China?',\n",
       " 'Which country has the highest number of languages spoken?',\n",
       " 'What is the largest living organism in the world?',\n",
       " 'Who was the first person to walk on the moon?',\n",
       " 'What is the most widely spoken language in the world?',\n",
       " 'What is the highest mountain peak in the solar system?',\n",
       " 'Who is the author of the oldest known surviving work of literature?',\n",
       " 'What is the deepest part of the ocean?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evaluation_questions)\n",
    "evaluation_questions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1db54502-1c3f-4cc2-88d5-bb1a993bffd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://science.nasa.gov/climate-change/causes/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.epa.gov/ghgemissions/overview-greenhouse-gases \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.un.org/en/climatechange/science/causes-effects-climate-change \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nrdc.org/stories/greenhouse-effect-101 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nrdc.org/stories/what-are-causes-climate-change \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.un.org/en/climatechange/science/causes-effects-climate-change\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.87 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: To answer this question, we need to identify the primary drivers of climate change based on the provided context. The context mentions human activities as the main cause of climate change, specifically the expansion of the \"greenhouse effect\" due to human-made emissions in the atmosphere.\n",
      "\n",
      "Answer: The main causes of climate change are human activities, including the burning of fossil fuels, deforestation, and other industrial processes that release greenhouse gases such as carbon dioxide, methane, and nitrous oxide into the atmosphere. These gases trap heat and slow heat loss to space, leading to global warming and climate change.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 20 Documents Correctness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocs_correct\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_evaluation()\n",
      "Cell \u001b[0;32mIn[15], line 68\u001b[0m, in \u001b[0;36mrun_evaluation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_evaluation\u001b[39m():\n\u001b[1;32m     67\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the main causes of climate change?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 68\u001b[0m     answer, context_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_query(question, num_expansions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_urls\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_docs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, num_rerank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, rerank_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnano\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_70b_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     70\u001b[0m     all_docs \u001b[38;5;241m=\u001b[39m context_docs[:\u001b[38;5;241m150\u001b[39m]  \u001b[38;5;66;03m# All 80 retrieved documents\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     selected_docs \u001b[38;5;241m=\u001b[39m context_docs[:\u001b[38;5;241m20\u001b[39m]  \u001b[38;5;66;03m# Top 10 after reranking\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from langchain_fireworks import ChatFireworks\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Initialize the judge model (405B LLaMA)\n",
    "judge_model = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
    "\n",
    "def evaluate_answer_quality(question: str, answer: str, judge_model: Any) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer completely addresses the question.\n",
    "    Returns 1 if yes, 0 if no.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert evaluator. Your task is to determine if the given answer completely addresses the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \n",
    "    Does the answer completely address the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "def evaluate_document_selection(question: str, all_docs: List[str], selected_docs: List[str], judge_model: Any) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the selected documents are the best 20 out of the 150 to answer the question.\n",
    "    Returns 1 if yes, 0 if no.\n",
    "    \"\"\"\n",
    "    all_docs_text = \"\\n\".join([f\"{i+1}. {doc}...\" for i, doc in enumerate(all_docs)])\n",
    "    selected_indices = [all_docs.index(doc) + 1 for doc in selected_docs]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert information retrieval system. Your task is to determine if the selected documents are the best 20 out of the given 150 for answering the question completely.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Here are all 150 retrieved documents:\n",
    "    {all_docs_text}\n",
    "    \n",
    "    The system selected the following documents (by index): {', '.join(map(str, selected_indices))}\n",
    "    \n",
    "    Are these selected documents the best 20 out of the 150 for answering the question completely?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "async def evaluate_rag_system(question: str, answer: str, all_docs: List[str], selected_docs: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Evaluate the RAG system's performance.\n",
    "    \"\"\"\n",
    "    answer_correct = evaluate_answer_quality(question, answer, judge_model)\n",
    "    docs_correct = evaluate_document_selection(question, all_docs, selected_docs, judge_model)\n",
    "    \n",
    "    return {\n",
    "        \"answer_correct\": answer_correct,\n",
    "        \"docs_correct\": docs_correct\n",
    "    }\n",
    "\n",
    "async def run_evaluation():\n",
    "    question = \"What are the main causes of climate change?\"\n",
    "    answer, context_docs = await process_query(question, num_expansions=1, num_urls=5, num_docs=150, num_rerank=20, rerank_method=\"nano\", use_70b_model=False)\n",
    "    \n",
    "    all_docs = context_docs[:150]  # All 80 retrieved documents\n",
    "    selected_docs = context_docs[:20]  # Top 10 after reranking\n",
    "\n",
    "    random.shuffle(all_docs)\n",
    "    \n",
    "    evaluation_results = await evaluate_rag_system(question, answer, all_docs, selected_docs)\n",
    "    \n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Answer Correctness: {evaluation_results['answer_correct']}\")\n",
    "    print(f\"Top 20 Documents Correctness: {evaluation_results['docs_correct']}\")\n",
    "\n",
    "# Run the evaluation\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d4bfcd6-8f61-4b28-987e-e7f2e14a52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from langchain_fireworks import ChatFireworks\n",
    "\n",
    "# # Initialize the judge model (405B LLaMA)\n",
    "# judge_model = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
    "\n",
    "# def evaluate_answer_quality(question: str, answer: str) -> int:\n",
    "#     prompt = f\"\"\"\n",
    "#     You are an expert evaluator. Your task is to determine if the given answer completely addresses the question.\n",
    "    \n",
    "#     Question: {question}\n",
    "#     Answer: {answer}\n",
    "    \n",
    "#     Does the answer completely address the question?\n",
    "#     Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "#     Response:\n",
    "#     \"\"\"\n",
    "    \n",
    "#     response = judge_model.invoke(prompt)\n",
    "#     return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "# def evaluate_document_selection(question: str, all_docs: list, selected_docs: list) -> int:\n",
    "#     all_docs_text = \"\\n\".join([f\"{i+1}. {doc}...\" for i, doc in enumerate(all_docs)])\n",
    "#     selected_indices = [all_docs.index(doc) + 1 for doc in selected_docs]\n",
    "    \n",
    "#     prompt = f\"\"\"\n",
    "#     You are an expert information retrieval system. Your task is to determine if the selected documents are the best 10 out of the given 80 for answering the question completely.\n",
    "    \n",
    "#     Question: {question}\n",
    "    \n",
    "#     Here are the 80 retrieved documents:\n",
    "#     {all_docs_text}\n",
    "    \n",
    "#     The system selected the following documents (by index): {', '.join(map(str, selected_indices))}\n",
    "    \n",
    "#     Are these selected documents the best 10 out of the 80 for answering the question?\n",
    "#     Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "#     Response:\n",
    "#     \"\"\"\n",
    "    \n",
    "#     response = judge_model.invoke(prompt)\n",
    "#     return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "# async def run_evaluation(num_questions: int = 100):\n",
    "#     questions = evaluation_questions\n",
    "    \n",
    "#     results = []\n",
    "#     total_answer_correct = 0\n",
    "#     total_docs_correct = 0\n",
    "    \n",
    "#     for question in questions[:num_questions]:\n",
    "#         answer, context_docs = await process_query(question, num_expansions=1, num_urls=3, num_docs=80, num_rerank=10, rerank_method=\"nano\", use_70b_model=False)\n",
    "        \n",
    "#         all_docs = context_docs[:80]  # All 80 retrieved documents\n",
    "#         selected_docs = context_docs[:10]  # Top 10 after reranking\n",
    "\n",
    "#         random.shuffle(all_docs)\n",
    "        \n",
    "#         answer_correct = evaluate_answer_quality(question, answer)\n",
    "#         docs_correct = evaluate_document_selection(question, all_docs, selected_docs)\n",
    "        \n",
    "#         total_answer_correct += answer_correct\n",
    "#         total_docs_correct += docs_correct\n",
    "        \n",
    "#         result = {\n",
    "#             \"question\": question,\n",
    "#             \"answer\": answer,\n",
    "#             \"answer_correctness\": answer_correct,\n",
    "#             \"top_10_docs_correctness\": docs_correct,\n",
    "#             \"all_docs\": all_docs,\n",
    "#             \"selected_docs\": selected_docs\n",
    "#         }\n",
    "#         results.append(result)\n",
    "        \n",
    "#         print(f\"Question: {question}\")\n",
    "#         print(f\"Answer Correctness: {answer_correct}\")\n",
    "#         print(f\"Top 10 Documents Correctness: {docs_correct}\")\n",
    "#         print(\"---\")\n",
    "    \n",
    "#     avg_answer_correct = total_answer_correct / num_questions\n",
    "#     avg_docs_correct = total_docs_correct / num_questions\n",
    "#     print(f\"\\nAverage Results over {num_questions} questions:\")\n",
    "#     print(f\"Average Answer Correctness: {avg_answer_correct:.2f}\")\n",
    "#     print(f\"Average Top 10 Documents Correctness: {avg_docs_correct:.2f}\")\n",
    "    \n",
    "#     # Save results to a JSON file\n",
    "#     output = {\n",
    "#         \"results\": results,\n",
    "#         \"average_answer_correctness\": avg_answer_correct,\n",
    "#         \"average_top_10_docs_correctness\": avg_docs_correct\n",
    "#     }\n",
    "    \n",
    "#     with open('evaluation_results.json', 'w') as f:\n",
    "#         json.dump(output, f, indent=2)\n",
    "    \n",
    "#     print(\"\\nResults have been saved to 'evaluation_results.json'\")\n",
    "\n",
    "# # To run the evaluation, use:\n",
    "# # await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "540b507d-29b8-4976-aa40-2c5c3667172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        hyde_start = time.time()\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        hyde_time = time.time() - hyde_start\n",
    "        # print(f\"hypothetical_doc length: {len(hypothetical_doc)}\")\n",
    "        # print(f\"-----HyDE generation time: {hyde_time:.2f} seconds\")\n",
    "\n",
    "        embed_start = time.time()\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "        embed_time = time.time() - embed_start\n",
    "        # print(f\"-----Embedding time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        ext_start = time.time()\n",
    "        expanded_queries = query_expansion(query, num_expansions)\n",
    "        ext_time = time.time() - embed_start\n",
    "        # print(f\"-----Query expansion time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        scrape_start = time.time()\n",
    "        all_texts = await asyncio.gather(*[search_and_scrape(eq, num_urls) for eq in expanded_queries])\n",
    "        scrape_time = time.time() - scrape_start\n",
    "        # print(f\"-----Web scraping time: {scrape_time:.2f} seconds\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        # print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        sentence_windows = create_sentence_windows(combined_text)\n",
    "        # print(f\"Number of sentence windows: {len(sentence_windows)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=window) for window in sentence_windows]\n",
    "\n",
    "        vectorstore_start = time.time()\n",
    "        vectorstores = []\n",
    "        for i in range(0, len(index_documents), 256):\n",
    "            batch = index_documents[i:i + 256]\n",
    "\n",
    "            batch_embeddings = batch_embed_documents(batch)\n",
    "\n",
    "            texts = [doc.page_content for doc in batch]\n",
    "\n",
    "            vectorstore = FAISS.from_embeddings(\n",
    "                embedding=embeddings,\n",
    "                text_embeddings=list(zip(texts, batch_embeddings))\n",
    "            )\n",
    "            vectorstores.append(vectorstore)\n",
    "\n",
    "        vectorstore_time = time.time() - vectorstore_start\n",
    "        # print(f\"-----Vectorstore creation time: {vectorstore_time:.2f} seconds\")\n",
    "\n",
    "        retrieval_start = time.time()\n",
    "        retriever = get_hyde_retriever(vectorstores, hyde_embedding, num_docs, num_rerank, rerank_method)\n",
    "        retrieved_docs = retriever(query)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        # print(f\"-----Retrieval and reranking time: {retrieval_time:.2f} seconds\")\n",
    "\n",
    "        # print(f\"Number of retrieved and reranked documents: {len(retrieved_docs)}\")\n",
    "\n",
    "        context_docs = [doc.page_content for doc in retrieved_docs]\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "\n",
    "        total_processing_time = hyde_time + embed_time + scrape_time + vectorstore_time + retrieval_time\n",
    "        # print(f\"-----Total processing time before answer generation: {total_processing_time:.2f} seconds\")\n",
    "\n",
    "        answer_start = time.time()\n",
    "        prompt_template = \"\"\"\n",
    "        Use the following context to answer the question. Before answering the question generate a reasoning step. then answer.\n",
    "        If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        chosen_llm = llm_70b if use_70b_model else llm_8b\n",
    "\n",
    "        rag_chain = prompt | chosen_llm | StrOutputParser()\n",
    "        answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "        answer_time = time.time() - answer_start\n",
    "        print(f\"-----Answer generation time: {answer_time:.2f} seconds\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*120)\n",
    "        print(\"Final Answer:\\n\", answer)\n",
    "        print(\"-\"*120)\n",
    "\n",
    "        return answer, context_docs, [hyde_time, embed_time, ext_time, scrape_time, vectorstore_time, retrieval_time, total_processing_time, answer_time]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b9a25-d87d-4903-bf89-2412a8ab7128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating nano reranker:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[AINFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://medium.com/human-nature-group/what-is-the-air-speed-velocity-of-an-unladen-swallow-4c17087bbf33 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://medium.com/human-nature-group/what-is-the-air-speed-velocity-of-an-unladen-swallow-4c17087bbf33 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.youtube.com/watch?v=pJS4QDUtzzI \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-airspeed-velocity-of-an-unladen-swallow-1 \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 429 while requesting https://www.quora.com/What-is-the-airspeed-velocity-of-an-unladen-swallow-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://interestingengineering.com/science/monty-python-and-the-holy-grail-airspeed-velocity-of-an-unladen-swallow \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://interestingengineering.com/science/monty-python-and-the-holy-grail-airspeed-velocity-of-an-unladen-swallow \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 1.42 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " To answer the question, we need to follow the reasoning steps outlined in the context. Here's the reasoning step:\n",
      "\n",
      "1. Identify the species of swallow: The article mentions that the European (or 'Barn') swallow is the most studied species and will be used for the calculation.\n",
      "2. Determine the average mass and wing-length of the European swallow: The article states that the average mass is 20.3g and the wing-length is 0.122m (12.2cm).\n",
      "3. Estimate the amplitude and frequency of the swallow's wingbeats: The article mentions that the amplitude and frequency of the swallow's wingbeats have not been extensively studied, so estimates will be made based on similar birds.\n",
      "4. Use the Strouhal ratio to estimate the airspeed velocity: The Strouhal ratio is an equation that estimates the airspeed velocity of a bird based on the frequency of wingbeats, amplitude, and airspeed.\n",
      "\n",
      "Now, let's answer the question:\n",
      "\n",
      "The average airspeed velocity of an unladen European swallow is estimated to be 11-16 meters per second, which is equivalent to 40-60 kilometers per hour.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Evaluating nano reranker:   1%|          | 1/100 [00:11<19:25, 11.78s/it]\u001b[AINFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/84549-oldest-person-living \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/84549-oldest-person-living \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Oldest_people \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/84549-oldest-person-living\n",
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/84549-oldest-person-living\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_the_verified_oldest_people \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.demogr.mpg.de/books/odense/6/10.htm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 1.47 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " To answer the question, I will first generate a reasoning step.\n",
      "\n",
      "Reasoning step: The text mentions that the oldest known living person is Maria Branyas of Spain, who is 117 years old. However, it does not explicitly state that she has a verified birth certificate. To determine if she has a verified birth certificate, I will look for information in the text that suggests she has undergone age verification.\n",
      "\n",
      "After reviewing the text, I found that Maria Branyas is listed as the oldest known living person, but I did not find any information that explicitly states she has a verified birth certificate. However, the text does mention that the list includes supercentenarians validated by organisations specialising in extreme age verification such as the Gerontology Research Group (GRG), which suggests that Maria Branyas' age has been verified.\n",
      "\n",
      "Therefore, based on the information provided, I will answer the question as follows:\n",
      "\n",
      "Answer: Maria Branyas of Spain, who is 117 years old, is the oldest known living person, and her age has been verified by the Gerontology Research Group (GRG).\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Evaluating nano reranker:   2%|         | 2/100 [00:23<18:42, 11.45s/it]\u001b[AINFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/great-wall-china/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.travelchinaguide.com/china_great_wall/construction/material.htm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Great_Wall_of_China \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.chinahighlights.com/greatwall/construction-materials.htm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.worldatlas.com/articles/what-materials-were-used-to-build-the-great-wall-of-china.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.66 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " I don't have enough information to answer that question.\n",
      "\n",
      "Reasoning step: The context provided discusses the physical materials used to build the Great Wall of China, such as earth, stone, brick, lime, and wood, but it does not provide information on the chemical composition of the materials used. To answer the question, I would need information on the specific chemical compounds present in the materials used to build the Great Wall, which is not provided in the context.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Evaluating nano reranker:   3%|         | 3/100 [00:32<16:48, 10.40s/it]\u001b[AINFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.weforum.org/agenda/2023/04/worlds-most-multilingual-countries/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.weforum.org/agenda/2021/03/these-are-the-top-ten-countries-for-linguistic-diversity/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Number_of_languages_by_country \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.weforum.org/agenda/2023/04/worlds-most-multilingual-countries/\n",
      "Error response 403 while requesting https://www.weforum.org/agenda/2021/03/these-are-the-top-ten-countries-for-linguistic-diversity/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating none reranker:   1%|          | 1/100 [7:51:21<777:44:40, 28281.62s/it]\n",
      "INFO:httpx:HTTP Request: GET https://www.statista.com/statistics/1224629/the-most-linguistically-diverse-countries-worldwide-by-number-of-languages/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://www.visualcapitalist.com/the-countries-with-the-most-linguistic-diversity/: \n",
      "An error occurred while requesting https://www.visualcapitalist.com/the-countries-with-the-most-linguistic-diversity/: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.63 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Based on the context, the answer is Papua New Guinea, with 840 different languages spoken across the country as of 2021.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Evaluating nano reranker:   4%|         | 4/100 [00:51<22:15, 13.91s/it]\u001b[AINFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Largest_organisms \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Largest_organisms \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.scientificamerican.com/article/strange-but-true-largest-organism-is-fungus/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://frontenacarchbiosphere.ca/worlds-largest-organism/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.novausawood.com/pando-largest-living-organism \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.montrealsciencecentre.com/blog/the-two-largest-living-organisms-on-earth \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.78 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the largest living organism in the world, and the context provides information about various contenders for this title, including a fungus known as Armillaria solidipes (Honey fungus) and an Aspen Tree colony known as Pando. To answer the question, I need to identify which of these contenders is considered the largest living organism.\n",
      "\n",
      "Answer: The largest living organism in the world is the Armillaria solidipes (Honey fungus), which spans 5.5 kilometres across and covers about 2,384 acres.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Evaluating nano reranker:   5%|         | 5/100 [01:00<19:21, 12.23s/it]\u001b[AINFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Apollo_11 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nasa.gov/history/apollo-11-mission-overview/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nasa.gov/learning-resources/for-kids-and-students/who-was-neil-armstrong-grades-5-8/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.rmg.co.uk/stories/topics/how-many-people-have-walked-on-moon \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.smithsonianmag.com/travel/going-moon-apollo-11-astronauts-trained-these-five-sites-180972452/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sciencefocus.com/space/everything-you-ever-wanted-to-know-about-the-apollo-programme \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.73 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Before answering the question, I will generate a reasoning step. \n",
      "\n",
      "The question asks for the identity of the first person to walk on the moon. The context provided describes the Apollo 11 mission and the events surrounding the first moon landing. It mentions Neil Armstrong as the first person to step out of the lunar module and onto the moon's surface. \n",
      "\n",
      "Based on this information, I can conclude that Neil Armstrong was the first person to walk on the moon.\n",
      "\n",
      "Answer: Neil Armstrong.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Evaluating nano reranker:   6%|         | 6/100 [01:12<18:43, 11.95s/it]\u001b[AINFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.visualcapitalist.com/top-languages-spoken-in-the-world/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.visualcapitalist.com/top-languages-spoken-in-the-world/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.berlitz.com/blog/most-spoken-languages-world \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/languages-by-number-of-native-speakers-2228882 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.babbel.com/en/magazine/the-10-most-spoken-languages-in-the-world \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.50 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Based on the context, the most widely spoken language in the world is English, with 1.4+ billion speakers, including both native and non-native speakers.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Evaluating nano reranker:   7%|         | 7/100 [01:21<17:26, 11.25s/it]\u001b[AINFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_tallest_mountains_in_the_Solar_System \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_tallest_mountains_in_the_Solar_System \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://science.howstuffworks.com/tallest-mountain-in-solar-system.htm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.facebook.com/justlearning/videos/the-highest-mountain-in-our-solar-system-olympus-mons-on-mars-revealed/1252992958707588/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.jpl.nasa.gov/edu/learn/video/mars-in-a-minute-how-did-mars-get-such-enormous-mountains/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://coolcosmos.ipac.caltech.edu/ask/199-Where-is-the-highest-mountain-in-our-Solar-System- \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.66 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the highest mountain peak in the solar system, which requires identifying the tallest mountain among all the planets and moons in the solar system.\n",
      "\n",
      "Answer: The highest mountain peak in the solar system is Olympus Mons on Mars, with a height of 21.9 to 26 km (13.6 to 16.2 miles) above the Martian surface.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Evaluating nano reranker:   8%|         | 8/100 [01:30<15:50, 10.34s/it]\u001b[AINFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.history.com/news/what-is-the-oldest-known-piece-of-literature \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.history.com/news/what-is-the-oldest-known-piece-of-literature \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/story/what-was-the-first-book-ever-written \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/story/what-was-the-first-book-ever-written \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mentalfloss.com/posts/oldest-literature \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://yalebooks.yale.edu/2020/04/30/the-epic-of-gilgamesh/ \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "async def run_evaluation(num_questions: int = 100):\n",
    "    questions = evaluation_questions\n",
    "    rerankers = [\"nano\", \"small\", \"medium_t5\", \"medium_multilang\"]#[\"none\", \"llm\", \"nano\", \"small\", \"medium_t5\", \"medium_multilang\"]\n",
    "    \n",
    "    for reranker in rerankers:\n",
    "        results = []\n",
    "        total_answer_correct = 0\n",
    "        total_docs_correct = 0\n",
    "        total_times = [0] * 8  # For the 8 time measurements\n",
    "        \n",
    "        progress_bar = tqdm(total=num_questions, desc=f\"Evaluating {reranker} reranker\")\n",
    "        \n",
    "        for question in questions[:num_questions]:\n",
    "            answer, context_docs, times = await process_query(question, num_expansions=1, num_urls=5, num_docs=150, num_rerank=20, rerank_method=reranker, use_70b_model=False)\n",
    "            \n",
    "            all_docs = context_docs[:150]\n",
    "            selected_docs = context_docs[:20]\n",
    "            \n",
    "            answer_correct = evaluate_answer_quality(question, answer, judge_model)\n",
    "            docs_correct = evaluate_document_selection(question, all_docs, selected_docs, judge_model)\n",
    "            \n",
    "            total_answer_correct += answer_correct\n",
    "            total_docs_correct += docs_correct\n",
    "            total_times = [total + t for total, t in zip(total_times, times)]\n",
    "            \n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"answer_correctness\": answer_correct,\n",
    "                \"top_10_docs_correctness\": docs_correct,\n",
    "                \"all_docs\": all_docs,\n",
    "                \"selected_docs\": selected_docs,\n",
    "                \"times\": times\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        progress_bar.close()\n",
    "        \n",
    "        avg_answer_correct = total_answer_correct / num_questions\n",
    "        avg_docs_correct = total_docs_correct / num_questions\n",
    "        avg_times = [t / num_questions for t in total_times]\n",
    "        \n",
    "        print(f\"\\nAverage Results for {reranker} reranker over {num_questions} questions:\")\n",
    "        print(f\"Average Answer Correctness: {avg_answer_correct:.2f}\")\n",
    "        print(f\"Average Top 10 Documents Correctness: {avg_docs_correct:.2f}\")\n",
    "        print(f\"Average HyDE Time: {avg_times[0]:.2f} seconds\")\n",
    "        print(f\"Average Embedding Time: {avg_times[1]:.2f} seconds\")\n",
    "        print(f\"Average Query Expansion Time: {avg_times[2]:.2f} seconds\")\n",
    "        print(f\"Average Web Scraping Time: {avg_times[3]:.2f} seconds\")\n",
    "        print(f\"Average Vectorstore Creation Time: {avg_times[4]:.2f} seconds\")\n",
    "        print(f\"Average Retrieval and Reranking Time: {avg_times[5]:.2f} seconds\")\n",
    "        print(f\"Average Total Processing Time: {avg_times[6]:.2f} seconds\")\n",
    "        print(f\"Average Answer Generation Time: {avg_times[7]:.2f} seconds\")\n",
    "        \n",
    "        output = {\n",
    "            \"results\": results,\n",
    "            \"average_answer_correctness\": avg_answer_correct,\n",
    "            \"average_top_10_docs_correctness\": avg_docs_correct,\n",
    "            \"average_times\": {\n",
    "                \"hyde_time\": avg_times[0],\n",
    "                \"embedding_time\": avg_times[1],\n",
    "                \"query_expansion_time\": avg_times[2],\n",
    "                \"web_scraping_time\": avg_times[3],\n",
    "                \"vectorstore_creation_time\": avg_times[4],\n",
    "                \"retrieval_and_reranking_time\": avg_times[5],\n",
    "                \"total_processing_time\": avg_times[6],\n",
    "                \"answer_generation_time\": avg_times[7]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        filename = f'/home/ubuntu/maziar/11_efficient_eval_ranking/evaluation/{reranker}.json'\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults have been saved to '{filename}'\")\n",
    "\n",
    "# To run the evaluation, use:\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e54597-3393-47ce-ac63-a49878c4ef6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbdff1a-7fcf-4c19-9fea-9189aaef7970",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (eval_ranking)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
