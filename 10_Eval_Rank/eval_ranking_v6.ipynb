{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/maziar/eval_ranking/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_fireworks import FireworksEmbeddings, ChatFireworks\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "from typing import List, Tuple, Any\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import faiss\n",
    "import httpx\n",
    "from urllib.parse import urlparse\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU will be used automatically by FlashRank.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/ubuntu/.flashrank_cache/rank_zephyr_7b_v1_full/rank_zephyr_7b_v1_full.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = hub\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    98.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'hub', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set up API clients\n",
    "os.environ['FIREWORKS_API_KEY'] = 'API'\n",
    "# os.environ[\"SERPER_API_KEY\"] = 'API'\n",
    "os.environ[\"SERPER_API_KEY\"] = 'API'\n",
    "\n",
    "# Initialize components\n",
    "search = GoogleSerperAPIWrapper(k=3)\n",
    "embeddings = FireworksEmbeddings(model=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_8b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_70b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "# Create a directory for caching in the user's home folder\n",
    "cache_dir = Path.home() / \".flashrank_cache\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU will be used automatically by FlashRank.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. CPU will be used.\")\n",
    "\n",
    "# Initialize FlashRank rerankers\n",
    "ranker_nano = Ranker(cache_dir=str(cache_dir))\n",
    "ranker_small = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=str(cache_dir))\n",
    "ranker_medium_t5 = Ranker(model_name=\"rank-T5-flan\", cache_dir=str(cache_dir))\n",
    "ranker_medium_multilang = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=str(cache_dir))\n",
    "ranker_large = Ranker(model_name=\"rank_zephyr_7b_v1_full\", max_length=1024, cache_dir=str(cache_dir))\n",
    "\n",
    "# Ensure models are on GPU if available\n",
    "for ranker in [ranker_nano, ranker_small, ranker_medium_t5, ranker_medium_multilang, ranker_large]:\n",
    "    if hasattr(ranker, 'model') and hasattr(ranker.model, 'to'):\n",
    "        ranker.model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Download NLTK data\n",
    "# nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "async def scrape_webpage(client, url):\n",
    "    try:\n",
    "        response = await client.get(url, timeout=3.0)\n",
    "        response.raise_for_status()\n",
    "        text = response.text\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        content = ' '.join(soup.stripped_strings)\n",
    "        return content[:5000], len(content[:5000])\n",
    "    except (httpx.RequestError, httpx.TimeoutException) as exc:\n",
    "        print(f\"An error occurred while requesting {url}: {exc}\")\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(f\"Error response {exc.response.status_code} while requesting {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return \"\", 0\n",
    "\n",
    "async def search_and_scrape(query, num_urls):\n",
    "    search_results = search.results(query)\n",
    "    scraped_urls = set()\n",
    "    full_texts = []\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=httpx.Timeout(10.0, connect=3.0)) as client:\n",
    "        tasks = []\n",
    "        if 'organic' in search_results:\n",
    "            for result in search_results['organic']:\n",
    "                url = result.get('link')\n",
    "                domain = urlparse(url).netloc if url else None\n",
    "                if url and domain not in scraped_urls and len(tasks) < num_urls:\n",
    "                    tasks.append(scrape_webpage(client, url))\n",
    "                    scraped_urls.add(domain)\n",
    "\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        for result in results:\n",
    "            if isinstance(result, tuple) and result[1] > 0:\n",
    "                full_texts.append(result[0])\n",
    "\n",
    "    return \" \".join(full_texts)\n",
    "\n",
    "def query_expansion(query, num_expansions):\n",
    "    expansion_prompt = f\"\"\"\n",
    "    Given the following search query, generate {num_expansions} additional related queries that could help find more comprehensive information on the topic. The queries should be different from each other and explore various aspects of the main query. Provide only the additional queries, numbered 1-{num_expansions}.\n",
    "\n",
    "    Main query: {query}\n",
    "\n",
    "    Additional queries:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(expansion_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    expanded_queries = [query]\n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.strip() and line[0].isdigit():\n",
    "            expanded_queries.append(line.split('. ', 1)[1].strip())\n",
    "\n",
    "    return expanded_queries[:num_expansions + 1]\n",
    "\n",
    "def create_sentence_windows(text, window_size=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    windows = []\n",
    "    for i in range(len(sentences)):\n",
    "        window = \" \".join(sentences[max(0, i-window_size):min(len(sentences), i+window_size+1)])\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def generate_hypothetical_document(query):\n",
    "    hyde_prompt = f\"\"\"\n",
    "    Given the search query below, generate a hypothetical document that would be a perfect match for this query. The document should be concise, containing only 3 sentences of relevant information that directly addresses the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Document (3 sentences):\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def llm_rerank(query, documents):\n",
    "    rerank_prompt = \"\"\"\n",
    "    Given the following query and a list of document excerpts, rank the documents based on their relevance to the query. Provide the rankings as a list of numbers from 1 to {}, where 1 is the most relevant. Ensure you provide a ranking for every document.\n",
    "\n",
    "    Query: {}\n",
    "\n",
    "    Documents:\n",
    "    {}\n",
    "\n",
    "    Rankings (1 to {}):\n",
    "    \"\"\".format(len(documents), query, \"\\n\".join([f\"{i+1}. {doc.page_content[:200]}...\" for i, doc in enumerate(documents)]), len(documents))\n",
    "\n",
    "    response = llm.invoke(rerank_prompt)\n",
    "    rankings = [int(x) for x in response.content.split() if x.isdigit()]\n",
    "\n",
    "    if len(rankings) < len(documents):\n",
    "        remaining = set(range(1, len(documents) + 1)) - set(rankings)\n",
    "        rankings.extend(remaining)\n",
    "\n",
    "    sorted_docs = sorted(zip(documents, rankings), key=lambda x: x[1])\n",
    "    return sorted_docs\n",
    "\n",
    "def flashrank_rerank(query, documents, ranker):\n",
    "    rerank_request = RerankRequest(\n",
    "        query=query,\n",
    "        passages=[{\"text\": doc.page_content} for doc in documents]\n",
    "    )\n",
    "    reranked = ranker.rerank(rerank_request)\n",
    "    \n",
    "    if isinstance(reranked, list) and isinstance(reranked[0], dict):\n",
    "        sorted_results = sorted(reranked, key=lambda x: x.get('score', 0), reverse=True)\n",
    "        return [(documents[i], result.get('score', 0)) for i, result in enumerate(sorted_results)]\n",
    "    \n",
    "    elif isinstance(reranked, list) and hasattr(reranked[0], 'score'):\n",
    "        sorted_results = sorted(reranked, key=lambda x: x.score, reverse=True)\n",
    "        return [(documents[i], result.score) for i, result in enumerate(sorted_results)]\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unexpected reranked result type. Using original document order.\")\n",
    "        return [(doc, 1.0) for doc in documents]\n",
    "\n",
    "def get_hyde_retriever(vectorstores, hyde_embedding, num_docs, num_rerank, rerank_method):\n",
    "    def retriever(query):\n",
    "        all_docs = []\n",
    "        for vectorstore in vectorstores:\n",
    "            docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs)\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        unique_docs = []\n",
    "        seen_content = set()\n",
    "        for doc in all_docs:\n",
    "            content = doc.page_content\n",
    "            if content not in seen_content:\n",
    "                unique_docs.append(Document(page_content=content))\n",
    "                seen_content.add(content)\n",
    "\n",
    "        try:\n",
    "            if rerank_method == \"none\":\n",
    "                return unique_docs[:num_rerank]\n",
    "            elif rerank_method == \"llm\":\n",
    "                reranked_docs = llm_rerank(query, unique_docs)\n",
    "            elif rerank_method in [\"nano\", \"small\", \"medium_t5\", \"medium_multilang\", \"large\"]:\n",
    "                ranker = globals()[f\"ranker_{rerank_method}\"]\n",
    "                reranked_docs = flashrank_rerank(query, unique_docs, ranker)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown rerank method: {rerank_method}\")\n",
    "\n",
    "            return [doc for doc, _ in reranked_docs[:num_rerank]]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reranking with method {rerank_method}: {str(e)}\")\n",
    "            print(\"Traceback:\", traceback.format_exc())\n",
    "            print(\"Falling back to no reranking.\")\n",
    "            return unique_docs[:num_rerank]\n",
    "\n",
    "    return retriever\n",
    "\n",
    "def batch_embed_documents(documents, batch_size=512):\n",
    "    batched_embeddings = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        texts = [doc.page_content for doc in batch]\n",
    "        embeddings_batch = embeddings.embed_documents(texts)\n",
    "        batched_embeddings.extend(embeddings_batch)\n",
    "    return batched_embeddings\n",
    "\n",
    "async def process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        hyde_start = time.time()\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        hyde_time = time.time() - hyde_start\n",
    "        print(f\"hypothetical_doc length: {len(hypothetical_doc)}\")\n",
    "        print(f\"-----HyDE generation time: {hyde_time:.2f} seconds\")\n",
    "\n",
    "        embed_start = time.time()\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "        embed_time = time.time() - embed_start\n",
    "        print(f\"-----Embedding time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        ext_start = time.time()\n",
    "        expanded_queries = query_expansion(query, num_expansions)\n",
    "        ext_time = time.time() - embed_start\n",
    "        print(f\"-----Query expansion time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        scrape_start = time.time()\n",
    "        all_texts = await asyncio.gather(*[search_and_scrape(eq, num_urls) for eq in expanded_queries])\n",
    "        scrape_time = time.time() - scrape_start\n",
    "        print(f\"-----Web scraping time: {scrape_time:.2f} seconds\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        sentence_windows = create_sentence_windows(combined_text)\n",
    "        print(f\"Number of sentence windows: {len(sentence_windows)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=window) for window in sentence_windows]\n",
    "\n",
    "        vectorstore_start = time.time()\n",
    "        vectorstores = []\n",
    "        for i in range(0, len(index_documents), 256):\n",
    "            batch = index_documents[i:i + 256]\n",
    "\n",
    "            batch_embeddings = batch_embed_documents(batch)\n",
    "\n",
    "            texts = [doc.page_content for doc in batch]\n",
    "\n",
    "            vectorstore = FAISS.from_embeddings(\n",
    "                embedding=embeddings,\n",
    "                text_embeddings=list(zip(texts, batch_embeddings))\n",
    "            )\n",
    "            vectorstores.append(vectorstore)\n",
    "\n",
    "        vectorstore_time = time.time() - vectorstore_start\n",
    "        print(f\"-----Vectorstore creation time: {vectorstore_time:.2f} seconds\")\n",
    "\n",
    "        retrieval_start = time.time()\n",
    "        retriever = get_hyde_retriever(vectorstores, hyde_embedding, num_docs, num_rerank, rerank_method)\n",
    "        retrieved_docs = retriever(query)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        print(f\"-----Retrieval and reranking time: {retrieval_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"Number of retrieved and reranked documents: {len(retrieved_docs)}\")\n",
    "\n",
    "        context_docs = [doc.page_content for doc in retrieved_docs]\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "\n",
    "        total_processing_time = hyde_time + embed_time + scrape_time + vectorstore_time + retrieval_time\n",
    "        print(f\"-----Total processing time before answer generation: {total_processing_time:.2f} seconds\")\n",
    "\n",
    "        answer_start = time.time()\n",
    "        prompt_template = \"\"\"\n",
    "        Use the following context to answer the question. Before answering the question generate a reasoning step. then answer.\n",
    "        If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        chosen_llm = llm_70b if use_70b_model else llm_8b\n",
    "\n",
    "        rag_chain = prompt | chosen_llm | StrOutputParser()\n",
    "        answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "        answer_time = time.time() - answer_start\n",
    "        print(f\"-----Answer generation time: {answer_time:.2f} seconds\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*120)\n",
    "        print(\"Final Answer:\\n\", answer)\n",
    "        print(\"-\"*120)\n",
    "\n",
    "        return answer, context_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\", []\n",
    "\n",
    "def gradio_interface(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "    answer, context_docs = asyncio.run(process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model))\n",
    "\n",
    "    sys.stdout = old_stdout\n",
    "    captured_output = buffer.getvalue()\n",
    "\n",
    "    truncated_docs = [f\"Document {i+1}: {doc[:150]}...\" for i, doc in enumerate(context_docs)]\n",
    "    truncated_context = \"\\n\\n\".join(truncated_docs)\n",
    "\n",
    "    captured_output += f\"\\n\\nContext used for answer generation (first 150 characters of each document, {len(context_docs)} documents in total):\\n\" + truncated_context\n",
    "\n",
    "    return captured_output\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your query\"),\n",
    "        gr.Slider(minimum=0, maximum=3, value=1, step=1, label=\"Number of query expansions\"),\n",
    "        gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of URLs to scrape per extended query\"),\n",
    "        gr.Slider(minimum=20, maximum=80, value=80, step=1, label=\"Number of documents to retrieve with HyDE\"),\n",
    "        gr.Slider(minimum=10, maximum=80, value=50, step=1, label=\"Number of documents to keep after retrieval/reranking\"),\n",
    "        gr.Radio([\"none\", \"llm\", \"nano\", \"small\", \"medium_t5\", \"medium_multilang\"], label=\"Reranking method\", value=\"none\"),\n",
    "        gr.Checkbox(label=\"Use 70B model for QA (unchecked uses 8B)\", value=False)\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Advanced RAG Query Processing\",\n",
    "    description=\"Enter a query and adjust parameters to get a detailed answer based on web search and document analysis.\"\n",
    ")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     iface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 100 questions:\n",
      "1. What is the average airspeed velocity of an unladen swallow?\n",
      "2. Which ancient civilization built the first known suspension bridge?\n",
      "3. What is the most widely spoken language in the world?\n",
      "4. Who is the author of the famous painting \"The Ambassadors\"?\n",
      "5. What is the chemical composition of the human nose?\n",
      "6. In what year did the first computer bug occur?\n",
      "7. Who is the founder of the philosophical school of Stoicism?\n",
      "8. What is the world's largest living structure, according to the Guinness World Records?\n",
      "9. Which planet in our solar system has the longest day?\n",
      "10. Who wrote the famous poem \"The Love Song of J. Alfred Prufrock\"?\n",
      "11. What is the highest recorded temperature on Earth?\n",
      "12. Who is the inventor of the first successful polio vaccine?\n",
      "13. What is the name of the largest star known to science?\n",
      "14. In what year did the first email send?\n",
      "15. Who is the author of the famous novel \"One Hundred Years of Solitude\"?\n",
      "16. What is the deepest part of the ocean?\n",
      "17. Who is the founder of the company that developed the first smartphone?\n",
      "18. What is the most widely used programming language in the world?\n",
      "19. Who is the lead singer of the rock band Queen?\n",
      "20. What is the chemical symbol for gold?\n",
      "21. In what year did the first human walk on the moon?\n",
      "22. Who is the author of the famous play \"Romeo and Juliet\"?\n",
      "23. What is the world's largest waterfall, by volume of water?\n",
      "24. Who is the founder of the company that developed the first web browser?\n",
      "25. What is the highest mountain peak in the solar system?\n",
      "26. Who is the lead singer of the rock band Guns N' Roses?\n",
      "27. What is the most widely spoken language in the United States?\n",
      "28. Who is the author of the famous novel \"To Kill a Mockingbird\"?\n",
      "29. What is the chemical composition of the human brain?\n",
      "30. In what year did the first successful heart transplant occur?\n",
      "31. Who is the founder of the company that developed the first microprocessor?\n",
      "32. What is the world's largest desert?\n",
      "33. Who is the lead singer of the rock band U2?\n",
      "34. What is the highest recorded wind speed on Earth?\n",
      "35. Who is the author of the famous poem \"The Road Not Taken\"?\n",
      "36. What is the chemical symbol for silver?\n",
      "37. In what year did the first human fly in space?\n",
      "38. Who is the founder of the company that developed the first personal computer?\n",
      "39. What is the world's largest island?\n",
      "40. Who is the lead singer of the rock band AC/DC?\n",
      "41. What is the most widely used social media platform in the world?\n",
      "42. Who is the author of the famous novel \"The Great Gatsby\"?\n",
      "43. What is the chemical composition of the human eye?\n",
      "44. In what year did the first successful kidney transplant occur?\n",
      "45. Who is the founder of the company that developed the first laser printer?\n",
      "46. What is the world's largest mountain range?\n",
      "47. Who is the lead singer of the rock band Led Zeppelin?\n",
      "48. What is the highest recorded temperature on Mars?\n",
      "49. Who is the author of the famous play \"Hamlet\"?\n",
      "50. What is the chemical symbol for copper?\n",
      "51. In what year did the first human walk on the moon's surface?\n",
      "52. Who is the founder of the company that developed the first ATM?\n",
      "53. What is the world's largest river, by discharge volume?\n",
      "54. Who is the lead singer of the rock band Pink Floyd?\n",
      "55. What is the most widely used database management system in the world?\n",
      "56. Who is the author of the famous novel \"The Catcher in the Rye\"?\n",
      "57. What is the chemical composition of the human skin?\n",
      "58. In what year did the first successful liver transplant occur?\n",
      "59. Who is the founder of the company that developed the first smartphone app store?\n",
      "60. What is the world's largest waterfall, by height?\n",
      "61. Who is the lead singer of the rock band The Rolling Stones?\n",
      "62. What is the highest recorded wind speed on Mars?\n",
      "63. Who is the author of the famous poem \"The Waste Land\"?\n",
      "64. What is the chemical symbol for tin?\n",
      "65. In what year did the first human fly in a hot air balloon?\n",
      "66. Who is the founder of the company that developed the first email client?\n",
      "67. What is the world's largest lake, by surface area?\n",
      "68. Who is the lead singer of the rock band Aerosmith?\n",
      "69. What is the most widely used web framework in the world?\n",
      "70. Who is the author of the famous novel \"The Picture of Dorian Gray\"?\n",
      "71. What is the chemical composition of the human hair?\n",
      "72. In what year did the first successful bone marrow transplant occur?\n",
      "73. Who is the founder of the company that developed the first 3D printer?\n",
      "74. What is the world's largest mountain peak, by base-to-peak height?\n",
      "75. Who is the lead singer of the rock band Van Halen?\n",
      "76. What is the highest recorded temperature on Venus?\n",
      "77. Who is the author of the famous play \"Macbeth\"?\n",
      "78. What is the chemical symbol for lead?\n",
      "79. In what year did the first human walk on the moon's surface?\n",
      "80. Who is the founder of the company that developed the first GPS device?\n",
      "81. What is the world's largest river, by length?\n",
      "82. Who is the lead singer of the rock band Bon Jovi?\n",
      "83. What is the most widely used operating system in the world?\n",
      "84. Who is the author of the famous novel \"The Adventures of Huckleberry Finn\"?\n",
      "85. What is the chemical composition of the human nail?\n",
      "86. In what year did the first successful lung transplant occur?\n",
      "87. Who is the founder of the company that developed the first virtual reality headset?\n",
      "88. What is the world's largest waterfall, by volume of water?\n",
      "89. Who is the lead singer of the rock band Queen?\n",
      "90. What is the highest recorded wind speed on Earth?\n",
      "91. Who is the author of the famous poem \"The Raven\"?\n",
      "92. What is the chemical symbol for mercury?\n",
      "93. In what year did the first human fly in a jet-powered aircraft?\n",
      "94. Who is the founder of the company that developed the first smartphone with a touchscreen?\n",
      "95. What is the world's largest island, by population?\n",
      "96. Who is the lead singer of the rock band Guns N' Roses?\n",
      "97. What is the most widely used programming language in the world?\n",
      "98. Who is the author of the famous novel \"The Scarlet Letter\"?\n",
      "99. What is the chemical composition of the human tooth?\n",
      "100. In what year did the first human walk on the moon's surface?\n"
     ]
    }
   ],
   "source": [
    "#### evaluation \n",
    "\n",
    "# LLM for generating questions\n",
    "llm_generator = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0.6)\n",
    "\n",
    "# Question generation prompt\n",
    "question_gen_template = \"\"\"Generate exactly {num_questions} diverse and challenging questions that would require complex web searches to answer. The questions should:\n",
    "\n",
    "1. Cover a wide range of topics (e.g., science, history, current events, technology, arts, code)\n",
    "2. Include some questions that are easy to search and find solutions for\n",
    "3. Avoid long questions\n",
    "4. Include some easy factual questions in the list\n",
    "5. Ensure there is only one question per query. Query should NOT be multiple questions\n",
    "\n",
    "Please provide the questions as a numbered list, starting from 1 and ending at {num_questions}.\n",
    "\n",
    "Generated Questions:\"\"\"\n",
    "\n",
    "question_gen_prompt = PromptTemplate.from_template(question_gen_template)\n",
    "\n",
    "def generate_questions(num_questions, max_attempts=3):\n",
    "    for attempt in range(max_attempts):\n",
    "        question_gen_chain = question_gen_prompt | llm_generator | StrOutputParser()\n",
    "        questions_text = question_gen_chain.invoke({\"num_questions\": num_questions})\n",
    "\n",
    "        questions = []\n",
    "        for line in questions_text.split('\\n'):\n",
    "            match = re.match(r'^\\s*\\d+\\.\\s*(.+)$', line)\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                questions.append(question)\n",
    "\n",
    "        if len(questions) == num_questions:\n",
    "            return questions\n",
    "\n",
    "        print(f\"Attempt {attempt + 1}: Generated {len(questions)} questions instead of {num_questions}. Retrying...\")\n",
    "\n",
    "    raise ValueError(f\"Failed to generate exactly {num_questions} questions after {max_attempts} attempts.\")\n",
    "\n",
    "# Generate questions\n",
    "num_questions = 100\n",
    "try:\n",
    "    evaluation_questions = generate_questions(num_questions)\n",
    "    print(f\"Successfully generated {len(evaluation_questions)} questions:\")\n",
    "    for i, question in enumerate(evaluation_questions, 1):\n",
    "        print(f\"{i}. {question}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 625\n",
      "-----HyDE generation time: 0.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://science.nasa.gov/climate-change/causes/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nrdc.org/stories/greenhouse-effect-101 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nrdc.org/stories/what-are-causes-climate-change \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.un.org/en/climatechange/science/causes-effects-climate-change \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.epa.gov/ghgemissions/overview-greenhouse-gases \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.un.org/en/climatechange/science/causes-effects-climate-change\n",
      "-----Web scraping time: 2.05 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.82 seconds\n",
      "-----Retrieval and reranking time: 0.32 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.43 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.98 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the main causes of climate change, which requires identifying the primary factors contributing to the global warming trend observed since the mid-20th century.\n",
      "\n",
      "Answer: According to the provided context, the main causes of climate change are human activities, specifically the expansion of the \"greenhouse effect\" due to the burning of fossil fuels for energy. This is supported by the text, which states: \"Human activities are driving the global warming trend observed since the mid-20th century.\" Additionally, the text mentions that the unchecked burning of fossil fuels is a significant contributor to climate change.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_fireworks import ChatFireworks\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Initialize the judge model (405B LLaMA)\n",
    "judge_model = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
    "\n",
    "def evaluate_answer_quality(question: str, answer: str, judge_model: Any) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer completely addresses the question.\n",
    "    Returns 1 if yes, 0 if no.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert evaluator. Your task is to determine if the given answer completely addresses the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \n",
    "    Does the answer completely address the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "def evaluate_document_selection(question: str, all_docs: List[str], selected_docs: List[str], judge_model: Any) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the selected documents are the best 10 out of the 80 to answer the question.\n",
    "    Returns 1 if yes, 0 if no.\n",
    "    \"\"\"\n",
    "    all_docs_text = \"\\n\".join([f\"{i+1}. {doc}...\" for i, doc in enumerate(all_docs)])\n",
    "    selected_indices = [all_docs.index(doc) + 1 for doc in selected_docs]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert information retrieval system. Your task is to determine if the selected documents are the best 10 out of the given 80 for answering the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Here are the first 200 characters of all 80 retrieved documents:\n",
    "    {all_docs_text}\n",
    "    \n",
    "    The system selected the following documents (by index): {', '.join(map(str, selected_indices))}\n",
    "    \n",
    "    Are these selected documents the best 10 out of the 80 for answering the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "async def evaluate_rag_system(question: str, answer: str, all_docs: List[str], selected_docs: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Evaluate the RAG system's performance.\n",
    "    \"\"\"\n",
    "    answer_correct = evaluate_answer_quality(question, answer, judge_model)\n",
    "    docs_correct = evaluate_document_selection(question, all_docs, selected_docs, judge_model)\n",
    "    \n",
    "    return {\n",
    "        \"answer_correct\": answer_correct,\n",
    "        \"docs_correct\": docs_correct\n",
    "    }\n",
    "\n",
    "async def run_evaluation():\n",
    "    question = \"What are the main causes of climate change?\"\n",
    "    answer, context_docs = await process_query(question, num_expansions=1, num_urls=3, num_docs=80, num_rerank=10, rerank_method=\"nano\", use_70b_model=False)\n",
    "    \n",
    "    all_docs = context_docs[:80]  # All 80 retrieved documents\n",
    "    selected_docs = context_docs[:10]  # Top 10 after reranking\n",
    "    \n",
    "    evaluation_results = await evaluate_rag_system(question, answer, all_docs, selected_docs)\n",
    "    \n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Answer Correctness: {evaluation_results['answer_correct']}\")\n",
    "    print(f\"Top 10 Documents Correctness: {evaluation_results['docs_correct']}\")\n",
    "\n",
    "# Run the evaluation\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 715\n",
      "-----HyDE generation time: 0.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.youtube.com/watch?v=pJS4QDUtzzI \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://medium.com/human-nature-group/what-is-the-air-speed-velocity-of-an-unladen-swallow-4c17087bbf33 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://medium.com/human-nature-group/what-is-the-air-speed-velocity-of-an-unladen-swallow-4c17087bbf33 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://style.org/unladenswallow/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://interestingengineering.com/science/monty-python-and-the-holy-grail-airspeed-velocity-of-an-unladen-swallow \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://interestingengineering.com/science/monty-python-and-the-holy-grail-airspeed-velocity-of-an-unladen-swallow \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.34 seconds\n",
      "Combined text length: 23125 characters\n",
      "Number of sentence windows: 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.97 seconds\n",
      "-----Retrieval and reranking time: 0.09 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.97 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " To answer the question, I will follow a reasoning step.\n",
      "\n",
      "Reasoning step: The text provides multiple estimates of the airspeed velocity of an unladen European Swallow, based on different Strouhal values and formulas. To determine the average airspeed velocity, I will look for the most commonly cited or accurate estimate.\n",
      "\n",
      "Answer: According to the text, the airspeed velocity of an unladen European Swallow can be estimated to be roughly 10 meters per second, using the formula U  3 f A, where f is the frequency (15 beats per second) and A is the amplitude (0.22 meters per beat). This estimate is mentioned in the text as being accurate, though perhaps coincidental.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the average airspeed velocity of an unladen swallow?\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 0\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 433\n",
      "-----HyDE generation time: 0.64 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://brainly.com/question/22527717 \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-history-of-suspension-bridges-When-was-the-first-one-built-and-in-which-country \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://brainly.com/question/22527717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.sciencedirect.com/topics/engineering/suspension-bridges \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nytimes.com/2007/05/08/science/08bridg.html \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Suspension_bridge \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.nytimes.com/2007/05/08/science/08bridg.html\n",
      "Error response 403 while requesting https://www.sciencedirect.com/topics/engineering/suspension-bridges\n",
      "-----Web scraping time: 2.98 seconds\n",
      "Combined text length: 5222 characters\n",
      "Number of sentence windows: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.57 seconds\n",
      "-----Retrieval and reranking time: 0.09 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.65 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The text mentions that the earliest suspension bridges were ropes slung across a chasm, but it does not specify which ancient civilization built the first known suspension bridge. However, it does mention Thangtong Gyalpo, a Tibetan siddha and bridge-builder, who originated the use of iron chains in his version of simple suspension bridges in 1433.\n",
      "\n",
      "Answer: I don't have enough information to answer that question. The text does not provide information on which ancient civilization built the first known suspension bridge.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which ancient civilization built the first known suspension bridge?\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 466\n",
      "-----HyDE generation time: 0.50 seconds\n",
      "-----Embedding time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.visualcapitalist.com/top-languages-spoken-in-the-world/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.berlitz.com/blog/most-spoken-languages-world \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/languages-by-number-of-native-speakers-2228882 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.visualcapitalist.com/top-languages-spoken-in-the-world/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.babbel.com/en/magazine/the-10-most-spoken-languages-in-the-world \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 4.82 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.70 seconds\n",
      "-----Retrieval and reranking time: 0.17 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 7.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.74 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the most widely spoken language in the world, which implies that we need to consider the total number of native speakers, not just the number of speakers in a particular region or country. We also need to consider the context of the provided text, which mentions the difficulty of determining the most spoken languages due to the complexity of language classification and the availability of reliable data.\n",
      "\n",
      "Answer: According to the text, the most widely spoken language in the world is Chinese, with approximately 1.3 billion native speakers, followed closely by Spanish with 486 million native speakers. However, if we consider the total number of speakers, including non-native speakers, the ranking may be different.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the most widely spoken language in the world?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 495\n",
      "-----HyDE generation time: 1.73 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/The_Ambassadors_(Holbein) \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://smarthistory.org/hans-holbein-the-younger-the-ambassadors/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://about.jstor.org/blog/a-closer-look-at-hans-holbeins-the-ambassadors/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://smarthistory.org/hans-holbein-the-younger-the-ambassadors/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.artsy.net/article/artsy-editorial-decoding-symbolism-hans-holbeins-ambassadors \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.artsy.net/article/artsy-editorial-decoding-symbolism-hans-holbeins-ambassadors \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.39 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.52 seconds\n",
      "-----Retrieval and reranking time: 0.15 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.57 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the famous painting \"The Ambassadors\". To answer this question, we need to identify the person who created the painting. The text mentions Hans Holbein the Younger as the artist who painted \"The Ambassadors\" in 1533.\n",
      "\n",
      "Answer: Hans Holbein the Younger.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the author of the famous painting \"The Ambassadors\"?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 515\n",
      "-----HyDE generation time: 0.68 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.sciencedirect.com/topics/neuroscience/nose \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Human_nose \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://my.clevelandclinic.org/health/body/21778-nose \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.sciencedirect.com/topics/neuroscience/nose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3199822/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/books/NBK544232/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3199822/\n",
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/books/NBK544232/\n",
      "-----Web scraping time: 2.70 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.39 seconds\n",
      "-----Retrieval and reranking time: 0.20 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.51 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " I don't have enough information to answer that question.\n",
      "\n",
      "Reasoning step: The provided context describes the anatomy and functions of the human nose, but it does not mention the chemical composition of the nose. To answer this question, I would need information about the types and proportions of molecules that make up the nose, such as proteins, carbohydrates, lipids, and other biomolecules.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the chemical composition of the human nose?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 0\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 568\n",
      "-----HyDE generation time: 0.66 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-was-the-first-computer-bug \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/worlds-first-computer-bug/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/worlds-first-computer-bug/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://lunduke.substack.com/p/the-story-of-the-first-computer-bug \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.globalapptesting.com/blog/the-worlds-first-computer-bug-global-app-testing \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://daily.jstor.org/the-bug-in-the-computer-bug-story/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.43 seconds\n",
      "Combined text length: 22286 characters\n",
      "Number of sentence windows: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.11 seconds\n",
      "-----Retrieval and reranking time: 0.21 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.73 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year in which the first computer bug occurred. To answer this question, I need to find the relevant information in the provided context.\n",
      "\n",
      "Answer: According to the context, the first computer bug occurred on September 9, 1947.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In what year did the first computer bug occur?\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 561\n",
      "-----HyDE generation time: 0.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.reddit.com/r/Stoicism/comments/10dr49a/core_beliefs_required_to_call_yourself_a_stoic/ \"HTTP/1.1 302 Found\"\n",
      "INFO:httpx:HTTP Request: GET https://plato.stanford.edu/ENTRIES/stoicism/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://dailystoic.com/9-core-stoic-beliefs/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Stoicism \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 302 while requesting https://www.reddit.com/r/Stoicism/comments/10dr49a/core_beliefs_required_to_call_yourself_a_stoic/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://becomingbetter.org/10-essential-principles-and-practices-of-stoicism/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://iep.utm.edu/stoicism/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.22 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.07 seconds\n",
      "-----Retrieval and reranking time: 0.16 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.42 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the founder of the Stoic school, which is a specific philosophical school. To answer this question, I need to identify the individual who is credited with founding the Stoic school.\n",
      "\n",
      "Answer: According to the provided context, the Stoic school was founded around 300 BCE by Zeno of Citium.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the founder of the philosophical school of Stoicism?\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 0\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 372\n",
      "-----HyDE generation time: 0.58 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://brainly.com/question/35555711 \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/606952-largest-living-organism \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/606952-largest-living-organism \"HTTP/1.1 301 Moved Permanently\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://brainly.com/question/35555711\n",
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/606952-largest-living-organism\n",
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/606952-largest-living-organism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.esa.int/Applications/Observing_the_Earth/Earth_from_Space_World_s_largest_living_structure \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://oceanservice.noaa.gov/facts/gbrlargeststructure.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://oceanservice.noaa.gov/facts/gbrlargeststructure.html \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.98 seconds\n",
      "Combined text length: 6356 characters\n",
      "Number of sentence windows: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.79 seconds\n",
      "-----Retrieval and reranking time: 0.09 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.45 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the world's largest living structure according to the Guinness World Records, but the provided context does not mention the Guinness World Records. However, it does mention that the Great Barrier Reef is the largest living structure on Earth and the only one visible from space.\n",
      "\n",
      "Answer: I don't have enough information to answer that question.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the world's largest living structure, according to the Guinness World Records?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 0\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 385\n",
      "-----HyDE generation time: 0.52 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.reuters.com/lifestyle/science/how-long-is-solar-systems-longest-day-venus-has-answer-2021-05-03/ \"HTTP/1.1 401 HTTP Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.universetoday.com/84663/which-planet-has-the-longest-day/ \"HTTP/1.1 503 Service Temporarily Unavailable\"\n",
      "INFO:httpx:HTTP Request: GET https://www.rmg.co.uk/stories/topics/solar-system-data \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 401 while requesting https://www.reuters.com/lifestyle/science/how-long-is-solar-systems-longest-day-venus-has-answer-2021-05-03/\n",
      "Error response 503 while requesting https://www.universetoday.com/84663/which-planet-has-the-longest-day/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://spaceplace.nasa.gov/days/en/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://spaceplace.nasa.gov/days/en/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.10 seconds\n",
      "Combined text length: 14250 characters\n",
      "Number of sentence windows: 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.95 seconds\n",
      "-----Retrieval and reranking time: 0.22 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.98 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.52 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: To determine which planet in our solar system has the longest day, we need to look at the table provided in the context, which lists the day length for each planet in hours.\n",
      "\n",
      "Answer: Mars has the longest day, with a day length of 25 hours.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which planet in our solar system has the longest day?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 481\n",
      "-----HyDE generation time: 0.67 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.poetryfoundation.org/poetrymagazine/poems/44212/the-love-song-of-j-alfred-prufrock \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/The_Love_Song_of_J._Alfred_Prufrock \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sparknotes.com/poetry/eliot/section1/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.owleyes.org/text/love-song/analysis/historical-context \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.litcharts.com/poetry/t-s-eliot/the-love-song-of-j-alfred-prufrock \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.sparknotes.com/poetry/eliot/section1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.bu.edu/writingprogram/journal/past-issues/issue-11/immerwahr/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.01 seconds\n",
      "Combined text length: 23444 characters\n",
      "Number of sentence windows: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.83 seconds\n",
      "-----Retrieval and reranking time: 0.17 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.50 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the poem \"The Love Song of J. Alfred Prufrock\", which is mentioned in the context as being written by T. S. Eliot.\n",
      "\n",
      "Answer: T. S. Eliot.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who wrote the famous poem \"The Love Song of J. Alfred Prufrock\"?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 423\n",
      "-----HyDE generation time: 0.72 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/highest-recorded-temperature \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://wmo.asu.edu/content/world-highest-temperature \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.twinkl.com/teaching-wiki/the-hottest-place-on-earth \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Highest_temperature_recorded_on_Earth \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/highest-recorded-temperature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.autoeurope.com/travel-blog/the-worlds-coldest-and-warmest-places/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sciencefocus.com/planet-earth/hottest-place-on-earth \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.autoeurope.com/travel-blog/the-worlds-coldest-and-warmest-places/\n",
      "-----Web scraping time: 2.44 seconds\n",
      "Combined text length: 13333 characters\n",
      "Number of sentence windows: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.30 seconds\n",
      "-----Retrieval and reranking time: 0.18 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 1.03 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the highest recorded temperature on Earth, which requires identifying the highest temperature measurement among the three major ways of measurement: air, ground, and satellite observation.\n",
      "\n",
      "Answer: According to the provided context, the current official highest registered air temperature on Earth is 56.7C (134.1F), recorded on 10 July 1913 at Furnace Creek Ranch, in Death Valley in the United States. However, it is worth noting that this record is currently under scrutiny due to concerns about its legitimacy, and if it were to be decertified, the highest established recorded air temperature on Earth would be 54.0C (129.2F), also recorded in Death Valley on 20 June 2013, and in Mitribah, Kuwait on 21 July 2016.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the highest recorded temperature on Earth?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 594\n",
      "-----HyDE generation time: 0.82 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-polio-vaccination \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-polio-vaccination \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Jonas_Salk \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/biography/Jonas-Salk \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/\n",
      "-----Web scraping time: 2.44 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.41 seconds\n",
      "-----Retrieval and reranking time: 0.14 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.74 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " The reasoning step is to identify the key information in the context that relates to the question. In this case, the context mentions Jonas Salk and his work on the polio vaccine, but it does not explicitly state that he is the inventor of the first successful polio vaccine.\n",
      "\n",
      "The answer is: Jonas Salk.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the inventor of the first successful polio vaccine?\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 579\n",
      "-----HyDE generation time: 0.68 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-largest-star-in-our-universe-What-are-its-properties-mass-diameter \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_largest_stars \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://science.howstuffworks.com/largest-star-in-the-universe.htm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.space.com/41290-biggest-star.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.space.com/41290-biggest-star.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.usatoday.com/story/tech/science/2023/02/16/largest-star-universe-red-hypergiant/11075755002/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.20 seconds\n",
      "Combined text length: 24085 characters\n",
      "Number of sentence windows: 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.69 seconds\n",
      "-----Retrieval and reranking time: 0.15 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.48 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the name of the largest star known to science, and the context provides information about a star called UY Scuti, which is described as the largest known star in the universe.\n",
      "\n",
      "Answer: UY Scuti.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the name of the largest star known to science?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 0\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 435\n",
      "-----HyDE generation time: 0.60 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_email \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_email \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/Do-you-know-who-was-the-first-person-to-use-email-and-who-was-the-first-person-to-receive-it \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mail.com/blog/posts/fiftieth-anniversary-of-email/20/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mail.com/blog/posts/fiftieth-anniversary-of-email/20/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://www.emailonacid.com/blog/article/email-marketing/history-of-email/: \n",
      "-----Web scraping time: 9.57 seconds\n",
      "Combined text length: 20224 characters\n",
      "Number of sentence windows: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "# Initialize the judge model (405B LLaMA)\n",
    "judge_model = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
    "\n",
    "def evaluate_answer_quality(question: str, answer: str) -> int:\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert evaluator. Your task is to determine if the given answer completely addresses the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \n",
    "    Does the answer completely address the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "def evaluate_document_selection(question: str, all_docs: list, selected_docs: list) -> int:\n",
    "    all_docs_text = \"\\n\".join([f\"{i+1}. {doc}...\" for i, doc in enumerate(all_docs)])\n",
    "    selected_indices = [all_docs.index(doc) + 1 for doc in selected_docs]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert information retrieval system. Your task is to determine if the selected documents are the best 10 out of the given 80 for answering the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Here are the 80 retrieved documents:\n",
    "    {all_docs_text}\n",
    "    \n",
    "    The system selected the following documents (by index): {', '.join(map(str, selected_indices))}\n",
    "    \n",
    "    Are these selected documents the best 10 out of the 80 for answering the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "async def run_evaluation(num_questions: int = 100):\n",
    "    questions = evaluation_questions\n",
    "    # [\n",
    "    #     \"What are the main causes of climate change?\",\n",
    "    #     \"How does artificial intelligence impact job markets?\",\n",
    "    #     \"What are the benefits and drawbacks of renewable energy sources?\",\n",
    "    #     \"How does diet affect mental health?\",\n",
    "    #     \"What are the major challenges in space exploration?\"\n",
    "    # ]\n",
    "    \n",
    "    results = []\n",
    "    total_answer_correct = 0\n",
    "    total_docs_correct = 0\n",
    "    \n",
    "    for question in questions[:num_questions]:\n",
    "        answer, context_docs = await process_query(question, num_expansions=1, num_urls=3, num_docs=80, num_rerank=10, rerank_method=\"nano\", use_70b_model=False)\n",
    "        \n",
    "        all_docs = context_docs[:80]  # All 80 retrieved documents\n",
    "        selected_docs = context_docs[:10]  # Top 10 after reranking\n",
    "        \n",
    "        answer_correct = evaluate_answer_quality(question, answer)\n",
    "        docs_correct = evaluate_document_selection(question, all_docs, selected_docs)\n",
    "        \n",
    "        total_answer_correct += answer_correct\n",
    "        total_docs_correct += docs_correct\n",
    "        \n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"answer_correctness\": answer_correct,\n",
    "            \"top_10_docs_correctness\": docs_correct,\n",
    "            \"all_docs\": all_docs,\n",
    "            \"selected_docs\": selected_docs\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer Correctness: {answer_correct}\")\n",
    "        print(f\"Top 10 Documents Correctness: {docs_correct}\")\n",
    "        print(\"---\")\n",
    "    \n",
    "    avg_answer_correct = total_answer_correct / num_questions\n",
    "    avg_docs_correct = total_docs_correct / num_questions\n",
    "    print(f\"\\nAverage Results over {num_questions} questions:\")\n",
    "    print(f\"Average Answer Correctness: {avg_answer_correct:.2f}\")\n",
    "    print(f\"Average Top 10 Documents Correctness: {avg_docs_correct:.2f}\")\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "    output = {\n",
    "        \"results\": results,\n",
    "        \"average_answer_correctness\": avg_answer_correct,\n",
    "        \"average_top_10_docs_correctness\": avg_docs_correct\n",
    "    }\n",
    "    \n",
    "    with open('evaluation_results.json', 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    print(\"\\nResults have been saved to 'evaluation_results.json'\")\n",
    "\n",
    "# To run the evaluation, use:\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating none reranker:   0%|          | 0/100 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 715\n",
      "-----HyDE generation time: 1.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://medium.com/human-nature-group/what-is-the-air-speed-velocity-of-an-unladen-swallow-4c17087bbf33 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://medium.com/human-nature-group/what-is-the-air-speed-velocity-of-an-unladen-swallow-4c17087bbf33 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.youtube.com/watch?v=pJS4QDUtzzI \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-airspeed-velocity-of-an-unladen-swallow-1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://interestingengineering.com/science/monty-python-and-the-holy-grail-airspeed-velocity-of-an-unladen-swallow \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://interestingengineering.com/science/monty-python-and-the-holy-grail-airspeed-velocity-of-an-unladen-swallow \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.79 seconds\n",
      "Combined text length: 18304 characters\n",
      "Number of sentence windows: 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.11 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 7.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 1.34 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The text mentions that the Strouhal number, which is a measure of the efficiency of a bird's flight pattern, averages between 0.2 and 0.4 for most birds. However, it does not provide a specific value for the airspeed velocity of an unladen swallow. To answer the question, we need to use the Strouhal ratio equation to estimate the airspeed velocity of the swallow.\n",
      "\n",
      "Answer: Unfortunately, the text does not provide enough information to calculate the exact airspeed velocity of an unladen swallow. However, we can use the Strouhal ratio equation to make an estimate. Let's assume that the frequency of wingbeats of a swallow is around 4-5 Hz (a typical value for birds), and the amplitude of its wingbeats is around 0.1-0.2 meters (a rough estimate). Using the Strouhal ratio equation, we can estimate the airspeed velocity of the swallow to be around 5-11 meters per second (m/s). However, please note that this is a very rough estimate and should be taken as a rough order of magnitude rather than a precise value.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   1%|          | 1/100 [00:11<18:12, 11.04s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 433\n",
      "-----HyDE generation time: 0.42 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.nytimes.com/2007/05/08/science/08bridg.html \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://brainly.com/question/22527717 \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-history-of-suspension-bridges-When-was-the-first-one-built-and-in-which-country \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sciencedirect.com/topics/engineering/suspension-bridges \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Suspension_bridge \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Suspension_bridge \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.nytimes.com/2007/05/08/science/08bridg.html\n",
      "Error response 403 while requesting https://brainly.com/question/22527717\n",
      "Error response 403 while requesting https://www.sciencedirect.com/topics/engineering/suspension-bridges\n",
      "-----Web scraping time: 2.56 seconds\n",
      "Combined text length: 10222 characters\n",
      "Number of sentence windows: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.90 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.64 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The context provided mentions the history of suspension bridges, including the use of ropes and iron chains in early bridges. It also mentions the Tibetan siddha and bridge-builder Thangtong Gyalpo, who originated the use of iron chains in his version of simple suspension bridges in 1433. However, it does not explicitly mention an ancient civilization that built the first known suspension bridge.\n",
      "\n",
      "Answer: I don't have enough information to answer that question.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   2%|         | 2/100 [00:17<13:48,  8.45s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 466\n",
      "-----HyDE generation time: 0.76 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.visualcapitalist.com/top-languages-spoken-in-the-world/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.berlitz.com/blog/most-spoken-languages-world \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/languages-by-number-of-native-speakers-2228882 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.visualcapitalist.com/top-languages-spoken-in-the-world/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.babbel.com/en/magazine/the-10-most-spoken-languages-in-the-world \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 4.27 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.66 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.88 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 1.04 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the most widely spoken language in the world, which implies that we need to consider both native and non-native speakers. However, the provided context only mentions the number of native speakers for each language. To answer the question accurately, we need to look for information that includes both native and non-native speakers.\n",
      "\n",
      "Answer: According to the context, the most widely spoken language in the world is Chinese, with 1.3 billion native speakers. However, the context also mentions that the numbers vary widely, and Ethnologue puts the number of native speakers at 1.3 billion. But to answer the question accurately, we need to look at the list of languages by total number of speakers, which is mentioned in the context but not explicitly provided. According to the list of languages by total number of speakers, the most widely spoken language in the world is actually English, with 379,682,200 native speakers, but it's not clear if this number includes non-native speakers.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   3%|         | 3/100 [00:27<14:35,  9.03s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 495\n",
      "-----HyDE generation time: 0.78 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://smarthistory.org/hans-holbein-the-younger-the-ambassadors/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/The_Ambassadors_(Holbein) \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://smarthistory.org/hans-holbein-the-younger-the-ambassadors/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/maziar/eval_ranking/.venv/lib/python3.10/site-packages/bs4/builder/_lxml.py:291: RuntimeWarning: coroutine 'run_evaluation' was never awaited\n",
      "  for attr, value in list(attrs.items()):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "INFO:httpx:HTTP Request: GET https://about.jstor.org/blog/a-closer-look-at-hans-holbeins-the-ambassadors/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.artsy.net/article/artsy-editorial-decoding-symbolism-hans-holbeins-ambassadors \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.artsy.net/article/artsy-editorial-decoding-symbolism-hans-holbeins-ambassadors \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.60 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.87 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.82 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the famous painting \"The Ambassadors\". To answer this question, we need to identify the person who created the painting. The text mentions Hans Holbein the Younger as the artist who painted \"The Ambassadors\" in 1533.\n",
      "\n",
      "Answer: Hans Holbein the Younger.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   4%|         | 4/100 [00:35<13:59,  8.75s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 515\n",
      "-----HyDE generation time: 0.74 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.sciencedirect.com/topics/neuroscience/nose \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Human_nose \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.sciencedirect.com/topics/neuroscience/nose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/books/NBK526086/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/books/NBK544232/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://my.clevelandclinic.org/health/body/21778-nose \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/books/NBK526086/\n",
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/books/NBK544232/\n",
      "-----Web scraping time: 2.44 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.35 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.82 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " I don't have enough information to answer that question.\n",
      "\n",
      "Reasoning step: The provided context describes the anatomy and functions of the human nose, but it does not mention the chemical composition of the nose. To answer this question, I would need information about the types and proportions of molecules that make up the nose, such as proteins, carbohydrates, lipids, and other biomolecules.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   5%|         | 5/100 [00:43<13:13,  8.35s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 568\n",
      "-----HyDE generation time: 0.69 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/worlds-first-computer-bug/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://lunduke.substack.com/p/the-story-of-the-first-computer-bug \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/worlds-first-computer-bug/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-was-the-first-computer-bug \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.globalapptesting.com/blog/the-worlds-first-computer-bug-global-app-testing \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://daily.jstor.org/the-bug-in-the-computer-bug-story/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 4.34 seconds\n",
      "Combined text length: 22286 characters\n",
      "Number of sentence windows: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.05 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 7.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.53 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year in which the first computer bug occurred. To answer this question, I need to find the relevant information in the provided context.\n",
      "\n",
      "Answer: According to the context, the first computer bug occurred on September 9, 1947.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   6%|         | 6/100 [00:53<13:48,  8.81s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 561\n",
      "-----HyDE generation time: 0.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.reddit.com/r/Stoicism/comments/10dr49a/core_beliefs_required_to_call_yourself_a_stoic/ \"HTTP/1.1 302 Found\"\n",
      "INFO:httpx:HTTP Request: GET https://plato.stanford.edu/ENTRIES/stoicism/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://dailystoic.com/9-core-stoic-beliefs/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Stoicism \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 302 while requesting https://www.reddit.com/r/Stoicism/comments/10dr49a/core_beliefs_required_to_call_yourself_a_stoic/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://iep.utm.edu/stoicism/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://becomingbetter.org/10-essential-principles-and-practices-of-stoicism/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.44 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.35 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.79 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the founder of the Stoic school, which is a specific philosophical school. To answer this question, I need to identify the individual who is credited with founding the Stoic school.\n",
      "\n",
      "Answer: According to the provided context, the Stoic school was founded around 300 BCE by Zeno of Citium.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   7%|         | 7/100 [01:01<13:26,  8.67s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 372\n",
      "-----HyDE generation time: 0.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://brainly.com/question/35555711 \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.esa.int/Applications/Observing_the_Earth/Earth_from_Space_World_s_largest_living_structure \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://brainly.com/question/35555711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://oceanservice.noaa.gov/facts/gbrlargeststructure.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/606952-largest-living-organism \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://oceanservice.noaa.gov/facts/gbrlargeststructure.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/606952-largest-living-organism \"HTTP/1.1 301 Moved Permanently\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/606952-largest-living-organism\n",
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/606952-largest-living-organism\n",
      "-----Web scraping time: 2.02 seconds\n",
      "Combined text length: 6356 characters\n",
      "Number of sentence windows: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.80 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 3.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.78 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the world's largest living structure according to the Guinness World Records, but the provided context does not mention the Guinness World Records. However, it does mention that the Great Barrier Reef is the largest living structure on Earth and the only one visible from space.\n",
      "\n",
      "Answer: I don't have enough information to answer that question.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   8%|         | 8/100 [01:08<12:34,  8.20s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 385\n",
      "-----HyDE generation time: 0.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.rmg.co.uk/stories/topics/solar-system-data \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.universetoday.com/84663/which-planet-has-the-longest-day/ \"HTTP/1.1 503 Service Temporarily Unavailable\"\n",
      "INFO:httpx:HTTP Request: GET https://www.reuters.com/lifestyle/science/how-long-is-solar-systems-longest-day-venus-has-answer-2021-05-03/ \"HTTP/1.1 401 HTTP Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 503 while requesting https://www.universetoday.com/84663/which-planet-has-the-longest-day/\n",
      "Error response 401 while requesting https://www.reuters.com/lifestyle/science/how-long-is-solar-systems-longest-day-venus-has-answer-2021-05-03/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://spaceplace.nasa.gov/days/en/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://spaceplace.nasa.gov/days/en/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.46 seconds\n",
      "Combined text length: 14250 characters\n",
      "Number of sentence windows: 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.81 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.61 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: To determine which planet in our solar system has the longest day, we need to look at the table provided in the context, which lists the day length for each planet in hours.\n",
      "\n",
      "Answer: Mars has the longest day, with a day length of 25 hours.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   9%|         | 9/100 [01:16<12:28,  8.22s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 481\n",
      "-----HyDE generation time: 0.75 seconds\n",
      "-----Embedding time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.poetryfoundation.org/poetrymagazine/poems/44212/the-love-song-of-j-alfred-prufrock \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://www.sparknotes.com/poetry/eliot/section1/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/The_Love_Song_of_J._Alfred_Prufrock \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.sparknotes.com/poetry/eliot/section1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.litcharts.com/poetry/t-s-eliot/the-love-song-of-j-alfred-prufrock \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.owleyes.org/text/love-song/analysis/historical-context \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.bu.edu/writingprogram/journal/past-issues/issue-11/immerwahr/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.71 seconds\n",
      "Combined text length: 23444 characters\n",
      "Number of sentence windows: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.63 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.47 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the poem \"The Love Song of J. Alfred Prufrock\", which is mentioned in the context as being written by T. S. Eliot.\n",
      "\n",
      "Answer: T. S. Eliot.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  10%|         | 10/100 [01:24<12:06,  8.08s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 423\n",
      "-----HyDE generation time: 0.49 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.twinkl.com/teaching-wiki/the-hottest-place-on-earth \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/highest-recorded-temperature \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://wmo.asu.edu/content/world-highest-temperature \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.autoeurope.com/travel-blog/the-worlds-coldest-and-warmest-places/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Highest_temperature_recorded_on_Earth \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/highest-recorded-temperature\n",
      "Error response 403 while requesting https://www.autoeurope.com/travel-blog/the-worlds-coldest-and-warmest-places/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.sciencefocus.com/planet-earth/hottest-place-on-earth \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.33 seconds\n",
      "Combined text length: 13333 characters\n",
      "Number of sentence windows: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.26 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.91 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the highest recorded temperature on Earth, which requires identifying the highest temperature measurement among the three major ways of measurement: air, ground, and satellite observation.\n",
      "\n",
      "Answer: According to the provided context, the current official highest registered air temperature on Earth is 56.7C (134.1F), recorded on 10 July 1913 at Furnace Creek Ranch, in Death Valley in the United States. However, it is worth noting that this record is currently under scrutiny due to concerns about its legitimacy, and if it were to be decertified, the highest established recorded air temperature on Earth would be 54.0C (129.2F), also recorded in Death Valley on 20 June 2013, and in Mitribah, Kuwait on 21 July 2016.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  11%|         | 11/100 [01:32<11:54,  8.03s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 594\n",
      "-----HyDE generation time: 0.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-polio-vaccination \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-polio-vaccination \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Jonas_Salk \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/\n",
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/\n",
      "-----Web scraping time: 2.48 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.10 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.60 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " The reasoning step is to identify the key information in the context that relates to the question. In this case, the context mentions Jonas Salk and his work on developing a vaccine against polio.\n",
      "\n",
      "The answer is: Jonas Salk.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  12%|        | 12/100 [01:40<11:43,  7.99s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 579\n",
      "-----HyDE generation time: 0.69 seconds\n",
      "-----Embedding time: 0.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.usatoday.com/story/tech/science/2023/02/16/largest-star-universe-red-hypergiant/11075755002/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.usatoday.com/story/tech/science/2023/02/16/largest-star-universe-red-hypergiant/11075755002/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.space.com/41290-biggest-star.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-biggest-known-star-in-terms-of-size-and-mass \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://science.howstuffworks.com/largest-star-in-the-universe.htm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_largest_stars \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.91 seconds\n",
      "Combined text length: 22929 characters\n",
      "Number of sentence windows: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.00 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.41 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the name of the largest star known to science, which is mentioned in the context as UY Scuti.\n",
      "\n",
      "Answer: UY Scuti.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  13%|        | 13/100 [01:50<12:22,  8.53s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 435\n",
      "-----HyDE generation time: 0.57 seconds\n",
      "-----Embedding time: 0.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.quora.com/Do-you-know-who-was-the-first-person-to-use-email-and-who-was-the-first-person-to-receive-it \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_email \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_email \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.emailonacid.com/blog/article/email-marketing/history-of-email/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mail.com/blog/posts/fiftieth-anniversary-of-email/20/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mail.com/blog/posts/fiftieth-anniversary-of-email/20/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.59 seconds\n",
      "Combined text length: 25225 characters\n",
      "Number of sentence windows: 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.96 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.47 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year in which the first email was sent. The text mentions that Ray Tomlinson sent the first email in 1971, but it does not explicitly state the year in which the first email was sent. However, it is mentioned that Tomlinson sent the first email in 1971, and that he had a hard time remembering the exact date or content of the first email 25 years later.\n",
      "\n",
      "Answer: 1971\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  14%|        | 14/100 [01:57<11:50,  8.26s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 579\n",
      "-----HyDE generation time: 0.87 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.theatlantic.com/entertainment/archive/2017/05/one-hundred-years-of-solitude-50-years-later/527118/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/One_Hundred_Years_of_Solitude \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/One-Hundred-Years-of-Solitude \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://thenarrativearc.org/years-solitude \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://edubirdie.com/examples/magic-realism-in-one-hundred-years-of-solitude-by-gabriel-garcia-marquez/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://rupkatha.com/V2/n3/MagicRealisminMarquez.pdf \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.88 seconds\n",
      "Combined text length: 30005 characters\n",
      "Number of sentence windows: 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.59 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 7.55 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.56 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the famous novel \"One Hundred Years of Solitude\". To answer this question, I need to identify the person mentioned in the context as the writer of the novel.\n",
      "\n",
      "Answer: Gabriel Garca Mrquez.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  15%|        | 15/100 [02:08<12:47,  9.03s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 351\n",
      "-----HyDE generation time: 0.50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/ocean-trench/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Mariana_Trench \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/science/deep-sea-trench \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.whoi.edu/know-your-ocean/ocean-topics/how-the-ocean-works/seafloor-below/ocean-trenches/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://oceanservice.noaa.gov/facts/oceandepth.html \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.22 seconds\n",
      "Combined text length: 22212 characters\n",
      "Number of sentence windows: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.81 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.52 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " **Reasoning Step:** The question asks for the deepest part of the ocean, which is a specific location within the ocean. To answer this question, I need to identify the location mentioned in the context that is described as the deepest part of the ocean.\n",
      "\n",
      "**Answer:** The deepest part of the ocean is called the Challenger Deep, which is located beneath the western Pacific Ocean in the southern end of the Mariana Trench.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  16%|        | 16/100 [02:15<11:46,  8.41s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 444\n",
      "-----HyDE generation time: 0.64 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Smartphone \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.weforum.org/agenda/2018/03/remembering-first-smartphone-simon-ibm/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://simpletexting.com/blog/where-have-we-come-since-the-first-smartphone/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.textline.com/blog/smartphone-history \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://simpletexting.com/blog/where-have-we-come-since-the-first-smartphone/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.weforum.org/agenda/2018/03/remembering-first-smartphone-simon-ibm/\n",
      "Error response 403 while requesting https://simpletexting.com/blog/where-have-we-come-since-the-first-smartphone/\n",
      "Error response 403 while requesting https://simpletexting.com/blog/where-have-we-come-since-the-first-smartphone/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://blog.textedly.com/smartphone-history-when-were-smartphones-invented \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.56 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.83 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.56 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " I don't have enough information to answer that question.\n",
      "\n",
      "However, I can provide a reasoning step to answer the question \"When were smartphones invented?\"\n",
      "\n",
      "Reasoning step: The context mentions that the first smartphone was announced by IBM in 1992, and it was available for purchase in 1994.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  17%|        | 17/100 [02:23<11:16,  8.15s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 522\n",
      "-----HyDE generation time: 0.59 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.bairesdev.com/blog/best-programming-languages-web-development/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.simplilearn.com/best-programming-languages-start-learning-today-article \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.browserstack.com/guide/best-language-for-web-development \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.statista.com/statistics/793628/worldwide-developer-survey-most-used-languages/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.bairesdev.com/blog/best-programming-languages-web-development/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.coursera.org/articles/popular-programming-languages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://eluminoustechnologies.com/blog/top-10-web-programming-languages/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.21 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.36 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.49 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the most widely used programming language in the world, which requires identifying the language with the highest usage rate among software developers worldwide.\n",
      "\n",
      "Answer: According to the text, JavaScript is the most popular language to learn, and it is also the most commonly used programming language among software developers around the world, with more than 62.3 percent of respondents stating that they used JavaScript.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  18%|        | 18/100 [02:32<11:26,  8.37s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 409\n",
      "-----HyDE generation time: 0.59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/Queen-British-rock-group \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Queen_(band) \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Queen_(band) \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.34 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.45 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.55 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the lead singer of the rock band Queen. To answer this question, we need to identify the person who is credited as the lead vocalist of the band. \n",
      "\n",
      "Answer: Freddie Mercury was the lead singer of the rock band Queen.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  19%|        | 19/100 [02:41<11:33,  8.56s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 327\n",
      "-----HyDE generation time: 0.44 seconds\n",
      "-----Embedding time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.physicalgold.com/insights/physical-properties-of-gold/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/science/gold-chemical-element \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.rsc.org/periodic-table/element/79/gold \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.open.edu/openlearn/science-maths-technology/science/chemistry/properties-gold \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://pubchem.ncbi.nlm.nih.gov/element/Gold: \n",
      "-----Web scraping time: 9.93 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.06 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 12.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.51 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the chemical symbol of gold, which is mentioned in the context as \"Au,\" derived from the Latin word \"aurum,\" meaning \"shining dawn.\"\n",
      "\n",
      "Answer: The chemical symbol for gold is \"Au.\"\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  20%|        | 20/100 [02:56<14:13, 10.66s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 508\n",
      "-----HyDE generation time: 0.74 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.nasa.gov/history/apollo-11-mission-overview/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.rmg.co.uk/stories/topics/apollo-11-crew \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/Which-year-did-people-go-to-the-moon-for-the-first-time \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://science.nasa.gov/moon/moon-walkers/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.rmg.co.uk/stories/topics/how-many-people-have-walked-on-moon \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Apollo_11 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.92 seconds\n",
      "Combined text length: 25188 characters\n",
      "Number of sentence windows: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.87 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.60 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year when the first human walked on the moon. The context provides information about the Apollo 11 mission, which includes the names of the astronauts, the spacecraft, and the dates of the mission. However, the specific year when the first human walked on the moon is not explicitly mentioned in the provided text.\n",
      "\n",
      "Answer: 1969.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  21%|        | 21/100 [03:04<13:04,  9.92s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 493\n",
      "-----HyDE generation time: 0.62 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://study.com/academy/lesson/social-and-historical-context-of-romeo-and-juliet.html \"HTTP/1.1 403 HTTP Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sparknotes.com/shakespeare/romeojuliet/context/historical/what-did-shakespeares-audience-know-about-italy/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://rainford.org.uk/wp-content/uploads/2018/10/Romeo-and-Juliet-Self-Testing-Example.pdf \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Romeo_and_Juliet \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://study.com/academy/lesson/social-and-historical-context-of-romeo-and-juliet.html\n",
      "Error response 403 while requesting https://www.sparknotes.com/shakespeare/romeojuliet/context/historical/what-did-shakespeares-audience-know-about-italy/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/Romeo-and-Juliet \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.60 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.54 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.98 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.52 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the famous play \"Romeo and Juliet\". To answer this question, I need to identify the person who wrote the play. The context provides information about the play, its plot, and its history, but it does not explicitly mention the author's name. However, it does mention that the play was written by William Shakespeare early in his career.\n",
      "\n",
      "Answer: William Shakespeare.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  22%|       | 22/100 [03:12<11:59,  9.22s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 492\n",
      "-----HyDE generation time: 0.79 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_waterfalls_by_flow_rate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_waterfalls_by_flow_rate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://chasingchanelle.com/biggest-waterfall-in-the-world/ \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://www.worldwaterfalldatabase.com/largest-waterfalls/volume \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.worldwaterfalldatabase.com/largest-waterfalls/volume \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.46 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.94 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.40 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.84 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: To determine the world's largest waterfall by volume of water, we need to look for the waterfall with the highest flow rate or discharge of water.\n",
      "\n",
      "Answer: According to the list of waterfalls by flow rate on Wikipedia, the world's largest waterfall by volume of water is Inga Falls, with a flow rate of 25,768 cubic meters per second.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  23%|       | 23/100 [03:21<11:49,  9.21s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 521\n",
      "-----HyDE generation time: 0.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_the_web_browser \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mozilla.org/en-US/firefox/browsers/browser-history/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mozilla.org/en-US/firefox/browsers/browser-history/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.meetsidekick.com/who-created-the-first-web-browser/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.meetsidekick.com/who-created-the-first-web-browser/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_the_web_browser \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.21 seconds\n",
      "Combined text length: 26583 characters\n",
      "Number of sentence windows: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.35 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.46 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " The founder of the company that developed the first web browser is Marc Andreessen. He founded the company that released Netscape Navigator, which was one of the first popular web browsers, in 1994.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  24%|       | 24/100 [03:31<12:03,  9.52s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 522\n",
      "-----HyDE generation time: 0.51 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.reddit.com/r/spaceporn/comments/xy1d00/the_tallest_mountain_in_the_solar_system_olympus/ \"HTTP/1.1 302 Found\"\n",
      "INFO:httpx:HTTP Request: GET https://science.howstuffworks.com/tallest-mountain-in-solar-system.htm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.jpl.nasa.gov/edu/learn/video/mars-in-a-minute-how-did-mars-get-such-enormous-mountains/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_tallest_mountains_in_the_Solar_System \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_tallest_mountains_in_the_Solar_System \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 302 while requesting https://www.reddit.com/r/spaceporn/comments/xy1d00/the_tallest_mountain_in_the_solar_system_olympus/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://coolcosmos.ipac.caltech.edu/ask/199-Where-is-the-highest-mountain-in-our-Solar-System- \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.43 seconds\n",
      "Combined text length: 19607 characters\n",
      "Number of sentence windows: 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.55 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.67 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.43 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the highest mountain peak in the solar system, and the provided context mentions Olympus Mons on Mars as the tallest mountain and volcano in the solar system.\n",
      "\n",
      "Answer: Olympus Mons on Mars.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  25%|       | 25/100 [03:39<11:06,  8.89s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 333\n",
      "-----HyDE generation time: 0.41 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.rollingstone.com/feature/guns-n-roses-excerpt-nothin-but-a-good-time-book-1130385/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://rockhall.com/inductees/guns-n-roses/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/Guns-N-Roses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Guns_N%27_Roses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Guns_N%27_Roses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://rockhall.com/inductees/guns-n-roses/\n",
      "-----Web scraping time: 3.72 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.75 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.47 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the lead singer of the rock band Guns N' Roses. To answer this question, I need to identify the vocalist mentioned in the context as a member of the band.\n",
      "\n",
      "Answer: According to the context, the lead singer of the rock band Guns N' Roses is Axl Rose.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  26%|       | 26/100 [03:47<10:53,  8.83s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 506\n",
      "-----HyDE generation time: 0.49 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://propio-ls.com/2021/11/11/top-languages-usa/ \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Languages_of_the_United_States \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 301 while requesting https://propio-ls.com/2021/11/11/top-languages-usa/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.census.gov/library/stories/2022/12/languages-we-speak-in-united-states.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.census.gov/library/stories/2022/12/languages-we-speak-in-united-states.html \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.53 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.20 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.43 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.81 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the most widely spoken language in the United States. To answer this question, we need to look for information in the provided context that mentions the most common language spoken in the country.\n",
      "\n",
      "Answer: According to the text \"Languages of the United States - Wikipedia\", the most widely spoken language in the United States is English, with 78.0% of the population speaking it.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  27%|       | 27/100 [03:56<10:29,  8.63s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 540\n",
      "-----HyDE generation time: 0.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://study.com/academy/lesson/to-kill-a-mockingbird-themes-symbols-imagery.html \"HTTP/1.1 403 HTTP Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nytimes.com/2016/02/20/arts/harper-lee-dies.html \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sparknotes.com/lit/mocking/themes/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Harper_Lee \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/To-Kill-a-Mockingbird \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://study.com/academy/lesson/to-kill-a-mockingbird-themes-symbols-imagery.html\n",
      "Error response 403 while requesting https://www.nytimes.com/2016/02/20/arts/harper-lee-dies.html\n",
      "Error response 403 while requesting https://www.sparknotes.com/lit/mocking/themes/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.yourdictionary.com/articles/tkm-symbols \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.57 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.20 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.48 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the famous novel \"To Kill a Mockingbird\". To answer this question, I need to identify the person mentioned in the context as the author of the novel.\n",
      "\n",
      "Answer: Nelle Harper Lee.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  28%|       | 28/100 [04:03<09:50,  8.21s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 528\n",
      "-----HyDE generation time: 0.61 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.sciencedirect.com/science/article/pii/S0891061817300601 \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Human_brain \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.medicalnewstoday.com/articles/326649 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/books/NBK539894/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.sciencedirect.com/science/article/pii/S0891061817300601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://my.clevelandclinic.org/health/articles/22513-neurotransmitters \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/books/NBK539894/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.hopkinsmedicine.org/health/conditions-and-diseases/anatomy-of-the-brain \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.39 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.16 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.55 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the chemical composition of the human brain, which is mentioned in the context as a combination of water, protein, carbohydrates, and salts.\n",
      "\n",
      "Answer: The brain is made up of a combination of water, protein, carbohydrates, and salts, with the majority being water and the remaining 40% consisting of these other components.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  29%|       | 29/100 [04:10<09:27,  7.99s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 513\n",
      "-----HyDE generation time: 0.55 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4200566/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6062759/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://optn.transplant.hrsa.gov/news/first-heart-transplant-performed-35-years-ago/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4200566/\n",
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6062759/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.bhf.org.uk/informationsupport/heart-matters-magazine/medical/history-of-uk-heart-transplant \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.bhf.org.uk/informationsupport/heart-matters-magazine/medical/history-of-uk-heart-transplant \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.07 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.03 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.60 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year of the first successful heart transplant. To answer this question, I need to identify the year mentioned in the text as the year of the first successful heart transplant.\n",
      "\n",
      "Answer: According to the text, the first successful heart transplant occurred in 1967, when Christiaan Barnard transplanted a heart from a 25-year-old woman into Lewis Washkansky, a 53-year-old South African grocer.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "async def run_evaluation(num_questions: int = 100):\n",
    "    questions = evaluation_questions\n",
    "    rerankers = [\"none\", \"llm\", \"nano\", \"small\", \"medium_t5\", \"medium_multilang\"]\n",
    "    \n",
    "    for reranker in rerankers:\n",
    "        results = []\n",
    "        total_answer_correct = 0\n",
    "        total_docs_correct = 0\n",
    "        \n",
    "        # Create a progress bar for each reranker\n",
    "        progress_bar = tqdm(total=num_questions, desc=f\"Evaluating {reranker} reranker\")\n",
    "        \n",
    "        for question in questions:\n",
    "            answer, context_docs = await process_query(question, num_expansions=1, num_urls=3, num_docs=80, num_rerank=10, rerank_method=reranker, use_70b_model=False)\n",
    "            \n",
    "            all_docs = context_docs[:80]  # All 80 retrieved documents\n",
    "            selected_docs = context_docs[:10]  # Top 10 after reranking\n",
    "            \n",
    "            answer_correct = evaluate_answer_quality(question, answer)\n",
    "            docs_correct = evaluate_document_selection(question, all_docs, selected_docs)\n",
    "            \n",
    "            total_answer_correct += answer_correct\n",
    "            total_docs_correct += docs_correct\n",
    "            \n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"answer_correctness\": answer_correct,\n",
    "                \"top_10_docs_correctness\": docs_correct,\n",
    "                \"all_docs\": all_docs,\n",
    "                \"selected_docs\": selected_docs\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # Close the progress bar\n",
    "        progress_bar.close()\n",
    "        \n",
    "        avg_answer_correct = total_answer_correct / num_questions\n",
    "        avg_docs_correct = total_docs_correct / num_questions\n",
    "        print(f\"\\nAverage Results for {reranker} reranker over {num_questions} questions:\")\n",
    "        print(f\"Average Answer Correctness: {avg_answer_correct:.2f}\")\n",
    "        print(f\"Average Top 10 Documents Correctness: {avg_docs_correct:.2f}\")\n",
    "        \n",
    "        # Save results to a JSON file\n",
    "        output = {\n",
    "            \"results\": results,\n",
    "            \"average_answer_correctness\": avg_answer_correct,\n",
    "            \"average_top_10_docs_correctness\": avg_docs_correct\n",
    "        }\n",
    "        \n",
    "        filename = f'evaluation_{reranker}.json'\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults have been saved to '{filename}'\")\n",
    "\n",
    "# To run the evaluation, use:\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/maziar/eval_ranking/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_fireworks import FireworksEmbeddings, ChatFireworks\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "from typing import List, Tuple, Any\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import faiss\n",
    "import httpx\n",
    "from urllib.parse import urlparse\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU will be used automatically by FlashRank.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/ubuntu/.flashrank_cache/rank_zephyr_7b_v1_full/rank_zephyr_7b_v1_full.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = hub\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    98.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'hub', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set up API clients\n",
    "os.environ['FIREWORKS_API_KEY'] = 'RvonxWF0t8SZrxuJZf8HXyemIJg5j5UbgD07viljBkTlA5Ah'\n",
    "# os.environ[\"SERPER_API_KEY\"] = '7f21350b9f815fde7c71b5441682c5f07e573438'\n",
    "os.environ[\"SERPER_API_KEY\"] = '47f29c8b4577479bfb549fe1c567b715a1587c44'\n",
    "\n",
    "# Initialize components\n",
    "search = GoogleSerperAPIWrapper(k=3)\n",
    "embeddings = FireworksEmbeddings(model=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_8b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_70b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "# Create a directory for caching in the user's home folder\n",
    "cache_dir = Path.home() / \".flashrank_cache\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU will be used automatically by FlashRank.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. CPU will be used.\")\n",
    "\n",
    "# Initialize FlashRank rerankers\n",
    "ranker_nano = Ranker(cache_dir=str(cache_dir))\n",
    "ranker_small = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=str(cache_dir))\n",
    "ranker_medium_t5 = Ranker(model_name=\"rank-T5-flan\", cache_dir=str(cache_dir))\n",
    "ranker_medium_multilang = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=str(cache_dir))\n",
    "ranker_large = Ranker(model_name=\"rank_zephyr_7b_v1_full\", max_length=1024, cache_dir=str(cache_dir))\n",
    "\n",
    "# Ensure models are on GPU if available\n",
    "for ranker in [ranker_nano, ranker_small, ranker_medium_t5, ranker_medium_multilang, ranker_large]:\n",
    "    if hasattr(ranker, 'model') and hasattr(ranker.model, 'to'):\n",
    "        ranker.model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Download NLTK data\n",
    "# nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "async def scrape_webpage(client, url):\n",
    "    try:\n",
    "        response = await client.get(url, timeout=3.0)\n",
    "        response.raise_for_status()\n",
    "        text = response.text\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        content = ' '.join(soup.stripped_strings)\n",
    "        return content[:5000], len(content[:5000])\n",
    "    except (httpx.RequestError, httpx.TimeoutException) as exc:\n",
    "        print(f\"An error occurred while requesting {url}: {exc}\")\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(f\"Error response {exc.response.status_code} while requesting {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return \"\", 0\n",
    "\n",
    "async def search_and_scrape(query, num_urls):\n",
    "    search_results = search.results(query)\n",
    "    scraped_urls = set()\n",
    "    full_texts = []\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=httpx.Timeout(10.0, connect=3.0)) as client:\n",
    "        tasks = []\n",
    "        if 'organic' in search_results:\n",
    "            for result in search_results['organic']:\n",
    "                url = result.get('link')\n",
    "                domain = urlparse(url).netloc if url else None\n",
    "                if url and domain not in scraped_urls and len(tasks) < num_urls:\n",
    "                    tasks.append(scrape_webpage(client, url))\n",
    "                    scraped_urls.add(domain)\n",
    "\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        for result in results:\n",
    "            if isinstance(result, tuple) and result[1] > 0:\n",
    "                full_texts.append(result[0])\n",
    "\n",
    "    return \" \".join(full_texts)\n",
    "\n",
    "def query_expansion(query, num_expansions):\n",
    "    expansion_prompt = f\"\"\"\n",
    "    Given the following search query, generate {num_expansions} additional related queries that could help find more comprehensive information on the topic. The queries should be different from each other and explore various aspects of the main query. Provide only the additional queries, numbered 1-{num_expansions}.\n",
    "\n",
    "    Main query: {query}\n",
    "\n",
    "    Additional queries:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(expansion_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    expanded_queries = [query]\n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.strip() and line[0].isdigit():\n",
    "            expanded_queries.append(line.split('. ', 1)[1].strip())\n",
    "\n",
    "    return expanded_queries[:num_expansions + 1]\n",
    "\n",
    "def create_sentence_windows(text, window_size=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    windows = []\n",
    "    for i in range(len(sentences)):\n",
    "        window = \" \".join(sentences[max(0, i-window_size):min(len(sentences), i+window_size+1)])\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def generate_hypothetical_document(query):\n",
    "    hyde_prompt = f\"\"\"\n",
    "    Given the search query below, generate a hypothetical document that would be a perfect match for this query. The document should be concise, containing only 3 sentences of relevant information that directly addresses the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Document (3 sentences):\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def llm_rerank(query, documents):\n",
    "    rerank_prompt = \"\"\"\n",
    "    Given the following query and a list of document excerpts, rank the documents based on their relevance to the query. Provide the rankings as a list of numbers from 1 to {}, where 1 is the most relevant. Ensure you provide a ranking for every document.\n",
    "\n",
    "    Query: {}\n",
    "\n",
    "    Documents:\n",
    "    {}\n",
    "\n",
    "    Rankings (1 to {}):\n",
    "    \"\"\".format(len(documents), query, \"\\n\".join([f\"{i+1}. {doc.page_content[:200]}...\" for i, doc in enumerate(documents)]), len(documents))\n",
    "\n",
    "    response = llm.invoke(rerank_prompt)\n",
    "    rankings = [int(x) for x in response.content.split() if x.isdigit()]\n",
    "\n",
    "    if len(rankings) < len(documents):\n",
    "        remaining = set(range(1, len(documents) + 1)) - set(rankings)\n",
    "        rankings.extend(remaining)\n",
    "\n",
    "    sorted_docs = sorted(zip(documents, rankings), key=lambda x: x[1])\n",
    "    return sorted_docs\n",
    "\n",
    "def flashrank_rerank(query, documents, ranker):\n",
    "    rerank_request = RerankRequest(\n",
    "        query=query,\n",
    "        passages=[{\"text\": doc.page_content} for doc in documents]\n",
    "    )\n",
    "    reranked = ranker.rerank(rerank_request)\n",
    "    \n",
    "    if isinstance(reranked, list) and isinstance(reranked[0], dict):\n",
    "        sorted_results = sorted(reranked, key=lambda x: x.get('score', 0), reverse=True)\n",
    "        return [(documents[i], result.get('score', 0)) for i, result in enumerate(sorted_results)]\n",
    "    \n",
    "    elif isinstance(reranked, list) and hasattr(reranked[0], 'score'):\n",
    "        sorted_results = sorted(reranked, key=lambda x: x.score, reverse=True)\n",
    "        return [(documents[i], result.score) for i, result in enumerate(sorted_results)]\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unexpected reranked result type. Using original document order.\")\n",
    "        return [(doc, 1.0) for doc in documents]\n",
    "\n",
    "def get_hyde_retriever(vectorstores, hyde_embedding, num_docs, num_rerank, rerank_method):\n",
    "    def retriever(query):\n",
    "        all_docs = []\n",
    "        for vectorstore in vectorstores:\n",
    "            docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs)\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        unique_docs = []\n",
    "        seen_content = set()\n",
    "        for doc in all_docs:\n",
    "            content = doc.page_content\n",
    "            if content not in seen_content:\n",
    "                unique_docs.append(Document(page_content=content))\n",
    "                seen_content.add(content)\n",
    "\n",
    "        try:\n",
    "            if rerank_method == \"none\":\n",
    "                return unique_docs[:num_rerank]\n",
    "            elif rerank_method == \"llm\":\n",
    "                reranked_docs = llm_rerank(query, unique_docs)\n",
    "            elif rerank_method in [\"nano\", \"small\", \"medium_t5\", \"medium_multilang\", \"large\"]:\n",
    "                ranker = globals()[f\"ranker_{rerank_method}\"]\n",
    "                reranked_docs = flashrank_rerank(query, unique_docs, ranker)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown rerank method: {rerank_method}\")\n",
    "\n",
    "            return [doc for doc, _ in reranked_docs[:num_rerank]]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reranking with method {rerank_method}: {str(e)}\")\n",
    "            print(\"Traceback:\", traceback.format_exc())\n",
    "            print(\"Falling back to no reranking.\")\n",
    "            return unique_docs[:num_rerank]\n",
    "\n",
    "    return retriever\n",
    "\n",
    "def batch_embed_documents(documents, batch_size=512):\n",
    "    batched_embeddings = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        texts = [doc.page_content for doc in batch]\n",
    "        embeddings_batch = embeddings.embed_documents(texts)\n",
    "        batched_embeddings.extend(embeddings_batch)\n",
    "    return batched_embeddings\n",
    "\n",
    "async def process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        hyde_start = time.time()\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        hyde_time = time.time() - hyde_start\n",
    "        print(f\"hypothetical_doc length: {len(hypothetical_doc)}\")\n",
    "        print(f\"-----HyDE generation time: {hyde_time:.2f} seconds\")\n",
    "\n",
    "        embed_start = time.time()\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "        embed_time = time.time() - embed_start\n",
    "        print(f\"-----Embedding time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        ext_start = time.time()\n",
    "        expanded_queries = query_expansion(query, num_expansions)\n",
    "        ext_time = time.time() - embed_start\n",
    "        print(f\"-----Query expansion time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        scrape_start = time.time()\n",
    "        all_texts = await asyncio.gather(*[search_and_scrape(eq, num_urls) for eq in expanded_queries])\n",
    "        scrape_time = time.time() - scrape_start\n",
    "        print(f\"-----Web scraping time: {scrape_time:.2f} seconds\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        sentence_windows = create_sentence_windows(combined_text)\n",
    "        print(f\"Number of sentence windows: {len(sentence_windows)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=window) for window in sentence_windows]\n",
    "\n",
    "        vectorstore_start = time.time()\n",
    "        vectorstores = []\n",
    "        for i in range(0, len(index_documents), 256):\n",
    "            batch = index_documents[i:i + 256]\n",
    "\n",
    "            batch_embeddings = batch_embed_documents(batch)\n",
    "\n",
    "            texts = [doc.page_content for doc in batch]\n",
    "\n",
    "            vectorstore = FAISS.from_embeddings(\n",
    "                embedding=embeddings,\n",
    "                text_embeddings=list(zip(texts, batch_embeddings))\n",
    "            )\n",
    "            vectorstores.append(vectorstore)\n",
    "\n",
    "        vectorstore_time = time.time() - vectorstore_start\n",
    "        print(f\"-----Vectorstore creation time: {vectorstore_time:.2f} seconds\")\n",
    "\n",
    "        retrieval_start = time.time()\n",
    "        retriever = get_hyde_retriever(vectorstores, hyde_embedding, num_docs, num_rerank, rerank_method)\n",
    "        retrieved_docs = retriever(query)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        print(f\"-----Retrieval and reranking time: {retrieval_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"Number of retrieved and reranked documents: {len(retrieved_docs)}\")\n",
    "\n",
    "        context_docs = [doc.page_content for doc in retrieved_docs]\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "\n",
    "        total_processing_time = hyde_time + embed_time + scrape_time + vectorstore_time + retrieval_time\n",
    "        print(f\"-----Total processing time before answer generation: {total_processing_time:.2f} seconds\")\n",
    "\n",
    "        answer_start = time.time()\n",
    "        prompt_template = \"\"\"\n",
    "        Use the following context to answer the question. Before answering the question generate a reasoning step. then answer.\n",
    "        If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        chosen_llm = llm_70b if use_70b_model else llm_8b\n",
    "\n",
    "        rag_chain = prompt | chosen_llm | StrOutputParser()\n",
    "        answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "        answer_time = time.time() - answer_start\n",
    "        print(f\"-----Answer generation time: {answer_time:.2f} seconds\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*120)\n",
    "        print(\"Final Answer:\\n\", answer)\n",
    "        print(\"-\"*120)\n",
    "\n",
    "        return answer, context_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\", []\n",
    "\n",
    "def gradio_interface(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "    answer, context_docs = asyncio.run(process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model))\n",
    "\n",
    "    sys.stdout = old_stdout\n",
    "    captured_output = buffer.getvalue()\n",
    "\n",
    "    truncated_docs = [f\"Document {i+1}: {doc[:150]}...\" for i, doc in enumerate(context_docs)]\n",
    "    truncated_context = \"\\n\\n\".join(truncated_docs)\n",
    "\n",
    "    captured_output += f\"\\n\\nContext used for answer generation (first 150 characters of each document, {len(context_docs)} documents in total):\\n\" + truncated_context\n",
    "\n",
    "    return captured_output\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your query\"),\n",
    "        gr.Slider(minimum=0, maximum=3, value=1, step=1, label=\"Number of query expansions\"),\n",
    "        gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of URLs to scrape per extended query\"),\n",
    "        gr.Slider(minimum=20, maximum=80, value=80, step=1, label=\"Number of documents to retrieve with HyDE\"),\n",
    "        gr.Slider(minimum=10, maximum=80, value=50, step=1, label=\"Number of documents to keep after retrieval/reranking\"),\n",
    "        gr.Radio([\"none\", \"llm\", \"nano\", \"small\", \"medium_t5\", \"medium_multilang\"], label=\"Reranking method\", value=\"none\"),\n",
    "        gr.Checkbox(label=\"Use 70B model for QA (unchecked uses 8B)\", value=False)\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Advanced RAG Query Processing\",\n",
    "    description=\"Enter a query and adjust parameters to get a detailed answer based on web search and document analysis.\"\n",
    ")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     iface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 100 questions:\n",
      "1. What is the average airspeed velocity of an unladen swallow?\n",
      "2. Which ancient civilization built the first known suspension bridge?\n",
      "3. What is the most widely spoken language in the world?\n",
      "4. Who is the author of the famous painting \"The Ambassadors\"?\n",
      "5. What is the chemical composition of the human nose?\n",
      "6. In what year did the first computer bug occur?\n",
      "7. Who is the founder of the philosophical school of Stoicism?\n",
      "8. What is the world's largest living structure, according to the Guinness World Records?\n",
      "9. Which planet in our solar system has the longest day?\n",
      "10. Who wrote the famous poem \"The Love Song of J. Alfred Prufrock\"?\n",
      "11. What is the highest recorded temperature on Earth?\n",
      "12. Who is the inventor of the first successful polio vaccine?\n",
      "13. What is the name of the largest star known to science?\n",
      "14. In what year did the first email send?\n",
      "15. Who is the author of the famous novel \"One Hundred Years of Solitude\"?\n",
      "16. What is the deepest part of the ocean?\n",
      "17. Who is the founder of the company that developed the first smartphone?\n",
      "18. What is the most widely used programming language in the world?\n",
      "19. Who is the lead singer of the rock band Queen?\n",
      "20. What is the chemical symbol for gold?\n",
      "21. In what year did the first human walk on the moon?\n",
      "22. Who is the author of the famous play \"Romeo and Juliet\"?\n",
      "23. What is the world's largest waterfall, by volume of water?\n",
      "24. Who is the founder of the company that developed the first web browser?\n",
      "25. What is the highest mountain peak in the solar system?\n",
      "26. Who is the lead singer of the rock band Guns N' Roses?\n",
      "27. What is the most widely spoken language in the United States?\n",
      "28. Who is the author of the famous novel \"To Kill a Mockingbird\"?\n",
      "29. What is the chemical composition of the human brain?\n",
      "30. In what year did the first successful heart transplant occur?\n",
      "31. Who is the founder of the company that developed the first microprocessor?\n",
      "32. What is the world's largest desert?\n",
      "33. Who is the lead singer of the rock band U2?\n",
      "34. What is the highest recorded wind speed on Earth?\n",
      "35. Who is the author of the famous poem \"The Road Not Taken\"?\n",
      "36. What is the chemical symbol for silver?\n",
      "37. In what year did the first human fly in space?\n",
      "38. Who is the founder of the company that developed the first personal computer?\n",
      "39. What is the world's largest island?\n",
      "40. Who is the lead singer of the rock band AC/DC?\n",
      "41. What is the most widely used social media platform in the world?\n",
      "42. Who is the author of the famous novel \"The Great Gatsby\"?\n",
      "43. What is the chemical composition of the human eye?\n",
      "44. In what year did the first successful kidney transplant occur?\n",
      "45. Who is the founder of the company that developed the first laser printer?\n",
      "46. What is the world's largest mountain range?\n",
      "47. Who is the lead singer of the rock band Led Zeppelin?\n",
      "48. What is the highest recorded temperature on Mars?\n",
      "49. Who is the author of the famous play \"Hamlet\"?\n",
      "50. What is the chemical symbol for copper?\n",
      "51. In what year did the first human walk on the moon's surface?\n",
      "52. Who is the founder of the company that developed the first ATM?\n",
      "53. What is the world's largest river, by discharge volume?\n",
      "54. Who is the lead singer of the rock band Pink Floyd?\n",
      "55. What is the most widely used database management system in the world?\n",
      "56. Who is the author of the famous novel \"The Catcher in the Rye\"?\n",
      "57. What is the chemical composition of the human skin?\n",
      "58. In what year did the first successful liver transplant occur?\n",
      "59. Who is the founder of the company that developed the first smartphone app store?\n",
      "60. What is the world's largest waterfall, by height?\n",
      "61. Who is the lead singer of the rock band The Rolling Stones?\n",
      "62. What is the highest recorded wind speed on Mars?\n",
      "63. Who is the author of the famous poem \"The Waste Land\"?\n",
      "64. What is the chemical symbol for tin?\n",
      "65. In what year did the first human fly in a hot air balloon?\n",
      "66. Who is the founder of the company that developed the first email client?\n",
      "67. What is the world's largest lake, by surface area?\n",
      "68. Who is the lead singer of the rock band Aerosmith?\n",
      "69. What is the most widely used web framework in the world?\n",
      "70. Who is the author of the famous novel \"The Picture of Dorian Gray\"?\n",
      "71. What is the chemical composition of the human hair?\n",
      "72. In what year did the first successful bone marrow transplant occur?\n",
      "73. Who is the founder of the company that developed the first 3D printer?\n",
      "74. What is the world's largest mountain peak, by base-to-peak height?\n",
      "75. Who is the lead singer of the rock band Van Halen?\n",
      "76. What is the highest recorded temperature on Venus?\n",
      "77. Who is the author of the famous play \"Macbeth\"?\n",
      "78. What is the chemical symbol for lead?\n",
      "79. In what year did the first human walk on the moon's surface?\n",
      "80. Who is the founder of the company that developed the first GPS device?\n",
      "81. What is the world's largest river, by length?\n",
      "82. Who is the lead singer of the rock band Bon Jovi?\n",
      "83. What is the most widely used operating system in the world?\n",
      "84. Who is the author of the famous novel \"The Adventures of Huckleberry Finn\"?\n",
      "85. What is the chemical composition of the human nail?\n",
      "86. In what year did the first successful lung transplant occur?\n",
      "87. Who is the founder of the company that developed the first virtual reality headset?\n",
      "88. What is the world's largest waterfall, by volume of water?\n",
      "89. Who is the lead singer of the rock band Queen?\n",
      "90. What is the highest recorded wind speed on Earth?\n",
      "91. Who is the author of the famous poem \"The Raven\"?\n",
      "92. What is the chemical symbol for mercury?\n",
      "93. In what year did the first human fly in a jet-powered aircraft?\n",
      "94. Who is the founder of the company that developed the first smartphone with a touchscreen?\n",
      "95. What is the world's largest island, by population?\n",
      "96. Who is the lead singer of the rock band Guns N' Roses?\n",
      "97. What is the most widely used programming language in the world?\n",
      "98. Who is the author of the famous novel \"The Scarlet Letter\"?\n",
      "99. What is the chemical composition of the human tooth?\n",
      "100. In what year did the first human walk on the moon's surface?\n"
     ]
    }
   ],
   "source": [
    "#### evaluation \n",
    "\n",
    "# LLM for generating questions\n",
    "llm_generator = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0.6)\n",
    "\n",
    "# Question generation prompt\n",
    "question_gen_template = \"\"\"Generate exactly {num_questions} diverse and challenging questions that would require complex web searches to answer. The questions should:\n",
    "\n",
    "1. Cover a wide range of topics (e.g., science, history, current events, technology, arts, code)\n",
    "2. Include some questions that are easy to search and find solutions for\n",
    "3. Avoid long questions\n",
    "4. Include some easy factual questions in the list\n",
    "5. Ensure there is only one question per query. Query should NOT be multiple questions\n",
    "\n",
    "Please provide the questions as a numbered list, starting from 1 and ending at {num_questions}.\n",
    "\n",
    "Generated Questions:\"\"\"\n",
    "\n",
    "question_gen_prompt = PromptTemplate.from_template(question_gen_template)\n",
    "\n",
    "def generate_questions(num_questions, max_attempts=3):\n",
    "    for attempt in range(max_attempts):\n",
    "        question_gen_chain = question_gen_prompt | llm_generator | StrOutputParser()\n",
    "        questions_text = question_gen_chain.invoke({\"num_questions\": num_questions})\n",
    "\n",
    "        questions = []\n",
    "        for line in questions_text.split('\\n'):\n",
    "            match = re.match(r'^\\s*\\d+\\.\\s*(.+)$', line)\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                questions.append(question)\n",
    "\n",
    "        if len(questions) == num_questions:\n",
    "            return questions\n",
    "\n",
    "        print(f\"Attempt {attempt + 1}: Generated {len(questions)} questions instead of {num_questions}. Retrying...\")\n",
    "\n",
    "    raise ValueError(f\"Failed to generate exactly {num_questions} questions after {max_attempts} attempts.\")\n",
    "\n",
    "# Generate questions\n",
    "num_questions = 100\n",
    "try:\n",
    "    evaluation_questions = generate_questions(num_questions)\n",
    "    print(f\"Successfully generated {len(evaluation_questions)} questions:\")\n",
    "    for i, question in enumerate(evaluation_questions, 1):\n",
    "        print(f\"{i}. {question}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 625\n",
      "-----HyDE generation time: 0.91 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://science.nasa.gov/climate-change/causes/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nrdc.org/stories/greenhouse-effect-101 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nrdc.org/stories/what-are-causes-climate-change \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.un.org/en/climatechange/science/causes-effects-climate-change \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.epa.gov/ghgemissions/overview-greenhouse-gases \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.un.org/en/climatechange/science/causes-effects-climate-change\n",
      "-----Web scraping time: 2.05 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.82 seconds\n",
      "-----Retrieval and reranking time: 0.32 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.43 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.98 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the main causes of climate change, which requires identifying the primary factors contributing to the global warming trend observed since the mid-20th century.\n",
      "\n",
      "Answer: According to the provided context, the main causes of climate change are human activities, specifically the expansion of the \"greenhouse effect\" due to the burning of fossil fuels for energy. This is supported by the text, which states: \"Human activities are driving the global warming trend observed since the mid-20th century.\" Additionally, the text mentions that the unchecked burning of fossil fuels is a significant contributor to climate change.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_fireworks import ChatFireworks\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Initialize the judge model (405B LLaMA)\n",
    "judge_model = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
    "\n",
    "def evaluate_answer_quality(question: str, answer: str, judge_model: Any) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer completely addresses the question.\n",
    "    Returns 1 if yes, 0 if no.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert evaluator. Your task is to determine if the given answer completely addresses the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \n",
    "    Does the answer completely address the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "def evaluate_document_selection(question: str, all_docs: List[str], selected_docs: List[str], judge_model: Any) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the selected documents are the best 10 out of the 80 to answer the question.\n",
    "    Returns 1 if yes, 0 if no.\n",
    "    \"\"\"\n",
    "    all_docs_text = \"\\n\".join([f\"{i+1}. {doc}...\" for i, doc in enumerate(all_docs)])\n",
    "    selected_indices = [all_docs.index(doc) + 1 for doc in selected_docs]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert information retrieval system. Your task is to determine if the selected documents are the best 10 out of the given 80 for answering the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Here are the first 200 characters of all 80 retrieved documents:\n",
    "    {all_docs_text}\n",
    "    \n",
    "    The system selected the following documents (by index): {', '.join(map(str, selected_indices))}\n",
    "    \n",
    "    Are these selected documents the best 10 out of the 80 for answering the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "async def evaluate_rag_system(question: str, answer: str, all_docs: List[str], selected_docs: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Evaluate the RAG system's performance.\n",
    "    \"\"\"\n",
    "    answer_correct = evaluate_answer_quality(question, answer, judge_model)\n",
    "    docs_correct = evaluate_document_selection(question, all_docs, selected_docs, judge_model)\n",
    "    \n",
    "    return {\n",
    "        \"answer_correct\": answer_correct,\n",
    "        \"docs_correct\": docs_correct\n",
    "    }\n",
    "\n",
    "async def run_evaluation():\n",
    "    question = \"What are the main causes of climate change?\"\n",
    "    answer, context_docs = await process_query(question, num_expansions=1, num_urls=3, num_docs=80, num_rerank=10, rerank_method=\"nano\", use_70b_model=False)\n",
    "    \n",
    "    all_docs = context_docs[:80]  # All 80 retrieved documents\n",
    "    selected_docs = context_docs[:10]  # Top 10 after reranking\n",
    "    \n",
    "    evaluation_results = await evaluate_rag_system(question, answer, all_docs, selected_docs)\n",
    "    \n",
    "    print(f\"Evaluation Results:\")\n",
    "    print(f\"Answer Correctness: {evaluation_results['answer_correct']}\")\n",
    "    print(f\"Top 10 Documents Correctness: {evaluation_results['docs_correct']}\")\n",
    "\n",
    "# Run the evaluation\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 715\n",
      "-----HyDE generation time: 0.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.youtube.com/watch?v=pJS4QDUtzzI \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://medium.com/human-nature-group/what-is-the-air-speed-velocity-of-an-unladen-swallow-4c17087bbf33 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://medium.com/human-nature-group/what-is-the-air-speed-velocity-of-an-unladen-swallow-4c17087bbf33 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://style.org/unladenswallow/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://interestingengineering.com/science/monty-python-and-the-holy-grail-airspeed-velocity-of-an-unladen-swallow \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://interestingengineering.com/science/monty-python-and-the-holy-grail-airspeed-velocity-of-an-unladen-swallow \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.34 seconds\n",
      "Combined text length: 23125 characters\n",
      "Number of sentence windows: 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.97 seconds\n",
      "-----Retrieval and reranking time: 0.09 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.97 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " To answer the question, I will follow a reasoning step.\n",
      "\n",
      "Reasoning step: The text provides multiple estimates of the airspeed velocity of an unladen European Swallow, based on different Strouhal values and formulas. To determine the average airspeed velocity, I will look for the most commonly cited or accurate estimate.\n",
      "\n",
      "Answer: According to the text, the airspeed velocity of an unladen European Swallow can be estimated to be roughly 10 meters per second, using the formula U  3 f A, where f is the frequency (15 beats per second) and A is the amplitude (0.22 meters per beat). This estimate is mentioned in the text as being accurate, though perhaps coincidental.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the average airspeed velocity of an unladen swallow?\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 0\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 433\n",
      "-----HyDE generation time: 0.64 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://brainly.com/question/22527717 \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-history-of-suspension-bridges-When-was-the-first-one-built-and-in-which-country \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://brainly.com/question/22527717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.sciencedirect.com/topics/engineering/suspension-bridges \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nytimes.com/2007/05/08/science/08bridg.html \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Suspension_bridge \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.nytimes.com/2007/05/08/science/08bridg.html\n",
      "Error response 403 while requesting https://www.sciencedirect.com/topics/engineering/suspension-bridges\n",
      "-----Web scraping time: 2.98 seconds\n",
      "Combined text length: 5222 characters\n",
      "Number of sentence windows: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.57 seconds\n",
      "-----Retrieval and reranking time: 0.09 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.65 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The text mentions that the earliest suspension bridges were ropes slung across a chasm, but it does not specify which ancient civilization built the first known suspension bridge. However, it does mention Thangtong Gyalpo, a Tibetan siddha and bridge-builder, who originated the use of iron chains in his version of simple suspension bridges in 1433.\n",
      "\n",
      "Answer: I don't have enough information to answer that question. The text does not provide information on which ancient civilization built the first known suspension bridge.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which ancient civilization built the first known suspension bridge?\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 466\n",
      "-----HyDE generation time: 0.50 seconds\n",
      "-----Embedding time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.visualcapitalist.com/top-languages-spoken-in-the-world/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.berlitz.com/blog/most-spoken-languages-world \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/languages-by-number-of-native-speakers-2228882 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.visualcapitalist.com/top-languages-spoken-in-the-world/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.babbel.com/en/magazine/the-10-most-spoken-languages-in-the-world \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 4.82 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.70 seconds\n",
      "-----Retrieval and reranking time: 0.17 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 7.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.74 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the most widely spoken language in the world, which implies that we need to consider the total number of native speakers, not just the number of speakers in a particular region or country. We also need to consider the context of the provided text, which mentions the difficulty of determining the most spoken languages due to the complexity of language classification and the availability of reliable data.\n",
      "\n",
      "Answer: According to the text, the most widely spoken language in the world is Chinese, with approximately 1.3 billion native speakers, followed closely by Spanish with 486 million native speakers. However, if we consider the total number of speakers, including non-native speakers, the ranking may be different.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the most widely spoken language in the world?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 495\n",
      "-----HyDE generation time: 1.73 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/The_Ambassadors_(Holbein) \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://smarthistory.org/hans-holbein-the-younger-the-ambassadors/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://about.jstor.org/blog/a-closer-look-at-hans-holbeins-the-ambassadors/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://smarthistory.org/hans-holbein-the-younger-the-ambassadors/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.artsy.net/article/artsy-editorial-decoding-symbolism-hans-holbeins-ambassadors \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.artsy.net/article/artsy-editorial-decoding-symbolism-hans-holbeins-ambassadors \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.39 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.52 seconds\n",
      "-----Retrieval and reranking time: 0.15 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.57 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the famous painting \"The Ambassadors\". To answer this question, we need to identify the person who created the painting. The text mentions Hans Holbein the Younger as the artist who painted \"The Ambassadors\" in 1533.\n",
      "\n",
      "Answer: Hans Holbein the Younger.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the author of the famous painting \"The Ambassadors\"?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 515\n",
      "-----HyDE generation time: 0.68 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.sciencedirect.com/topics/neuroscience/nose \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Human_nose \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://my.clevelandclinic.org/health/body/21778-nose \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.sciencedirect.com/topics/neuroscience/nose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3199822/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/books/NBK544232/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3199822/\n",
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/books/NBK544232/\n",
      "-----Web scraping time: 2.70 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.39 seconds\n",
      "-----Retrieval and reranking time: 0.20 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.51 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " I don't have enough information to answer that question.\n",
      "\n",
      "Reasoning step: The provided context describes the anatomy and functions of the human nose, but it does not mention the chemical composition of the nose. To answer this question, I would need information about the types and proportions of molecules that make up the nose, such as proteins, carbohydrates, lipids, and other biomolecules.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the chemical composition of the human nose?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 0\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 568\n",
      "-----HyDE generation time: 0.66 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-was-the-first-computer-bug \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/worlds-first-computer-bug/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/worlds-first-computer-bug/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://lunduke.substack.com/p/the-story-of-the-first-computer-bug \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.globalapptesting.com/blog/the-worlds-first-computer-bug-global-app-testing \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://daily.jstor.org/the-bug-in-the-computer-bug-story/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.43 seconds\n",
      "Combined text length: 22286 characters\n",
      "Number of sentence windows: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.11 seconds\n",
      "-----Retrieval and reranking time: 0.21 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.73 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year in which the first computer bug occurred. To answer this question, I need to find the relevant information in the provided context.\n",
      "\n",
      "Answer: According to the context, the first computer bug occurred on September 9, 1947.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In what year did the first computer bug occur?\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 561\n",
      "-----HyDE generation time: 0.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.reddit.com/r/Stoicism/comments/10dr49a/core_beliefs_required_to_call_yourself_a_stoic/ \"HTTP/1.1 302 Found\"\n",
      "INFO:httpx:HTTP Request: GET https://plato.stanford.edu/ENTRIES/stoicism/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://dailystoic.com/9-core-stoic-beliefs/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Stoicism \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 302 while requesting https://www.reddit.com/r/Stoicism/comments/10dr49a/core_beliefs_required_to_call_yourself_a_stoic/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://becomingbetter.org/10-essential-principles-and-practices-of-stoicism/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://iep.utm.edu/stoicism/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.22 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.07 seconds\n",
      "-----Retrieval and reranking time: 0.16 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.42 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the founder of the Stoic school, which is a specific philosophical school. To answer this question, I need to identify the individual who is credited with founding the Stoic school.\n",
      "\n",
      "Answer: According to the provided context, the Stoic school was founded around 300 BCE by Zeno of Citium.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the founder of the philosophical school of Stoicism?\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 0\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 372\n",
      "-----HyDE generation time: 0.58 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://brainly.com/question/35555711 \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/606952-largest-living-organism \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/606952-largest-living-organism \"HTTP/1.1 301 Moved Permanently\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://brainly.com/question/35555711\n",
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/606952-largest-living-organism\n",
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/606952-largest-living-organism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.esa.int/Applications/Observing_the_Earth/Earth_from_Space_World_s_largest_living_structure \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://oceanservice.noaa.gov/facts/gbrlargeststructure.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://oceanservice.noaa.gov/facts/gbrlargeststructure.html \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.98 seconds\n",
      "Combined text length: 6356 characters\n",
      "Number of sentence windows: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.79 seconds\n",
      "-----Retrieval and reranking time: 0.09 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.45 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the world's largest living structure according to the Guinness World Records, but the provided context does not mention the Guinness World Records. However, it does mention that the Great Barrier Reef is the largest living structure on Earth and the only one visible from space.\n",
      "\n",
      "Answer: I don't have enough information to answer that question.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the world's largest living structure, according to the Guinness World Records?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 0\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 385\n",
      "-----HyDE generation time: 0.52 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.reuters.com/lifestyle/science/how-long-is-solar-systems-longest-day-venus-has-answer-2021-05-03/ \"HTTP/1.1 401 HTTP Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.universetoday.com/84663/which-planet-has-the-longest-day/ \"HTTP/1.1 503 Service Temporarily Unavailable\"\n",
      "INFO:httpx:HTTP Request: GET https://www.rmg.co.uk/stories/topics/solar-system-data \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 401 while requesting https://www.reuters.com/lifestyle/science/how-long-is-solar-systems-longest-day-venus-has-answer-2021-05-03/\n",
      "Error response 503 while requesting https://www.universetoday.com/84663/which-planet-has-the-longest-day/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://spaceplace.nasa.gov/days/en/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://spaceplace.nasa.gov/days/en/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.10 seconds\n",
      "Combined text length: 14250 characters\n",
      "Number of sentence windows: 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.95 seconds\n",
      "-----Retrieval and reranking time: 0.22 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.98 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.52 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: To determine which planet in our solar system has the longest day, we need to look at the table provided in the context, which lists the day length for each planet in hours.\n",
      "\n",
      "Answer: Mars has the longest day, with a day length of 25 hours.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which planet in our solar system has the longest day?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 481\n",
      "-----HyDE generation time: 0.67 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.poetryfoundation.org/poetrymagazine/poems/44212/the-love-song-of-j-alfred-prufrock \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/The_Love_Song_of_J._Alfred_Prufrock \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sparknotes.com/poetry/eliot/section1/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.owleyes.org/text/love-song/analysis/historical-context \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.litcharts.com/poetry/t-s-eliot/the-love-song-of-j-alfred-prufrock \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.sparknotes.com/poetry/eliot/section1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.bu.edu/writingprogram/journal/past-issues/issue-11/immerwahr/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.01 seconds\n",
      "Combined text length: 23444 characters\n",
      "Number of sentence windows: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.83 seconds\n",
      "-----Retrieval and reranking time: 0.17 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.50 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the poem \"The Love Song of J. Alfred Prufrock\", which is mentioned in the context as being written by T. S. Eliot.\n",
      "\n",
      "Answer: T. S. Eliot.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who wrote the famous poem \"The Love Song of J. Alfred Prufrock\"?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 423\n",
      "-----HyDE generation time: 0.72 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/highest-recorded-temperature \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://wmo.asu.edu/content/world-highest-temperature \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.twinkl.com/teaching-wiki/the-hottest-place-on-earth \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Highest_temperature_recorded_on_Earth \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/highest-recorded-temperature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.autoeurope.com/travel-blog/the-worlds-coldest-and-warmest-places/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sciencefocus.com/planet-earth/hottest-place-on-earth \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.autoeurope.com/travel-blog/the-worlds-coldest-and-warmest-places/\n",
      "-----Web scraping time: 2.44 seconds\n",
      "Combined text length: 13333 characters\n",
      "Number of sentence windows: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.30 seconds\n",
      "-----Retrieval and reranking time: 0.18 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 1.03 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the highest recorded temperature on Earth, which requires identifying the highest temperature measurement among the three major ways of measurement: air, ground, and satellite observation.\n",
      "\n",
      "Answer: According to the provided context, the current official highest registered air temperature on Earth is 56.7C (134.1F), recorded on 10 July 1913 at Furnace Creek Ranch, in Death Valley in the United States. However, it is worth noting that this record is currently under scrutiny due to concerns about its legitimacy, and if it were to be decertified, the highest established recorded air temperature on Earth would be 54.0C (129.2F), also recorded in Death Valley on 20 June 2013, and in Mitribah, Kuwait on 21 July 2016.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the highest recorded temperature on Earth?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 594\n",
      "-----HyDE generation time: 0.82 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-polio-vaccination \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-polio-vaccination \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Jonas_Salk \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/biography/Jonas-Salk \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/\n",
      "-----Web scraping time: 2.44 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.41 seconds\n",
      "-----Retrieval and reranking time: 0.14 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.74 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " The reasoning step is to identify the key information in the context that relates to the question. In this case, the context mentions Jonas Salk and his work on the polio vaccine, but it does not explicitly state that he is the inventor of the first successful polio vaccine.\n",
      "\n",
      "The answer is: Jonas Salk.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the inventor of the first successful polio vaccine?\n",
      "Answer Correctness: 1\n",
      "Top 10 Documents Correctness: 1\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 579\n",
      "-----HyDE generation time: 0.68 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-largest-star-in-our-universe-What-are-its-properties-mass-diameter \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_largest_stars \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://science.howstuffworks.com/largest-star-in-the-universe.htm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.space.com/41290-biggest-star.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.space.com/41290-biggest-star.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.usatoday.com/story/tech/science/2023/02/16/largest-star-universe-red-hypergiant/11075755002/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.20 seconds\n",
      "Combined text length: 24085 characters\n",
      "Number of sentence windows: 179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.69 seconds\n",
      "-----Retrieval and reranking time: 0.15 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.92 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.48 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the name of the largest star known to science, and the context provides information about a star called UY Scuti, which is described as the largest known star in the universe.\n",
      "\n",
      "Answer: UY Scuti.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the name of the largest star known to science?\n",
      "Answer Correctness: 0\n",
      "Top 10 Documents Correctness: 0\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 435\n",
      "-----HyDE generation time: 0.60 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_email \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_email \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/Do-you-know-who-was-the-first-person-to-use-email-and-who-was-the-first-person-to-receive-it \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mail.com/blog/posts/fiftieth-anniversary-of-email/20/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mail.com/blog/posts/fiftieth-anniversary-of-email/20/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://www.emailonacid.com/blog/article/email-marketing/history-of-email/: \n",
      "-----Web scraping time: 9.57 seconds\n",
      "Combined text length: 20224 characters\n",
      "Number of sentence windows: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_fireworks import ChatFireworks\n",
    "\n",
    "# Initialize the judge model (405B LLaMA)\n",
    "judge_model = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
    "\n",
    "def evaluate_answer_quality(question: str, answer: str) -> int:\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert evaluator. Your task is to determine if the given answer completely addresses the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \n",
    "    Does the answer completely address the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "def evaluate_document_selection(question: str, all_docs: list, selected_docs: list) -> int:\n",
    "    all_docs_text = \"\\n\".join([f\"{i+1}. {doc}...\" for i, doc in enumerate(all_docs)])\n",
    "    selected_indices = [all_docs.index(doc) + 1 for doc in selected_docs]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert information retrieval system. Your task is to determine if the selected documents are the best 10 out of the given 80 for answering the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Here are the 80 retrieved documents:\n",
    "    {all_docs_text}\n",
    "    \n",
    "    The system selected the following documents (by index): {', '.join(map(str, selected_indices))}\n",
    "    \n",
    "    Are these selected documents the best 10 out of the 80 for answering the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "async def run_evaluation(num_questions: int = 100):\n",
    "    questions = evaluation_questions\n",
    "    # [\n",
    "    #     \"What are the main causes of climate change?\",\n",
    "    #     \"How does artificial intelligence impact job markets?\",\n",
    "    #     \"What are the benefits and drawbacks of renewable energy sources?\",\n",
    "    #     \"How does diet affect mental health?\",\n",
    "    #     \"What are the major challenges in space exploration?\"\n",
    "    # ]\n",
    "    \n",
    "    results = []\n",
    "    total_answer_correct = 0\n",
    "    total_docs_correct = 0\n",
    "    \n",
    "    for question in questions[:num_questions]:\n",
    "        answer, context_docs = await process_query(question, num_expansions=1, num_urls=3, num_docs=80, num_rerank=10, rerank_method=\"nano\", use_70b_model=False)\n",
    "        \n",
    "        all_docs = context_docs[:80]  # All 80 retrieved documents\n",
    "        selected_docs = context_docs[:10]  # Top 10 after reranking\n",
    "        \n",
    "        answer_correct = evaluate_answer_quality(question, answer)\n",
    "        docs_correct = evaluate_document_selection(question, all_docs, selected_docs)\n",
    "        \n",
    "        total_answer_correct += answer_correct\n",
    "        total_docs_correct += docs_correct\n",
    "        \n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"answer_correctness\": answer_correct,\n",
    "            \"top_10_docs_correctness\": docs_correct,\n",
    "            \"all_docs\": all_docs,\n",
    "            \"selected_docs\": selected_docs\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer Correctness: {answer_correct}\")\n",
    "        print(f\"Top 10 Documents Correctness: {docs_correct}\")\n",
    "        print(\"---\")\n",
    "    \n",
    "    avg_answer_correct = total_answer_correct / num_questions\n",
    "    avg_docs_correct = total_docs_correct / num_questions\n",
    "    print(f\"\\nAverage Results over {num_questions} questions:\")\n",
    "    print(f\"Average Answer Correctness: {avg_answer_correct:.2f}\")\n",
    "    print(f\"Average Top 10 Documents Correctness: {avg_docs_correct:.2f}\")\n",
    "    \n",
    "    # Save results to a JSON file\n",
    "    output = {\n",
    "        \"results\": results,\n",
    "        \"average_answer_correctness\": avg_answer_correct,\n",
    "        \"average_top_10_docs_correctness\": avg_docs_correct\n",
    "    }\n",
    "    \n",
    "    with open('evaluation_results.json', 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    print(\"\\nResults have been saved to 'evaluation_results.json'\")\n",
    "\n",
    "# To run the evaluation, use:\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating none reranker:   0%|          | 0/100 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 715\n",
      "-----HyDE generation time: 1.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://medium.com/human-nature-group/what-is-the-air-speed-velocity-of-an-unladen-swallow-4c17087bbf33 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://medium.com/human-nature-group/what-is-the-air-speed-velocity-of-an-unladen-swallow-4c17087bbf33 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.youtube.com/watch?v=pJS4QDUtzzI \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-airspeed-velocity-of-an-unladen-swallow-1 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://interestingengineering.com/science/monty-python-and-the-holy-grail-airspeed-velocity-of-an-unladen-swallow \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://interestingengineering.com/science/monty-python-and-the-holy-grail-airspeed-velocity-of-an-unladen-swallow \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.79 seconds\n",
      "Combined text length: 18304 characters\n",
      "Number of sentence windows: 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.11 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 7.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 1.34 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The text mentions that the Strouhal number, which is a measure of the efficiency of a bird's flight pattern, averages between 0.2 and 0.4 for most birds. However, it does not provide a specific value for the airspeed velocity of an unladen swallow. To answer the question, we need to use the Strouhal ratio equation to estimate the airspeed velocity of the swallow.\n",
      "\n",
      "Answer: Unfortunately, the text does not provide enough information to calculate the exact airspeed velocity of an unladen swallow. However, we can use the Strouhal ratio equation to make an estimate. Let's assume that the frequency of wingbeats of a swallow is around 4-5 Hz (a typical value for birds), and the amplitude of its wingbeats is around 0.1-0.2 meters (a rough estimate). Using the Strouhal ratio equation, we can estimate the airspeed velocity of the swallow to be around 5-11 meters per second (m/s). However, please note that this is a very rough estimate and should be taken as a rough order of magnitude rather than a precise value.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   1%|          | 1/100 [00:11<18:12, 11.04s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 433\n",
      "-----HyDE generation time: 0.42 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.nytimes.com/2007/05/08/science/08bridg.html \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://brainly.com/question/22527717 \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-history-of-suspension-bridges-When-was-the-first-one-built-and-in-which-country \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sciencedirect.com/topics/engineering/suspension-bridges \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Suspension_bridge \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Suspension_bridge \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.nytimes.com/2007/05/08/science/08bridg.html\n",
      "Error response 403 while requesting https://brainly.com/question/22527717\n",
      "Error response 403 while requesting https://www.sciencedirect.com/topics/engineering/suspension-bridges\n",
      "-----Web scraping time: 2.56 seconds\n",
      "Combined text length: 10222 characters\n",
      "Number of sentence windows: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.90 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.64 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The context provided mentions the history of suspension bridges, including the use of ropes and iron chains in early bridges. It also mentions the Tibetan siddha and bridge-builder Thangtong Gyalpo, who originated the use of iron chains in his version of simple suspension bridges in 1433. However, it does not explicitly mention an ancient civilization that built the first known suspension bridge.\n",
      "\n",
      "Answer: I don't have enough information to answer that question.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   2%|         | 2/100 [00:17<13:48,  8.45s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 466\n",
      "-----HyDE generation time: 0.76 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.visualcapitalist.com/top-languages-spoken-in-the-world/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.berlitz.com/blog/most-spoken-languages-world \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/languages-by-number-of-native-speakers-2228882 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.visualcapitalist.com/top-languages-spoken-in-the-world/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.babbel.com/en/magazine/the-10-most-spoken-languages-in-the-world \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 4.27 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.66 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.88 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 1.04 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the most widely spoken language in the world, which implies that we need to consider both native and non-native speakers. However, the provided context only mentions the number of native speakers for each language. To answer the question accurately, we need to look for information that includes both native and non-native speakers.\n",
      "\n",
      "Answer: According to the context, the most widely spoken language in the world is Chinese, with 1.3 billion native speakers. However, the context also mentions that the numbers vary widely, and Ethnologue puts the number of native speakers at 1.3 billion. But to answer the question accurately, we need to look at the list of languages by total number of speakers, which is mentioned in the context but not explicitly provided. According to the list of languages by total number of speakers, the most widely spoken language in the world is actually English, with 379,682,200 native speakers, but it's not clear if this number includes non-native speakers.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   3%|         | 3/100 [00:27<14:35,  9.03s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 495\n",
      "-----HyDE generation time: 0.78 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://smarthistory.org/hans-holbein-the-younger-the-ambassadors/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/The_Ambassadors_(Holbein) \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://smarthistory.org/hans-holbein-the-younger-the-ambassadors/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/maziar/eval_ranking/.venv/lib/python3.10/site-packages/bs4/builder/_lxml.py:291: RuntimeWarning: coroutine 'run_evaluation' was never awaited\n",
      "  for attr, value in list(attrs.items()):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "INFO:httpx:HTTP Request: GET https://about.jstor.org/blog/a-closer-look-at-hans-holbeins-the-ambassadors/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.artsy.net/article/artsy-editorial-decoding-symbolism-hans-holbeins-ambassadors \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.artsy.net/article/artsy-editorial-decoding-symbolism-hans-holbeins-ambassadors \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.60 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.87 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.82 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the famous painting \"The Ambassadors\". To answer this question, we need to identify the person who created the painting. The text mentions Hans Holbein the Younger as the artist who painted \"The Ambassadors\" in 1533.\n",
      "\n",
      "Answer: Hans Holbein the Younger.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   4%|         | 4/100 [00:35<13:59,  8.75s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 515\n",
      "-----HyDE generation time: 0.74 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.sciencedirect.com/topics/neuroscience/nose \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Human_nose \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.sciencedirect.com/topics/neuroscience/nose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/books/NBK526086/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/books/NBK544232/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://my.clevelandclinic.org/health/body/21778-nose \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/books/NBK526086/\n",
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/books/NBK544232/\n",
      "-----Web scraping time: 2.44 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.35 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.82 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " I don't have enough information to answer that question.\n",
      "\n",
      "Reasoning step: The provided context describes the anatomy and functions of the human nose, but it does not mention the chemical composition of the nose. To answer this question, I would need information about the types and proportions of molecules that make up the nose, such as proteins, carbohydrates, lipids, and other biomolecules.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   5%|         | 5/100 [00:43<13:13,  8.35s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 568\n",
      "-----HyDE generation time: 0.69 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/worlds-first-computer-bug/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://lunduke.substack.com/p/the-story-of-the-first-computer-bug \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/worlds-first-computer-bug/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-was-the-first-computer-bug \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.globalapptesting.com/blog/the-worlds-first-computer-bug-global-app-testing \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://daily.jstor.org/the-bug-in-the-computer-bug-story/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 4.34 seconds\n",
      "Combined text length: 22286 characters\n",
      "Number of sentence windows: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.05 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 7.29 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.53 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year in which the first computer bug occurred. To answer this question, I need to find the relevant information in the provided context.\n",
      "\n",
      "Answer: According to the context, the first computer bug occurred on September 9, 1947.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   6%|         | 6/100 [00:53<13:48,  8.81s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 561\n",
      "-----HyDE generation time: 0.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.reddit.com/r/Stoicism/comments/10dr49a/core_beliefs_required_to_call_yourself_a_stoic/ \"HTTP/1.1 302 Found\"\n",
      "INFO:httpx:HTTP Request: GET https://plato.stanford.edu/ENTRIES/stoicism/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://dailystoic.com/9-core-stoic-beliefs/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Stoicism \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 302 while requesting https://www.reddit.com/r/Stoicism/comments/10dr49a/core_beliefs_required_to_call_yourself_a_stoic/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://iep.utm.edu/stoicism/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://becomingbetter.org/10-essential-principles-and-practices-of-stoicism/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.44 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.35 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.79 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the founder of the Stoic school, which is a specific philosophical school. To answer this question, I need to identify the individual who is credited with founding the Stoic school.\n",
      "\n",
      "Answer: According to the provided context, the Stoic school was founded around 300 BCE by Zeno of Citium.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   7%|         | 7/100 [01:01<13:26,  8.67s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 372\n",
      "-----HyDE generation time: 0.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://brainly.com/question/35555711 \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.esa.int/Applications/Observing_the_Earth/Earth_from_Space_World_s_largest_living_structure \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://brainly.com/question/35555711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://oceanservice.noaa.gov/facts/gbrlargeststructure.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/606952-largest-living-organism \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://oceanservice.noaa.gov/facts/gbrlargeststructure.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/606952-largest-living-organism \"HTTP/1.1 301 Moved Permanently\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/606952-largest-living-organism\n",
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/606952-largest-living-organism\n",
      "-----Web scraping time: 2.02 seconds\n",
      "Combined text length: 6356 characters\n",
      "Number of sentence windows: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.80 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 3.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.78 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the world's largest living structure according to the Guinness World Records, but the provided context does not mention the Guinness World Records. However, it does mention that the Great Barrier Reef is the largest living structure on Earth and the only one visible from space.\n",
      "\n",
      "Answer: I don't have enough information to answer that question.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   8%|         | 8/100 [01:08<12:34,  8.20s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 385\n",
      "-----HyDE generation time: 0.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.rmg.co.uk/stories/topics/solar-system-data \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.universetoday.com/84663/which-planet-has-the-longest-day/ \"HTTP/1.1 503 Service Temporarily Unavailable\"\n",
      "INFO:httpx:HTTP Request: GET https://www.reuters.com/lifestyle/science/how-long-is-solar-systems-longest-day-venus-has-answer-2021-05-03/ \"HTTP/1.1 401 HTTP Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 503 while requesting https://www.universetoday.com/84663/which-planet-has-the-longest-day/\n",
      "Error response 401 while requesting https://www.reuters.com/lifestyle/science/how-long-is-solar-systems-longest-day-venus-has-answer-2021-05-03/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://spaceplace.nasa.gov/days/en/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://spaceplace.nasa.gov/days/en/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.46 seconds\n",
      "Combined text length: 14250 characters\n",
      "Number of sentence windows: 139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.81 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.61 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: To determine which planet in our solar system has the longest day, we need to look at the table provided in the context, which lists the day length for each planet in hours.\n",
      "\n",
      "Answer: Mars has the longest day, with a day length of 25 hours.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:   9%|         | 9/100 [01:16<12:28,  8.22s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 481\n",
      "-----HyDE generation time: 0.75 seconds\n",
      "-----Embedding time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.poetryfoundation.org/poetrymagazine/poems/44212/the-love-song-of-j-alfred-prufrock \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://www.sparknotes.com/poetry/eliot/section1/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/The_Love_Song_of_J._Alfred_Prufrock \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.sparknotes.com/poetry/eliot/section1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.litcharts.com/poetry/t-s-eliot/the-love-song-of-j-alfred-prufrock \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.owleyes.org/text/love-song/analysis/historical-context \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.bu.edu/writingprogram/journal/past-issues/issue-11/immerwahr/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.71 seconds\n",
      "Combined text length: 23444 characters\n",
      "Number of sentence windows: 109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.63 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.47 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the poem \"The Love Song of J. Alfred Prufrock\", which is mentioned in the context as being written by T. S. Eliot.\n",
      "\n",
      "Answer: T. S. Eliot.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  10%|         | 10/100 [01:24<12:06,  8.08s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 423\n",
      "-----HyDE generation time: 0.49 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.twinkl.com/teaching-wiki/the-hottest-place-on-earth \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://guinnessworldrecords.com/world-records/highest-recorded-temperature \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://wmo.asu.edu/content/world-highest-temperature \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.autoeurope.com/travel-blog/the-worlds-coldest-and-warmest-places/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Highest_temperature_recorded_on_Earth \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 301 while requesting https://guinnessworldrecords.com/world-records/highest-recorded-temperature\n",
      "Error response 403 while requesting https://www.autoeurope.com/travel-blog/the-worlds-coldest-and-warmest-places/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.sciencefocus.com/planet-earth/hottest-place-on-earth \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.33 seconds\n",
      "Combined text length: 13333 characters\n",
      "Number of sentence windows: 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.26 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.91 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the highest recorded temperature on Earth, which requires identifying the highest temperature measurement among the three major ways of measurement: air, ground, and satellite observation.\n",
      "\n",
      "Answer: According to the provided context, the current official highest registered air temperature on Earth is 56.7C (134.1F), recorded on 10 July 1913 at Furnace Creek Ranch, in Death Valley in the United States. However, it is worth noting that this record is currently under scrutiny due to concerns about its legitimacy, and if it were to be decertified, the highest established recorded air temperature on Earth would be 54.0C (129.2F), also recorded in Death Valley on 20 June 2013, and in Mitribah, Kuwait on 21 July 2016.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  11%|         | 11/100 [01:32<11:54,  8.03s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 594\n",
      "-----HyDE generation time: 0.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-polio-vaccination \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-polio-vaccination \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Jonas_Salk \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/\n",
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/\n",
      "-----Web scraping time: 2.48 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.10 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.60 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " The reasoning step is to identify the key information in the context that relates to the question. In this case, the context mentions Jonas Salk and his work on developing a vaccine against polio.\n",
      "\n",
      "The answer is: Jonas Salk.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  12%|        | 12/100 [01:40<11:43,  7.99s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 579\n",
      "-----HyDE generation time: 0.69 seconds\n",
      "-----Embedding time: 0.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.usatoday.com/story/tech/science/2023/02/16/largest-star-universe-red-hypergiant/11075755002/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.usatoday.com/story/tech/science/2023/02/16/largest-star-universe-red-hypergiant/11075755002/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.space.com/41290-biggest-star.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-is-the-biggest-known-star-in-terms-of-size-and-mass \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://science.howstuffworks.com/largest-star-in-the-universe.htm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_largest_stars \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.91 seconds\n",
      "Combined text length: 22929 characters\n",
      "Number of sentence windows: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.00 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.41 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the name of the largest star known to science, which is mentioned in the context as UY Scuti.\n",
      "\n",
      "Answer: UY Scuti.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  13%|        | 13/100 [01:50<12:22,  8.53s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 435\n",
      "-----HyDE generation time: 0.57 seconds\n",
      "-----Embedding time: 0.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.quora.com/Do-you-know-who-was-the-first-person-to-use-email-and-who-was-the-first-person-to-receive-it \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_email \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_email \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.emailonacid.com/blog/article/email-marketing/history-of-email/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mail.com/blog/posts/fiftieth-anniversary-of-email/20/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mail.com/blog/posts/fiftieth-anniversary-of-email/20/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.59 seconds\n",
      "Combined text length: 25225 characters\n",
      "Number of sentence windows: 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.96 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.47 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year in which the first email was sent. The text mentions that Ray Tomlinson sent the first email in 1971, but it does not explicitly state the year in which the first email was sent. However, it is mentioned that Tomlinson sent the first email in 1971, and that he had a hard time remembering the exact date or content of the first email 25 years later.\n",
      "\n",
      "Answer: 1971\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  14%|        | 14/100 [01:57<11:50,  8.26s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 579\n",
      "-----HyDE generation time: 0.87 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.theatlantic.com/entertainment/archive/2017/05/one-hundred-years-of-solitude-50-years-later/527118/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/One_Hundred_Years_of_Solitude \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/One-Hundred-Years-of-Solitude \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://thenarrativearc.org/years-solitude \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://edubirdie.com/examples/magic-realism-in-one-hundred-years-of-solitude-by-gabriel-garcia-marquez/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://rupkatha.com/V2/n3/MagicRealisminMarquez.pdf \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.88 seconds\n",
      "Combined text length: 30005 characters\n",
      "Number of sentence windows: 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.59 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 7.55 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.56 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the famous novel \"One Hundred Years of Solitude\". To answer this question, I need to identify the person mentioned in the context as the writer of the novel.\n",
      "\n",
      "Answer: Gabriel Garca Mrquez.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  15%|        | 15/100 [02:08<12:47,  9.03s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 351\n",
      "-----HyDE generation time: 0.50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/ocean-trench/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Mariana_Trench \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/science/deep-sea-trench \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.whoi.edu/know-your-ocean/ocean-topics/how-the-ocean-works/seafloor-below/ocean-trenches/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://oceanservice.noaa.gov/facts/oceandepth.html \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.22 seconds\n",
      "Combined text length: 22212 characters\n",
      "Number of sentence windows: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.81 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.52 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " **Reasoning Step:** The question asks for the deepest part of the ocean, which is a specific location within the ocean. To answer this question, I need to identify the location mentioned in the context that is described as the deepest part of the ocean.\n",
      "\n",
      "**Answer:** The deepest part of the ocean is called the Challenger Deep, which is located beneath the western Pacific Ocean in the southern end of the Mariana Trench.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  16%|        | 16/100 [02:15<11:46,  8.41s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 444\n",
      "-----HyDE generation time: 0.64 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Smartphone \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.weforum.org/agenda/2018/03/remembering-first-smartphone-simon-ibm/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://simpletexting.com/blog/where-have-we-come-since-the-first-smartphone/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.textline.com/blog/smartphone-history \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://simpletexting.com/blog/where-have-we-come-since-the-first-smartphone/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.weforum.org/agenda/2018/03/remembering-first-smartphone-simon-ibm/\n",
      "Error response 403 while requesting https://simpletexting.com/blog/where-have-we-come-since-the-first-smartphone/\n",
      "Error response 403 while requesting https://simpletexting.com/blog/where-have-we-come-since-the-first-smartphone/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://blog.textedly.com/smartphone-history-when-were-smartphones-invented \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.56 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.83 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.56 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " I don't have enough information to answer that question.\n",
      "\n",
      "However, I can provide a reasoning step to answer the question \"When were smartphones invented?\"\n",
      "\n",
      "Reasoning step: The context mentions that the first smartphone was announced by IBM in 1992, and it was available for purchase in 1994.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  17%|        | 17/100 [02:23<11:16,  8.15s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 522\n",
      "-----HyDE generation time: 0.59 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.bairesdev.com/blog/best-programming-languages-web-development/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.simplilearn.com/best-programming-languages-start-learning-today-article \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.browserstack.com/guide/best-language-for-web-development \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.statista.com/statistics/793628/worldwide-developer-survey-most-used-languages/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.bairesdev.com/blog/best-programming-languages-web-development/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.coursera.org/articles/popular-programming-languages \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://eluminoustechnologies.com/blog/top-10-web-programming-languages/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.21 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.36 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.49 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the most widely used programming language in the world, which requires identifying the language with the highest usage rate among software developers worldwide.\n",
      "\n",
      "Answer: According to the text, JavaScript is the most popular language to learn, and it is also the most commonly used programming language among software developers around the world, with more than 62.3 percent of respondents stating that they used JavaScript.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  18%|        | 18/100 [02:32<11:26,  8.37s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 409\n",
      "-----HyDE generation time: 0.59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/Queen-British-rock-group \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Queen_(band) \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Queen_(band) \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.34 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.45 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.55 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the lead singer of the rock band Queen. To answer this question, we need to identify the person who is credited as the lead vocalist of the band. \n",
      "\n",
      "Answer: Freddie Mercury was the lead singer of the rock band Queen.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  19%|        | 19/100 [02:41<11:33,  8.56s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 327\n",
      "-----HyDE generation time: 0.44 seconds\n",
      "-----Embedding time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.physicalgold.com/insights/physical-properties-of-gold/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/science/gold-chemical-element \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.rsc.org/periodic-table/element/79/gold \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.open.edu/openlearn/science-maths-technology/science/chemistry/properties-gold \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://pubchem.ncbi.nlm.nih.gov/element/Gold: \n",
      "-----Web scraping time: 9.93 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.06 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 12.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.51 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the chemical symbol of gold, which is mentioned in the context as \"Au,\" derived from the Latin word \"aurum,\" meaning \"shining dawn.\"\n",
      "\n",
      "Answer: The chemical symbol for gold is \"Au.\"\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  20%|        | 20/100 [02:56<14:13, 10.66s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 508\n",
      "-----HyDE generation time: 0.74 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.nasa.gov/history/apollo-11-mission-overview/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.rmg.co.uk/stories/topics/apollo-11-crew \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/Which-year-did-people-go-to-the-moon-for-the-first-time \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://science.nasa.gov/moon/moon-walkers/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.rmg.co.uk/stories/topics/how-many-people-have-walked-on-moon \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Apollo_11 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.92 seconds\n",
      "Combined text length: 25188 characters\n",
      "Number of sentence windows: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.87 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.60 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year when the first human walked on the moon. The context provides information about the Apollo 11 mission, which includes the names of the astronauts, the spacecraft, and the dates of the mission. However, the specific year when the first human walked on the moon is not explicitly mentioned in the provided text.\n",
      "\n",
      "Answer: 1969.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  21%|        | 21/100 [03:04<13:04,  9.92s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 493\n",
      "-----HyDE generation time: 0.62 seconds\n",
      "-----Embedding time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://study.com/academy/lesson/social-and-historical-context-of-romeo-and-juliet.html \"HTTP/1.1 403 HTTP Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sparknotes.com/shakespeare/romeojuliet/context/historical/what-did-shakespeares-audience-know-about-italy/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://rainford.org.uk/wp-content/uploads/2018/10/Romeo-and-Juliet-Self-Testing-Example.pdf \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Romeo_and_Juliet \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://study.com/academy/lesson/social-and-historical-context-of-romeo-and-juliet.html\n",
      "Error response 403 while requesting https://www.sparknotes.com/shakespeare/romeojuliet/context/historical/what-did-shakespeares-audience-know-about-italy/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/Romeo-and-Juliet \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.60 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.54 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.98 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.52 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the famous play \"Romeo and Juliet\". To answer this question, I need to identify the person who wrote the play. The context provides information about the play, its plot, and its history, but it does not explicitly mention the author's name. However, it does mention that the play was written by William Shakespeare early in his career.\n",
      "\n",
      "Answer: William Shakespeare.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  22%|       | 22/100 [03:12<11:59,  9.22s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 492\n",
      "-----HyDE generation time: 0.79 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_waterfalls_by_flow_rate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_waterfalls_by_flow_rate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://chasingchanelle.com/biggest-waterfall-in-the-world/ \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://www.worldwaterfalldatabase.com/largest-waterfalls/volume \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.worldwaterfalldatabase.com/largest-waterfalls/volume \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.46 seconds\n",
      "Combined text length: 25004 characters\n",
      "Number of sentence windows: 143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.94 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.40 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.84 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: To determine the world's largest waterfall by volume of water, we need to look for the waterfall with the highest flow rate or discharge of water.\n",
      "\n",
      "Answer: According to the list of waterfalls by flow rate on Wikipedia, the world's largest waterfall by volume of water is Inga Falls, with a flow rate of 25,768 cubic meters per second.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  23%|       | 23/100 [03:21<11:49,  9.21s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 521\n",
      "-----HyDE generation time: 0.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_the_web_browser \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mozilla.org/en-US/firefox/browsers/browser-history/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mozilla.org/en-US/firefox/browsers/browser-history/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.meetsidekick.com/who-created-the-first-web-browser/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.meetsidekick.com/who-created-the-first-web-browser/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/History_of_the_web_browser \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.21 seconds\n",
      "Combined text length: 26583 characters\n",
      "Number of sentence windows: 199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.35 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.46 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " The founder of the company that developed the first web browser is Marc Andreessen. He founded the company that released Netscape Navigator, which was one of the first popular web browsers, in 1994.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  24%|       | 24/100 [03:31<12:03,  9.52s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 522\n",
      "-----HyDE generation time: 0.51 seconds\n",
      "-----Embedding time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.reddit.com/r/spaceporn/comments/xy1d00/the_tallest_mountain_in_the_solar_system_olympus/ \"HTTP/1.1 302 Found\"\n",
      "INFO:httpx:HTTP Request: GET https://science.howstuffworks.com/tallest-mountain-in-solar-system.htm \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.jpl.nasa.gov/edu/learn/video/mars-in-a-minute-how-did-mars-get-such-enormous-mountains/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_tallest_mountains_in_the_Solar_System \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_tallest_mountains_in_the_Solar_System \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 302 while requesting https://www.reddit.com/r/spaceporn/comments/xy1d00/the_tallest_mountain_in_the_solar_system_olympus/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://coolcosmos.ipac.caltech.edu/ask/199-Where-is-the-highest-mountain-in-our-Solar-System- \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.43 seconds\n",
      "Combined text length: 19607 characters\n",
      "Number of sentence windows: 117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.55 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.67 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.43 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the highest mountain peak in the solar system, and the provided context mentions Olympus Mons on Mars as the tallest mountain and volcano in the solar system.\n",
      "\n",
      "Answer: Olympus Mons on Mars.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  25%|       | 25/100 [03:39<11:06,  8.89s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 333\n",
      "-----HyDE generation time: 0.41 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.rollingstone.com/feature/guns-n-roses-excerpt-nothin-but-a-good-time-book-1130385/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://rockhall.com/inductees/guns-n-roses/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/Guns-N-Roses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Guns_N%27_Roses \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Guns_N%27_Roses \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://rockhall.com/inductees/guns-n-roses/\n",
      "-----Web scraping time: 3.72 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.75 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 6.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.47 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the lead singer of the rock band Guns N' Roses. To answer this question, I need to identify the vocalist mentioned in the context as a member of the band.\n",
      "\n",
      "Answer: According to the context, the lead singer of the rock band Guns N' Roses is Axl Rose.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  26%|       | 26/100 [03:47<10:53,  8.83s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 506\n",
      "-----HyDE generation time: 0.49 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://propio-ls.com/2021/11/11/top-languages-usa/ \"HTTP/1.1 301 Moved Permanently\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Languages_of_the_United_States \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 301 while requesting https://propio-ls.com/2021/11/11/top-languages-usa/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.census.gov/library/stories/2022/12/languages-we-speak-in-united-states.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.census.gov/library/stories/2022/12/languages-we-speak-in-united-states.html \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.53 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.20 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.43 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.81 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the most widely spoken language in the United States. To answer this question, we need to look for information in the provided context that mentions the most common language spoken in the country.\n",
      "\n",
      "Answer: According to the text \"Languages of the United States - Wikipedia\", the most widely spoken language in the United States is English, with 78.0% of the population speaking it.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  27%|       | 27/100 [03:56<10:29,  8.63s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 540\n",
      "-----HyDE generation time: 0.58 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://study.com/academy/lesson/to-kill-a-mockingbird-themes-symbols-imagery.html \"HTTP/1.1 403 HTTP Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.nytimes.com/2016/02/20/arts/harper-lee-dies.html \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.sparknotes.com/lit/mocking/themes/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Harper_Lee \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.britannica.com/topic/To-Kill-a-Mockingbird \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://study.com/academy/lesson/to-kill-a-mockingbird-themes-symbols-imagery.html\n",
      "Error response 403 while requesting https://www.nytimes.com/2016/02/20/arts/harper-lee-dies.html\n",
      "Error response 403 while requesting https://www.sparknotes.com/lit/mocking/themes/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.yourdictionary.com/articles/tkm-symbols \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.57 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.20 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.62 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.48 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the famous novel \"To Kill a Mockingbird\". To answer this question, I need to identify the person mentioned in the context as the author of the novel.\n",
      "\n",
      "Answer: Nelle Harper Lee.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  28%|       | 28/100 [04:03<09:50,  8.21s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 528\n",
      "-----HyDE generation time: 0.61 seconds\n",
      "-----Embedding time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.20 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.sciencedirect.com/science/article/pii/S0891061817300601 \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Human_brain \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.medicalnewstoday.com/articles/326649 \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/books/NBK539894/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.sciencedirect.com/science/article/pii/S0891061817300601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://my.clevelandclinic.org/health/articles/22513-neurotransmitters \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/books/NBK539894/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.hopkinsmedicine.org/health/conditions-and-diseases/anatomy-of-the-brain \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.39 seconds\n",
      "Combined text length: 20003 characters\n",
      "Number of sentence windows: 175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 2.16 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.55 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the chemical composition of the human brain, which is mentioned in the context as a combination of water, protein, carbohydrates, and salts.\n",
      "\n",
      "Answer: The brain is made up of a combination of water, protein, carbohydrates, and salts, with the majority being water and the remaining 40% consisting of these other components.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating none reranker:  29%|       | 29/100 [04:10<09:27,  7.99s/it]INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 513\n",
      "-----HyDE generation time: 0.55 seconds\n",
      "-----Embedding time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Query expansion time: 0.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4200566/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6062759/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://optn.transplant.hrsa.gov/news/first-heart-transplant-performed-35-years-ago/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4200566/\n",
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6062759/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.bhf.org.uk/informationsupport/heart-matters-magazine/medical/history-of-uk-heart-transplant \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.bhf.org.uk/informationsupport/heart-matters-magazine/medical/history-of-uk-heart-transplant \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 3.07 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.03 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.60 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year of the first successful heart transplant. To answer this question, I need to identify the year mentioned in the text as the year of the first successful heart transplant.\n",
      "\n",
      "Answer: According to the text, the first successful heart transplant occurred in 1967, when Christiaan Barnard transplanted a heart from a 25-year-old woman into Lewis Washkansky, a 53-year-old South African grocer.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "async def run_evaluation(num_questions: int = 100):\n",
    "    questions = evaluation_questions\n",
    "    rerankers = [\"none\", \"llm\", \"nano\", \"small\", \"medium_t5\", \"medium_multilang\"]\n",
    "    \n",
    "    for reranker in rerankers:\n",
    "        results = []\n",
    "        total_answer_correct = 0\n",
    "        total_docs_correct = 0\n",
    "        \n",
    "        # Create a progress bar for each reranker\n",
    "        progress_bar = tqdm(total=num_questions, desc=f\"Evaluating {reranker} reranker\")\n",
    "        \n",
    "        for question in questions:\n",
    "            answer, context_docs = await process_query(question, num_expansions=1, num_urls=3, num_docs=80, num_rerank=10, rerank_method=reranker, use_70b_model=False)\n",
    "            \n",
    "            all_docs = context_docs[:80]  # All 80 retrieved documents\n",
    "            selected_docs = context_docs[:10]  # Top 10 after reranking\n",
    "            \n",
    "            answer_correct = evaluate_answer_quality(question, answer)\n",
    "            docs_correct = evaluate_document_selection(question, all_docs, selected_docs)\n",
    "            \n",
    "            total_answer_correct += answer_correct\n",
    "            total_docs_correct += docs_correct\n",
    "            \n",
    "            result = {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"answer_correctness\": answer_correct,\n",
    "                \"top_10_docs_correctness\": docs_correct,\n",
    "                \"all_docs\": all_docs,\n",
    "                \"selected_docs\": selected_docs\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Update the progress bar\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # Close the progress bar\n",
    "        progress_bar.close()\n",
    "        \n",
    "        avg_answer_correct = total_answer_correct / num_questions\n",
    "        avg_docs_correct = total_docs_correct / num_questions\n",
    "        print(f\"\\nAverage Results for {reranker} reranker over {num_questions} questions:\")\n",
    "        print(f\"Average Answer Correctness: {avg_answer_correct:.2f}\")\n",
    "        print(f\"Average Top 10 Documents Correctness: {avg_docs_correct:.2f}\")\n",
    "        \n",
    "        # Save results to a JSON file\n",
    "        output = {\n",
    "            \"results\": results,\n",
    "            \"average_answer_correctness\": avg_answer_correct,\n",
    "            \"average_top_10_docs_correctness\": avg_docs_correct\n",
    "        }\n",
    "        \n",
    "        filename = f'evaluation_{reranker}.json'\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults have been saved to '{filename}'\")\n",
    "\n",
    "# To run the evaluation, use:\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
