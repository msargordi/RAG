{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "930968b4-02cf-4f40-8360-0e07c67c0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_fireworks import FireworksEmbeddings, ChatFireworks\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import faiss\n",
    "import httpx\n",
    "from urllib.parse import urlparse\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import random\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae80fd5e-0346-4d85-8373-12da58b452f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU will be used automatically by FlashRank.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /home/ubuntu/.flashrank_cache/rank_zephyr_7b_v1_full/rank_zephyr_7b_v1_full.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = hub\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   128.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    98.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'hub', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set up API clients\n",
    "# os.environ['FIREWORKS_API_KEY'] = 'API'\n",
    "# os.environ[\"SERPER_API_KEY\"] = 'API' # my api\n",
    "os.environ[\"SERPER_API_KEY\"] = 'API'\n",
    "os.environ[\"FIREWORKS_API_KEY\"] = 'API'\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'API'\n",
    "\n",
    "# Initialize components\n",
    "search = GoogleSerperAPIWrapper(k=3)\n",
    "embeddings = FireworksEmbeddings(model=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "# llm = OpenAI()\n",
    "llm_8b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_70b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "# Create a directory for caching in the user's home folder\n",
    "cache_dir = Path.home() / \".flashrank_cache\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU will be used automatically by FlashRank.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. CPU will be used.\")\n",
    "\n",
    "# Initialize FlashRank rerankers\n",
    "ranker_nano = Ranker(cache_dir=str(cache_dir))\n",
    "ranker_small = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=str(cache_dir))\n",
    "ranker_medium_t5 = Ranker(model_name=\"rank-T5-flan\", cache_dir=str(cache_dir))\n",
    "ranker_medium_multilang = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=str(cache_dir))\n",
    "ranker_large = Ranker(model_name=\"rank_zephyr_7b_v1_full\", max_length=1024, cache_dir=str(cache_dir))\n",
    "\n",
    "# Ensure models are on GPU if available\n",
    "for ranker in [ranker_nano, ranker_small, ranker_medium_t5, ranker_medium_multilang, ranker_large]:\n",
    "    if hasattr(ranker, 'model') and hasattr(ranker.model, 'to'):\n",
    "        ranker.model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Download NLTK data\n",
    "# nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d416271b-08ea-4f8f-b77e-6240688038b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    def __init__(self, documents: List[Document]):\n",
    "        self.documents = documents\n",
    "        self.tokenized_corpus = [doc.page_content.split() for doc in documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "        \n",
    "    def retrieve(self, query: str, extended_queries: List[str] = None, k: int = 100) -> List[Tuple[Document, float]]:\n",
    "        if extended_queries:\n",
    "            full_query = f\"{query} {' '.join(extended_queries)}\"\n",
    "        else:\n",
    "            full_query = query\n",
    "        \n",
    "        tokenized_query = full_query.split()\n",
    "        doc_scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Normalize BM25 scores to 0-1 range\n",
    "        max_score = max(doc_scores)\n",
    "        min_score = min(doc_scores)\n",
    "        score_range = max_score - min_score\n",
    "        normalized_scores = [(score - min_score) / score_range for score in doc_scores] if score_range != 0 else [1.0 for _ in doc_scores]\n",
    "        \n",
    "        top_k_indices = sorted(range(len(normalized_scores)), key=lambda i: normalized_scores[i], reverse=True)[:k]\n",
    "        return [(self.documents[i], normalized_scores[i]) for i in top_k_indices]\n",
    "\n",
    "def combine_retrieval_methods(query: str, vectorstores: List[FAISS], bm25_retriever: BM25Retriever, \n",
    "                              hyde_embedding: List[float], num_docs: int, extended_queries: List[str] = None, \n",
    "                              alpha: float = 0.7) -> List[Document]:\n",
    "    # Retrieve documents using vector search\n",
    "    vector_docs = []\n",
    "    for vectorstore in vectorstores:\n",
    "        docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs)\n",
    "        vector_docs.extend(docs)\n",
    "    \n",
    "    # Retrieve documents using BM25\n",
    "    bm25_docs = bm25_retriever.retrieve(query, extended_queries, k=num_docs)\n",
    "    \n",
    "    # Combine the results\n",
    "    combined_docs = {}\n",
    "    for doc in vector_docs:\n",
    "        combined_docs[doc.page_content] = alpha * (1 - vector_docs.index(doc) / len(vector_docs))\n",
    "    \n",
    "    for doc, score in bm25_docs:\n",
    "        if doc.page_content in combined_docs:\n",
    "            combined_docs[doc.page_content] += (1 - alpha) * score\n",
    "        else:\n",
    "            combined_docs[doc.page_content] = (1 - alpha) * score\n",
    "    \n",
    "    # Sort the combined results\n",
    "    sorted_docs = sorted(combined_docs.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [Document(page_content=content) for content, _ in sorted_docs[:num_docs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "558e5c70-468c-45db-b830-489420a3a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def generate_iteratively(context, query, target_tokens, chosen_llm, max_attempts=5):\n",
    "    current_response = \"\"\n",
    "    attempts = 0\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Use the following context to answer the question. Follow these steps:\n",
    "        \n",
    "        1. Generate a brief reasoning step.\n",
    "        2. Provide a structured answer using bullet points or numbered lists and separate the contents.\n",
    "        3. Aim for a comprehensive answer of approximately {target_tokens} words.\n",
    "        4. If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Reasoning:\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    \n",
    "    while count_tokens(current_response) < target_tokens * 0.9 and attempts < max_attempts:\n",
    "        remaining_tokens = target_tokens - count_tokens(current_response)\n",
    "        \n",
    "        response = chosen_llm.invoke(conversation_history)\n",
    "        \n",
    "        next_chunk = response.content if hasattr(response, 'content') else str(response)\n",
    "        \n",
    "        current_response += next_chunk\n",
    "        attempts += 1\n",
    "        \n",
    "        # Add the model's response to the conversation history\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": next_chunk})\n",
    "        \n",
    "        # Add a new user message asking to continue if needed\n",
    "        if count_tokens(current_response) < target_tokens * 0.9:\n",
    "            conversation_history.append({\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Please continue your answer. Aim for approximately {remaining_tokens} additional words.\"\n",
    "            })\n",
    "        \n",
    "        if count_tokens(current_response) > target_tokens * 1.1:\n",
    "            # If we've exceeded the upper bound, truncate\n",
    "            words = current_response.split()\n",
    "            truncated_response = ' '.join(words[:target_tokens])\n",
    "            return truncated_response\n",
    "    \n",
    "    return current_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec19805-7363-4f1d-b12f-13fd280b33bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7864/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7864/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v2/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://502650039c7567a1cf.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://502650039c7567a1cf.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://502650039c7567a1cf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://machinelearningmastery.com/how-to-code-the-generative-adversarial-network-training-algorithm-and-loss-functions/ \"HTTP/1.1 403 Forbidden\"\n",
      "INFO:httpx:HTTP Request: GET https://www.mathworks.com/help/deeplearning/ug/train-conditional-generative-adversarial-network.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://neptune.ai/blog/gan-loss-functions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "async def scrape_webpage(client, url):\n",
    "    try:\n",
    "        response = await client.get(url, timeout=3.0)\n",
    "        response.raise_for_status()\n",
    "        text = response.text\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        content = ' '.join(soup.stripped_strings)\n",
    "        return content[:5000], len(content[:5000])\n",
    "    except (httpx.RequestError, httpx.TimeoutException) as exc:\n",
    "        print(f\"An error occurred while requesting {url}: {exc}\")\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(f\"Error response {exc.response.status_code} while requesting {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return \"\", 0\n",
    "\n",
    "async def search_and_scrape(query, num_urls):\n",
    "    search_results = search.results(query)\n",
    "    scraped_urls = set()\n",
    "    full_texts = []\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=httpx.Timeout(10.0, connect=3.0)) as client:\n",
    "        tasks = []\n",
    "        if 'organic' in search_results:\n",
    "            for result in search_results['organic']:\n",
    "                url = result.get('link')\n",
    "                domain = urlparse(url).netloc if url else None\n",
    "                if url and domain not in scraped_urls and len(tasks) < num_urls:\n",
    "                    tasks.append(scrape_webpage(client, url))\n",
    "                    scraped_urls.add(domain)\n",
    "\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        for result in results:\n",
    "            if isinstance(result, tuple) and result[1] > 0:\n",
    "                full_texts.append(result[0])\n",
    "\n",
    "    return \" \".join(full_texts)\n",
    "\n",
    "def query_expansion(query, num_expansions):\n",
    "    expansion_prompt = f\"\"\"\n",
    "    Given the following search query, generate {num_expansions} additional related queries that could help find more comprehensive information on the topic. The queries should be different from each other and explore various aspects of the main query. Provide only the additional queries, numbered 1-{num_expansions}.\n",
    "\n",
    "    Main query: {query}\n",
    "\n",
    "    Additional queries:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(expansion_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    expanded_queries = [query]\n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.strip() and line[0].isdigit():\n",
    "            expanded_queries.append(line.split('. ', 1)[1].strip())\n",
    "\n",
    "    return expanded_queries[:num_expansions + 1]\n",
    "\n",
    "def create_sentence_windows(text, window_size=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    windows = []\n",
    "    for i in range(len(sentences)):\n",
    "        window = \" \".join(sentences[max(0, i-window_size):min(len(sentences), i+window_size+1)])\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def generate_hypothetical_document(query):\n",
    "    hyde_prompt = f\"\"\"\n",
    "    Given the search query below, generate a hypothetical document that would be a perfect match for this query. The document should be concise, containing only 3 sentences of relevant information that directly addresses the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Document (3 sentences):\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def llm_rerank(query, documents):\n",
    "    rerank_prompt = \"\"\"\n",
    "    Given the following query and a list of document excerpts, rank the documents based on their relevance to the query. Provide the rankings as a list of numbers from 1 to {}, where 1 is the most relevant. Ensure you provide a ranking for every document.\n",
    "\n",
    "    Query: {}\n",
    "\n",
    "    Documents:\n",
    "    {}\n",
    "\n",
    "    Rankings (1 to {}):\n",
    "    \"\"\".format(len(documents), query, \"\\n\".join([f\"{i+1}. {doc.page_content[:200]}...\" for i, doc in enumerate(documents)]), len(documents))\n",
    "\n",
    "    response = llm.invoke(rerank_prompt)\n",
    "    rankings = [int(x) for x in response.content.split() if x.isdigit()]\n",
    "\n",
    "    if len(rankings) < len(documents):\n",
    "        remaining = set(range(1, len(documents) + 1)) - set(rankings)\n",
    "        rankings.extend(remaining)\n",
    "\n",
    "    sorted_docs = sorted(zip(documents, rankings), key=lambda x: x[1])\n",
    "    return sorted_docs\n",
    "\n",
    "def flashrank_rerank(query, documents, ranker):\n",
    "    rerank_request = RerankRequest(\n",
    "        query=query,\n",
    "        passages=[{\"text\": doc.page_content} for doc in documents]\n",
    "    )\n",
    "    reranked = ranker.rerank(rerank_request)\n",
    "    \n",
    "    if isinstance(reranked, list) and isinstance(reranked[0], dict):\n",
    "        sorted_results = sorted(reranked, key=lambda x: x.get('score', 0), reverse=True)\n",
    "        return [(documents[i], result.get('score', 0)) for i, result in enumerate(sorted_results)]\n",
    "    \n",
    "    elif isinstance(reranked, list) and hasattr(reranked[0], 'score'):\n",
    "        sorted_results = sorted(reranked, key=lambda x: x.score, reverse=True)\n",
    "        return [(documents[i], result.score) for i, result in enumerate(sorted_results)]\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unexpected reranked result type. Using original document order.\")\n",
    "        return [(doc, 1.0) for doc in documents]\n",
    "\n",
    "\n",
    "def batch_embed_documents(documents, embeddings, batch_size=512):\n",
    "    batched_embeddings = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        texts = [doc.page_content for doc in batch]\n",
    "        embeddings_batch = embeddings.embed_documents(texts)\n",
    "        batched_embeddings.extend(embeddings_batch)\n",
    "    return batched_embeddings\n",
    "\n",
    "def create_single_vectorstore(index_documents, embeddings):\n",
    "    vectorstore_start = time.time()\n",
    "    \n",
    "    all_documents = []\n",
    "    for doc in index_documents:\n",
    "        all_documents.append(Document(page_content=doc.page_content))\n",
    "    \n",
    "    # Batch process all embeddings\n",
    "    batch_embeddings = batch_embed_documents(all_documents, embeddings)\n",
    "    \n",
    "    # Create a single FAISS vectorstore\n",
    "    texts = [doc.page_content for doc in all_documents]\n",
    "    vectorstore = FAISS.from_embeddings(\n",
    "        embedding=embeddings,\n",
    "        text_embeddings=list(zip(texts, batch_embeddings))\n",
    "    )\n",
    "    \n",
    "    vectorstore_time = time.time() - vectorstore_start\n",
    "    print(f\"-----Vectorstore creation time: {vectorstore_time:.2f} seconds\")\n",
    "    \n",
    "    return vectorstore, all_documents\n",
    "\n",
    "\n",
    "# Update the retriever function to use the single vectorstore:\n",
    "def get_hyde_retriever(vectorstore, hyde_embedding, num_docs, num_rerank, rerank_method):\n",
    "    def retriever(query):\n",
    "        docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs)\n",
    "        \n",
    "        unique_docs = []\n",
    "        seen_content = set()\n",
    "        for doc in docs:\n",
    "            content = doc.page_content\n",
    "            if content not in seen_content:\n",
    "                unique_docs.append(Document(page_content=content))\n",
    "                seen_content.add(content)\n",
    "\n",
    "        try:\n",
    "            if rerank_method == \"none\":\n",
    "                return unique_docs[:num_rerank]\n",
    "            elif rerank_method == \"llm\":\n",
    "                reranked_docs = llm_rerank(query, unique_docs)\n",
    "            elif rerank_method in [\"nano\", \"small\", \"medium_t5\", \"medium_multilang\", \"large\"]:\n",
    "                ranker = globals()[f\"ranker_{rerank_method}\"]\n",
    "                reranked_docs = flashrank_rerank(query, unique_docs, ranker)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown rerank method: {rerank_method}\")\n",
    "\n",
    "            return [doc for doc, _ in reranked_docs[:num_rerank]]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reranking with method {rerank_method}: {str(e)}\")\n",
    "            print(\"Traceback:\", traceback.format_exc())\n",
    "            print(\"Falling back to no reranking.\")\n",
    "            return unique_docs[:num_rerank]\n",
    "\n",
    "    return retriever\n",
    "\n",
    "async def process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model, use_combined_retrieval, use_extended_queries, target_tokens):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        hyde_start = time.time()\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        hyde_time = time.time() - hyde_start\n",
    "        print(f\"hypothetical_doc length: {len(hypothetical_doc)}\")\n",
    "        print(f\"-----HyDE generation time: {hyde_time:.2f} seconds\")\n",
    "\n",
    "        embed_start = time.time()\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "        embed_time = time.time() - embed_start\n",
    "        print(f\"-----Embedding time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        ext_start = time.time()\n",
    "        extended_queries = query_expansion(query, num_expansions) if use_extended_queries else [query]\n",
    "        ext_time = time.time() - ext_start\n",
    "        print(f\"-----Query expansion time: {ext_time:.2f} seconds\")\n",
    "\n",
    "        scrape_start = time.time()\n",
    "        all_texts = await asyncio.gather(*[search_and_scrape(eq, num_urls) for eq in extended_queries])\n",
    "        scrape_time = time.time() - scrape_start\n",
    "        print(f\"-----Web scraping time: {scrape_time:.2f} seconds\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        sentence_windows = create_sentence_windows(combined_text)\n",
    "        print(f\"Number of sentence windows: {len(sentence_windows)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=window) for window in sentence_windows]\n",
    "\n",
    "        vectorstore_start = time.time()\n",
    "        vectorstore, all_documents = create_single_vectorstore(index_documents, embeddings)\n",
    "        \n",
    "        bm25_retriever = None\n",
    "        if use_combined_retrieval:\n",
    "            bm25_retriever = BM25Retriever(all_documents)\n",
    "        \n",
    "        vectorstore_time = time.time() - vectorstore_start\n",
    "        print(f\"-----Vectorstore {'and BM25 ' if use_combined_retrieval else ''}creation time: {vectorstore_time:.2f} seconds\")\n",
    "    \n",
    "        retrieval_start = time.time()\n",
    "        if use_combined_retrieval:\n",
    "            retrieved_docs = combine_retrieval_methods(query, [vectorstore], bm25_retriever, hyde_embedding, num_docs, extended_queries if use_extended_queries else None)\n",
    "        else:\n",
    "            retriever = get_hyde_retriever(vectorstore, hyde_embedding, num_docs, num_rerank, rerank_method)\n",
    "            retrieved_docs = retriever(query)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        print(f\"-----Retrieval and reranking time: {retrieval_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"Number of retrieved and reranked documents: {len(retrieved_docs)}\")\n",
    "\n",
    "        context_docs = [doc.page_content for doc in retrieved_docs]\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "\n",
    "        total_processing_time = hyde_time + embed_time + scrape_time + vectorstore_time + retrieval_time\n",
    "        print(f\"-----Total processing time before answer generation: {total_processing_time:.2f} seconds\")\n",
    "\n",
    "        answer_start = time.time()\n",
    "        \n",
    "        chosen_llm = llm_70b if use_70b_model else llm_8b\n",
    "        \n",
    "        # Use the generate_iteratively function\n",
    "        answer = generate_iteratively(context, query, target_tokens, chosen_llm)\n",
    "        \n",
    "        answer_time = time.time() - answer_start\n",
    "        print(f\"-----Answer generation time: {answer_time:.2f} seconds\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*120)\n",
    "        print(f\"Final Answer (approximately {count_tokens(answer)} words):\\n\", answer)\n",
    "        print(\"-\"*120)\n",
    "\n",
    "        return answer, context_docs, [hyde_time, hyde_embedding, ext_time, scrape_time, vectorstore_time, retrieval_time, total_processing_time, answer_time]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\", [], []\n",
    "\n",
    "\n",
    "def gradio_interface(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model, use_combined_retrieval, use_extended_queries, target_tokens):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "    answer, context_docs, _ = asyncio.run(process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model, use_combined_retrieval, use_extended_queries, target_tokens))\n",
    "\n",
    "    sys.stdout = old_stdout\n",
    "    captured_output = buffer.getvalue()\n",
    "\n",
    "    truncated_docs = [f\"Document {i+1}: {doc[:150]}...\" for i, doc in enumerate(context_docs)]\n",
    "    truncated_context = \"\\n\\n\".join(truncated_docs)\n",
    "\n",
    "    captured_output += f\"\\n\\nContext used for answer generation (first 150 characters of each document, {len(context_docs)} documents in total):\\n\" + truncated_context\n",
    "\n",
    "    return captured_output\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your query\"),\n",
    "        gr.Slider(minimum=0, maximum=3, value=1, step=1, label=\"Number of query expansions\"),\n",
    "        gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of URLs to scrape per extended query\"),\n",
    "        gr.Slider(minimum=20, maximum=150, value=80, step=1, label=\"Number of documents to retrieve with HyDE\"),\n",
    "        gr.Slider(minimum=10, maximum=120, value=50, step=1, label=\"Number of documents to keep after retrieval/reranking\"),\n",
    "        gr.Radio([\"none\", \"llm\", \"nano\", \"small\", \"medium_t5\", \"medium_multilang\"], label=\"Reranking method\", value=\"none\"),\n",
    "        gr.Checkbox(label=\"Use 70B model for QA (unchecked uses 8B)\", value=True),\n",
    "        gr.Checkbox(label=\"Use combined BM25 and embedding retrieval\", value=False),\n",
    "        gr.Checkbox(label=\"Use extended queries for BM25\", value=False),\n",
    "        gr.Slider(minimum=10, maximum=2000, value=1000, step=10, label=\"Target answer tokens\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Structured - Advanced RAG Query Processing\",\n",
    "    description=\"Enter a query and adjust parameters to get a detailed answer based on web search and document analysis.\",\n",
    "    examples=[\n",
    "        [\"How can I take care of my eyes?\", 3, 10, 150, 120, \"none\", True, False, False, 1000],\n",
    "        [\"How can I take care of my eyes?\", 1, 3, 80, 50, \"nano\", False, True, True, 500]\n",
    "    ]\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918a8247-ee12-4ca6-a588-20978b2ac68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 100 questions:\n",
      "1. What is the average lifespan of a quokka?\n",
      "2. Which ancient civilization built the first known suspension bridge?\n",
      "3. What is the name of the largest living organism in the world?\n",
      "4. Who is the author of the first computer bug?\n",
      "5. What is the chemical composition of the pigment Tyrian purple?\n",
      "6. In what year did the first successful heart transplant take place?\n",
      "7. What is the name of the largest waterfall in the world by volume?\n",
      "8. Who is the founder of the philosophical school of Stoicism?\n",
      "9. What is the name of the smallest country in the world by land area?\n",
      "10. What is the average airspeed velocity of an unladen swallow?\n",
      "11. Which planet in our solar system has the longest day?\n",
      "12. Who is the inventor of the first practical light bulb?\n",
      "13. What is the name of the largest desert in Asia?\n",
      "14. What is the chemical symbol for gold?\n",
      "15. In what year did the first human walk on the moon?\n",
      "16. What is the name of the largest living species of lizard?\n",
      "17. Who is the author of the famous painting \"The Ambassadors\"?\n",
      "18. What is the name of the largest island in the Mediterranean Sea?\n",
      "19. What is the average lifespan of a giant tortoise?\n",
      "20. Who is the founder of the first university in the world?\n",
      "21. What is the name of the largest mountain range in South America?\n",
      "22. What is the chemical composition of the pigment ultramarine?\n",
      "23. In what year did the first successful kidney transplant take place?\n",
      "24. What is the name of the largest lake in Africa?\n",
      "25. Who is the inventor of the first practical telephone?\n",
      "26. What is the name of the largest city in Scandinavia?\n",
      "27. What is the average airspeed velocity of a Boeing 747?\n",
      "28. Which ancient civilization built the first known astronomical observatory?\n",
      "29. What is the name of the largest waterfall in South America?\n",
      "30. Who is the author of the famous novel \"One Hundred Years of Solitude\"?\n",
      "31. What is the name of the largest island in the Caribbean Sea?\n",
      "32. What is the chemical symbol for silver?\n",
      "33. In what year did the first human walk on Mars?\n",
      "34. What is the name of the largest living species of snake?\n",
      "35. Who is the founder of the philosophical school of Epicureanism?\n",
      "36. What is the name of the largest desert in North America?\n",
      "37. What is the average lifespan of a blue whale?\n",
      "38. Who is the inventor of the first practical steam engine?\n",
      "39. What is the name of the largest mountain range in North America?\n",
      "40. What is the chemical composition of the pigment vermilion?\n",
      "41. In what year did the first successful liver transplant take place?\n",
      "42. What is the name of the largest lake in Asia?\n",
      "43. Who is the author of the famous play \"Hamlet\"?\n",
      "44. What is the name of the largest city in South America?\n",
      "45. What is the average airspeed velocity of a Concorde?\n",
      "46. Which ancient civilization built the first known canal?\n",
      "47. What is the name of the largest waterfall in Asia?\n",
      "48. Who is the inventor of the first practical refrigerator?\n",
      "49. What is the name of the largest island in the Indian Ocean?\n",
      "50. What is the chemical symbol for copper?\n",
      "51. In what year did the first human walk on the sun?\n",
      "52. What is the name of the largest living species of shark?\n",
      "53. Who is the founder of the philosophical school of Skepticism?\n",
      "54. What is the name of the largest desert in Australia?\n",
      "55. What is the average lifespan of a redwood tree?\n",
      "56. Who is the inventor of the first practical microscope?\n",
      "57. What is the name of the largest mountain range in Europe?\n",
      "58. What is the chemical composition of the pigment carmine?\n",
      "59. In what year did the first successful bone marrow transplant take place?\n",
      "60. What is the name of the largest lake in Europe?\n",
      "61. Who is the author of the famous novel \"To Kill a Mockingbird\"?\n",
      "62. What is the name of the largest city in North America?\n",
      "63. What is the average airspeed velocity of a F-16 fighter jet?\n",
      "64. Which ancient civilization built the first known clock tower?\n",
      "65. What is the name of the largest waterfall in Europe?\n",
      "66. Who is the inventor of the first practical laser?\n",
      "67. What is the name of the largest island in the Pacific Ocean?\n",
      "68. What is the chemical symbol for tin?\n",
      "69. In what year did the first human walk on the moon's south pole?\n",
      "70. What is the name of the largest living species of bear?\n",
      "71. Who is the founder of the philosophical school of Existentialism?\n",
      "72. What is the name of the largest desert in Africa?\n",
      "73. What is the average lifespan of a Komodo dragon?\n",
      "74. Who is the inventor of the first practical television?\n",
      "75. What is the name of the largest mountain range in Asia?\n",
      "76. What is the chemical composition of the pigment sienna?\n",
      "77. In what year did the first successful lung transplant take place?\n",
      "78. What is the name of the largest lake in North America?\n",
      "79. Who is the author of the famous play \"Romeo and Juliet\"?\n",
      "80. What is the name of the largest city in Europe?\n",
      "81. What is the average airspeed velocity of a Space Shuttle?\n",
      "82. Which ancient civilization built the first known aqueduct?\n",
      "83. What is the name of the largest waterfall in Africa?\n",
      "84. Who is the inventor of the first practical computer?\n",
      "85. What is the name of the largest island in the Atlantic Ocean?\n",
      "86. What is the chemical symbol for lead?\n",
      "87. In what year did the first human walk on the moon's north pole?\n",
      "88. What is the name of the largest living species of crocodile?\n",
      "89. Who is the founder of the philosophical school of Pragmatism?\n",
      "90. What is the name of the largest desert in South America?\n",
      "91. What is the average lifespan of a giant squid?\n",
      "92. Who is the inventor of the first practical robot?\n",
      "93. What is the name of the largest mountain range in Africa?\n",
      "94. What is the chemical composition of the pigment ochre?\n",
      "95. In what year did the first successful heart-lung transplant take place?\n",
      "96. What is the name of the largest lake in South America?\n",
      "97. Who is the author of the famous novel \"The Great Gatsby\"?\n",
      "98. What is the name of the largest city in Asia?\n",
      "99. What is the average airspeed velocity of a Black Hawk helicopter?\n",
      "100. Which ancient civilization built the first known lighthouse?\n"
     ]
    }
   ],
   "source": [
    "#### evaluation \n",
    "\n",
    "# LLM for generating questions\n",
    "llm_generator = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0.6)\n",
    "\n",
    "# Question generation prompt\n",
    "question_gen_template = \"\"\"Generate exactly {num_questions} diverse and challenging questions that would require complex web searches to answer. The questions should:\n",
    "\n",
    "1. Cover a wide range of topics (e.g., science, history, current events, technology, arts)\n",
    "2. Avoid long questions\n",
    "3. Ensure there is only one question per query. Query should NOT be multiple questions\n",
    "\n",
    "Please provide the questions as a numbered list, starting from 1 and ending at {num_questions}.\n",
    "\n",
    "Generated Questions:\"\"\"\n",
    "\n",
    "question_gen_prompt = PromptTemplate.from_template(question_gen_template)\n",
    "\n",
    "def generate_questions(num_questions, max_attempts=3):\n",
    "    for attempt in range(max_attempts):\n",
    "        question_gen_chain = question_gen_prompt | llm_generator | StrOutputParser()\n",
    "        questions_text = question_gen_chain.invoke({\"num_questions\": num_questions})\n",
    "\n",
    "        questions = []\n",
    "        for line in questions_text.split('\\n'):\n",
    "            match = re.match(r'^\\s*\\d+\\.\\s*(.+)$', line)\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                questions.append(question)\n",
    "\n",
    "        if len(questions) == num_questions:\n",
    "            return questions\n",
    "\n",
    "        print(f\"Attempt {attempt + 1}: Generated {len(questions)} questions instead of {num_questions}. Retrying...\")\n",
    "\n",
    "    raise ValueError(f\"Failed to generate exactly {num_questions} questions after {max_attempts} attempts.\")\n",
    "\n",
    "# Generate questions\n",
    "num_questions = 100\n",
    "\n",
    "evaluation_questions = generate_questions(num_questions)\n",
    "print(f\"Successfully generated {len(evaluation_questions)} questions:\")\n",
    "for i, question in enumerate(evaluation_questions, 1):\n",
    "    print(f\"{i}. {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbd89660-138d-444c-b4b3-b2bfa2b019e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the average lifespan of a quokka in the wild?',\n",
       " 'Which ancient civilization built the first known suspension bridge?',\n",
       " 'What is the chemical composition of the pigment used in the Mona Lisa?',\n",
       " 'Who is the founder of the first successful cryptocurrency?',\n",
       " 'What is the name of the largest living organism in the world?',\n",
       " 'In what year did the first computer bug occur?',\n",
       " 'What is the name of the ancient city buried under the sands of the Taklamakan Desert?',\n",
       " 'Who is the author of the first science fiction novel?',\n",
       " 'What is the process by which the human brain creates new neurons?',\n",
       " \"What is the name of the world's largest waterfall, by volume of water?\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evaluation_questions)\n",
    "evaluation_questions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1db54502-1c3f-4cc2-88d5-bb1a993bffd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the judge model (405B LLaMA)\n",
    "judge_model = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
    "\n",
    "def evaluate_answer_quality(question: str, answer: str, judge_model: Any) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer completely addresses the question.\n",
    "    Returns 1 if yes, 0 if no.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert evaluator. Your task is to determine if the given answer completely addresses the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \n",
    "    Does the answer completely address the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "def evaluate_document_selection(question: str, all_docs: List[str], selected_docs: List[str], judge_model: Any) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the selected documents are the best 10 out of the 80 to answer the question.\n",
    "    Returns 1 if yes, 0 if no.\n",
    "    \"\"\"\n",
    "    all_docs_text = \"\\n\".join([f\"{i+1}. {doc}...\" for i, doc in enumerate(all_docs)])\n",
    "    selected_indices = [all_docs.index(doc) + 1 for doc in selected_docs]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert information retrieval system. Your task is to determine if the selected documents are the best 10 out of the given 80 for answering the question completely.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Here are all 80 retrieved documents:\n",
    "    {all_docs_text}\n",
    "    \n",
    "    The system selected the following documents (by index): {', '.join(map(str, selected_indices))}\n",
    "    \n",
    "    Are these selected documents the best 10 out of the 80 for answering the question completely?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b9a25-d87d-4903-bf89-2412a8ab7128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating configuration: vectorstore_only\n",
      "Processing question 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 374\n",
      "-----HyDE generation time: 0.59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.37 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://genomics.senescence.info/species/entry.php?species=Setonix_brachyurus \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://australian.museum/learn/animals/mammals/quokka/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.42 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.83 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.46 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the average lifespan of a quokka in the wild, but the provided context only mentions that quokkas live over 10 years in the wild, without providing a specific average lifespan.\n",
      "\n",
      "Answer: I don't have enough information to answer that question.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 433\n",
      "-----HyDE generation time: 0.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Suspension_bridge \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-country-invented-the-first-suspension-bridge \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 429 while requesting https://www.quora.com/What-country-invented-the-first-suspension-bridge\n",
      "-----Web scraping time: 1.56 seconds\n",
      "Combined text length: 5000 characters\n",
      "Number of sentence windows: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.60 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 2.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.64 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks about the ancient civilization that built the first known suspension bridge. To answer this question, I need to look for information in the context about the earliest known suspension bridges and the civilization that built them.\n",
      "\n",
      "Answer: The Tibetan siddha and bridge-builder Thangtong Gyalpo originated the use of iron chains in his version of simple suspension bridges. In 1433, Gyalpo built eight bridges in eastern Bhutan. Therefore, the answer is the Tibetan civilization, specifically Thangtong Gyalpo.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 650\n",
      "-----HyDE generation time: 0.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.artnews.com/art-news/news/scientists-detect-rare-chemical-compound-mona-lisa-leonardo-1234681965/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.acs.org/pressroom/presspacs/2023/october/mona-lisa-hides-a-surprising-mix-of-toxic-pigments.html \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 1.60 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.03 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 3.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.77 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks about the chemical composition of the pigment used in the Mona Lisa. To answer this question, we need to look for information in the text that describes the chemical composition of the pigment used in the Mona Lisa.\n",
      "\n",
      "Answer: According to the text, the oil paint used by Leonardo da Vinci in the base layer of the Mona Lisa has a chemical composition distinct from his other works—and even those made by his famous contemporaries. The presence of the rare chemical compound, named plumbonacrite, has confirmed a long-held theory among art historians that Leonardo utilized lead oxide powder to thicken and dry the paint layers of the Mona Lisa.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 621\n",
      "-----HyDE generation time: 0.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.31 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Satoshi_Nakamoto \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.investopedia.com/terms/s/satoshi-nakamoto.asp \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://money.usnews.com/investing/articles/the-history-of-bitcoin: \n",
      "-----Web scraping time: 4.05 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.95 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.51 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the founder of the first successful cryptocurrency, which is Bitcoin. The context provided mentions Satoshi Nakamoto as the pseudonym for the person or people who developed Bitcoin, authored the Bitcoin whitepaper, and created the first blockchain database.\n",
      "\n",
      "Answer: Satoshi Nakamoto is the founder of the first successful cryptocurrency, Bitcoin.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 336\n",
      "-----HyDE generation time: 0.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.scientificamerican.com/article/strange-but-true-largest-organism-is-fungus/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Largest_organisms \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.montrealsciencecentre.com/blog/the-two-largest-living-organisms-on-earth \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 1.80 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.59 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.59 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The text mentions that the Armillaria solidipes is the second largest living organism on Earth, but it does not explicitly state that it is the largest. However, it also mentions that the discovery of this giant Armillaria ostoyae in 1998 heralded a new record holder for the title of the world's largest known organism, which is believed to be the 110-foot- (33.5-meter-) long, 200-ton blue whale. This suggests that the largest living organism is not a fungus, but rather a blue whale.\n",
      "\n",
      "Answer: The blue whale.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 568\n",
      "-----HyDE generation time: 0.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.globalapptesting.com/blog/the-worlds-first-computer-bug-global-app-testing \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/worlds-first-computer-bug/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://lunduke.substack.com/p/the-story-of-the-first-computer-bug \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 1.40 seconds\n",
      "Combined text length: 13563 characters\n",
      "Number of sentence windows: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.71 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.44 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year in which the first computer bug occurred. To answer this question, I need to find the relevant information in the context provided.\n",
      "\n",
      "Answer: According to the context, the first computer bug occurred on September 9, 1947.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 395\n",
      "-----HyDE generation time: 0.50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Taklamakan_Desert \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ancientpages.com/2020/06/01/secret-ancient-world-buried-under-the-vast-takla-makan-desert/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.60 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.16 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.98 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the name of the ancient city buried under the sands of the Taklamakan Desert. To answer this question, I need to identify the city mentioned in the context as being buried under the sand in the Taklamakan Desert.\n",
      "\n",
      "Answer: The ancient city buried under the sands of the Taklamakan Desert is Loulan.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 555\n",
      "-----HyDE generation time: 0.67 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.reddit.com/r/sciencefiction/comments/1728ki8/who_invented_science_fiction/ \"HTTP/1.1 302 Found\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Science_fiction \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 302 while requesting https://www.reddit.com/r/sciencefiction/comments/1728ki8/who_invented_science_fiction/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.theguardian.com/books/2016/may/23/work-from-1616-is-the-first-ever-science-fiction-novel \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 1.68 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.01 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 3.59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.57 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the first science fiction novel, which is mentioned in the context as Johann Valentin Andreae's work \"The Chemical Wedding\". To answer the question, we need to identify the author of this work.\n",
      "\n",
      "Answer: Johann Valentin Andreae.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 774\n",
      "-----HyDE generation time: 0.77 seconds\n",
      "-----Embedding time: 0.22 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.scientificamerican.com/article/the-adult-brain-does-grow-new-neurons-after-all-study-says/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://qbi.uq.edu.au/brain-basics/brain-physiology/what-neurogenesis \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://www.ninds.nih.gov/health-information/public-education/brain-basics/brain-basics-life-and-death-neuron: \n",
      "-----Web scraping time: 3.86 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.03 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.82 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question is asking about the process by which the human brain creates new neurons, which is a key concept in the provided context. To answer this question, I will look for information in the text that describes the process of creating new neurons in the human brain.\n",
      "\n",
      "Answer: The process by which the human brain creates new neurons is called neurogenesis. According to the text, neurogenesis is the process by which new neurons are formed in the brain, and it is crucial when an embryo is developing, but also continues in certain brain regions after birth and throughout our lifespan.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 447\n",
      "-----HyDE generation time: 0.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_waterfalls_by_flow_rate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.worldwaterfalldatabase.com/largest-waterfalls/volume \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://smartwatermagazine.com/q-a/which-largest-waterfall-world \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.95 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.40 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.71 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: To determine the name of the world's largest waterfall by volume of water, I will look for the waterfall with the highest flow rate or volume of water in the provided text.\n",
      "\n",
      "Answer: According to the text, Inga Falls is the largest waterfall in the world by flow rate, with an estimated flow rate of approximately 25,768.33 cubic metres per second. However, the text also provides a list of waterfalls by average volume, and Inga Falls is listed as having a volume of 910,000 cubic feet per second.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 594\n",
      "-----HyDE generation time: 0.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Jonas_Salk \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-polio-vaccination \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/\n",
      "-----Web scraping time: 1.78 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.11 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 3.78 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.55 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the inventor of the first successful polio vaccine, which is a specific piece of information that can be found in the context. To answer this question, I will look for a statement that directly mentions the inventor of the first successful polio vaccine.\n",
      "\n",
      "Answer: According to the context, the first successful vaccine was created by US physician Jonas Salk.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "def safe_add(a, b):\n",
    "    if isinstance(a, (int, float)) and isinstance(b, (int, float)):\n",
    "        return a + b\n",
    "    elif isinstance(a, list) and isinstance(b, list):\n",
    "        return [safe_add(x, y) for x, y in zip(a, b)]\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected types in safe_add: {type(a)} and {type(b)}\")\n",
    "        return 0  # or handle this case as appropriate\n",
    "\n",
    "async def run_evaluation(num_questions: int = 100):\n",
    "    questions = evaluation_questions\n",
    "    configurations = [\n",
    "        {\"name\": \"vectorstore_only\", \"use_bm25\": False, \"use_extended_queries\": False},\n",
    "        {\"name\": \"vectorstore_with_bm25\", \"use_bm25\": True, \"use_extended_queries\": False},\n",
    "        {\"name\": \"vectorstore_with_bm25_extended\", \"use_bm25\": True, \"use_extended_queries\": True}\n",
    "    ]\n",
    "    \n",
    "    for config in configurations:\n",
    "        results = []\n",
    "        total_answer_correct = 0\n",
    "        total_docs_correct = 0\n",
    "        total_times = [0] * 8  # For the 8 time measurements\n",
    "        \n",
    "        print(f\"Evaluating configuration: {config['name']}\")\n",
    "        \n",
    "        for i, question in enumerate(questions[:num_questions], 1):\n",
    "            print(f\"Processing question {i}/{num_questions}\")\n",
    "            \n",
    "            try:\n",
    "                answer, context_docs, times = await process_query(\n",
    "                    query=question,\n",
    "                    num_expansions=1,\n",
    "                    num_urls=3,\n",
    "                    num_docs=80,\n",
    "                    num_rerank=10,\n",
    "                    rerank_method=\"none\",\n",
    "                    use_70b_model=False,\n",
    "                    use_combined_retrieval=config['use_bm25'],\n",
    "                    use_extended_queries=config['use_extended_queries']\n",
    "                )\n",
    "                \n",
    "                all_docs = context_docs[:80]\n",
    "                selected_docs = context_docs[:10]\n",
    "                \n",
    "                answer_correct = evaluate_answer_quality(question, answer, judge_model)\n",
    "                docs_correct = evaluate_document_selection(question, all_docs, selected_docs, judge_model)\n",
    "                \n",
    "                total_answer_correct += answer_correct\n",
    "                total_docs_correct += docs_correct\n",
    "                total_times = [safe_add(total, t) for total, t in zip(total_times, times)]\n",
    "                \n",
    "                result = {\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"answer_correctness\": answer_correct,\n",
    "                    \"top_10_docs_correctness\": docs_correct,\n",
    "                    \"all_docs\": all_docs,\n",
    "                    \"selected_docs\": selected_docs,\n",
    "                    \"times\": times\n",
    "                }\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing question {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        avg_answer_correct = total_answer_correct / num_questions\n",
    "        avg_docs_correct = total_docs_correct / num_questions\n",
    "        avg_times = [t / num_questions if isinstance(t, (int, float)) else [x / num_questions for x in t] for t in total_times]\n",
    "        \n",
    "        print(f\"\\nAverage Results for {config['name']} over {num_questions} questions:\")\n",
    "        print(f\"Average Answer Correctness: {avg_answer_correct:.2f}\")\n",
    "        print(f\"Average Top 10 Documents Correctness: {avg_docs_correct:.2f}\")\n",
    "        print(f\"Average HyDE Time: {avg_times[0]:.2f} seconds\")\n",
    "        print(f\"Average Embedding Time: {avg_times[1]:.2f} seconds\")\n",
    "        print(f\"Average Query Expansion Time: {avg_times[2]:.2f} seconds\")\n",
    "        print(f\"Average Web Scraping Time: {avg_times[3]:.2f} seconds\")\n",
    "        print(f\"Average Vectorstore {'and BM25 ' if config['use_bm25'] else ''}Creation Time: {avg_times[4]:.2f} seconds\")\n",
    "        print(f\"Average Retrieval Time: {avg_times[5]:.2f} seconds\")\n",
    "        print(f\"Average Total Processing Time: {avg_times[6]:.2f} seconds\")\n",
    "        print(f\"Average Answer Generation Time: {avg_times[7]:.2f} seconds\")\n",
    "        \n",
    "        output = {\n",
    "            \"results\": results,\n",
    "            \"average_answer_correctness\": avg_answer_correct,\n",
    "            \"average_top_10_docs_correctness\": avg_docs_correct,\n",
    "            \"average_times\": {\n",
    "                \"hyde_time\": avg_times[0],\n",
    "                \"embedding_time\": avg_times[1],\n",
    "                \"query_expansion_time\": avg_times[2],\n",
    "                \"web_scraping_time\": avg_times[3],\n",
    "                \"vectorstore_creation_time\": avg_times[4],\n",
    "                \"retrieval_time\": avg_times[5],\n",
    "                \"total_processing_time\": avg_times[6],\n",
    "                \"answer_generation_time\": avg_times[7]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        filename = f'/home/ubuntu/maziar/12_efficient_ranking/evaluation/{config[\"name\"]}.json'\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults have been saved to '{filename}'\")\n",
    "\n",
    "# To run the evaluation, use:\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e54597-3393-47ce-ac63-a49878c4ef6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (eval_ranking)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
