{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "930968b4-02cf-4f40-8360-0e07c67c0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_fireworks import FireworksEmbeddings, ChatFireworks\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import faiss\n",
    "import httpx\n",
    "from urllib.parse import urlparse\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from pathlib import Path\n",
    "import traceback\n",
    "import random\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae80fd5e-0346-4d85-8373-12da58b452f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m FireworksEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnomic-ai/nomic-embed-text-v1.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m()\n\u001b[1;32m     15\u001b[0m llm_8b \u001b[38;5;241m=\u001b[39m ChatFireworks(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccounts/fireworks/models/llama-v3p1-8b-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m llm_70b \u001b[38;5;241m=\u001b[39m ChatFireworks(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccounts/fireworks/models/llama-v3p1-70b-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OpenAI' is not defined"
     ]
    }
   ],
   "source": [
    "# print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set up API clients\n",
    "# os.environ['FIREWORKS_API_KEY'] = 'API'\n",
    "# os.environ[\"SERPER_API_KEY\"] = 'API' # my api\n",
    "os.environ[\"SERPER_API_KEY\"] = 'API'\n",
    "os.environ[\"FIREWORKS_API_KEY\"] = 'API'\n",
    "os.environ[\"OPENAI_API_KEY\"] = 'API'\n",
    "\n",
    "# Initialize components\n",
    "search = GoogleSerperAPIWrapper(k=3)\n",
    "embeddings = FireworksEmbeddings(model=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "# llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm = OpenAI()\n",
    "llm_8b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_70b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "# Create a directory for caching in the user's home folder\n",
    "cache_dir = Path.home() / \".flashrank_cache\"\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU will be used automatically by FlashRank.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. CPU will be used.\")\n",
    "\n",
    "# Initialize FlashRank rerankers\n",
    "ranker_nano = Ranker(cache_dir=str(cache_dir))\n",
    "ranker_small = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=str(cache_dir))\n",
    "ranker_medium_t5 = Ranker(model_name=\"rank-T5-flan\", cache_dir=str(cache_dir))\n",
    "ranker_medium_multilang = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=str(cache_dir))\n",
    "ranker_large = Ranker(model_name=\"rank_zephyr_7b_v1_full\", max_length=1024, cache_dir=str(cache_dir))\n",
    "\n",
    "# Ensure models are on GPU if available\n",
    "for ranker in [ranker_nano, ranker_small, ranker_medium_t5, ranker_medium_multilang, ranker_large]:\n",
    "    if hasattr(ranker, 'model') and hasattr(ranker.model, 'to'):\n",
    "        ranker.model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Download NLTK data\n",
    "# nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d416271b-08ea-4f8f-b77e-6240688038b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever:\n",
    "    def __init__(self, documents: List[Document]):\n",
    "        self.documents = documents\n",
    "        self.tokenized_corpus = [doc.page_content.split() for doc in documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "        \n",
    "    def retrieve(self, query: str, extended_queries: List[str] = None, k: int = 100) -> List[Tuple[Document, float]]:\n",
    "        if extended_queries:\n",
    "            full_query = f\"{query} {' '.join(extended_queries)}\"\n",
    "        else:\n",
    "            full_query = query\n",
    "        \n",
    "        tokenized_query = full_query.split()\n",
    "        doc_scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Normalize BM25 scores to 0-1 range\n",
    "        max_score = max(doc_scores)\n",
    "        min_score = min(doc_scores)\n",
    "        score_range = max_score - min_score\n",
    "        normalized_scores = [(score - min_score) / score_range for score in doc_scores] if score_range != 0 else [1.0 for _ in doc_scores]\n",
    "        \n",
    "        top_k_indices = sorted(range(len(normalized_scores)), key=lambda i: normalized_scores[i], reverse=True)[:k]\n",
    "        return [(self.documents[i], normalized_scores[i]) for i in top_k_indices]\n",
    "\n",
    "def combine_retrieval_methods(query: str, vectorstores: List[FAISS], bm25_retriever: BM25Retriever, \n",
    "                              hyde_embedding: List[float], num_docs: int, extended_queries: List[str] = None, \n",
    "                              alpha: float = 0.7) -> List[Document]:\n",
    "    # Retrieve documents using vector search\n",
    "    vector_docs = []\n",
    "    for vectorstore in vectorstores:\n",
    "        docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs)\n",
    "        vector_docs.extend(docs)\n",
    "    \n",
    "    # Retrieve documents using BM25\n",
    "    bm25_docs = bm25_retriever.retrieve(query, extended_queries, k=num_docs)\n",
    "    \n",
    "    # Combine the results\n",
    "    combined_docs = {}\n",
    "    for doc in vector_docs:\n",
    "        combined_docs[doc.page_content] = alpha * (1 - vector_docs.index(doc) / len(vector_docs))\n",
    "    \n",
    "    for doc, score in bm25_docs:\n",
    "        if doc.page_content in combined_docs:\n",
    "            combined_docs[doc.page_content] += (1 - alpha) * score\n",
    "        else:\n",
    "            combined_docs[doc.page_content] = (1 - alpha) * score\n",
    "    \n",
    "    # Sort the combined results\n",
    "    sorted_docs = sorted(combined_docs.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [Document(page_content=content) for content, _ in sorted_docs[:num_docs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec19805-7363-4f1d-b12f-13fd280b33bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://checkip.amazonaws.com/ \"HTTP/1.1 200 \"\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/v2/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://7d0326a0cb2e25451e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://7d0326a0cb2e25451e.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7d0326a0cb2e25451e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://medlineplus.gov/eyecare.html \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.aoa.org/healthy-eyes/eye-and-vision-conditions/computer-vision-syndrome \"HTTP/1.1 302 Found\"\n",
      "INFO:httpx:HTTP Request: GET https://www.aao.org/eye-health/tips-prevention/computer-usage \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.cnet.com/health/personal-care/7-tips-to-take-care-of-your-eyes-if-you-work-in-front-of-a-computer-all-day/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "async def scrape_webpage(client, url):\n",
    "    try:\n",
    "        response = await client.get(url, timeout=3.0)\n",
    "        response.raise_for_status()\n",
    "        text = response.text\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        content = ' '.join(soup.stripped_strings)\n",
    "        return content[:5000], len(content[:5000])\n",
    "    except (httpx.RequestError, httpx.TimeoutException) as exc:\n",
    "        print(f\"An error occurred while requesting {url}: {exc}\")\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(f\"Error response {exc.response.status_code} while requesting {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return \"\", 0\n",
    "\n",
    "async def search_and_scrape(query, num_urls):\n",
    "    search_results = search.results(query)\n",
    "    scraped_urls = set()\n",
    "    full_texts = []\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=httpx.Timeout(10.0, connect=3.0)) as client:\n",
    "        tasks = []\n",
    "        if 'organic' in search_results:\n",
    "            for result in search_results['organic']:\n",
    "                url = result.get('link')\n",
    "                domain = urlparse(url).netloc if url else None\n",
    "                if url and domain not in scraped_urls and len(tasks) < num_urls:\n",
    "                    tasks.append(scrape_webpage(client, url))\n",
    "                    scraped_urls.add(domain)\n",
    "\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        for result in results:\n",
    "            if isinstance(result, tuple) and result[1] > 0:\n",
    "                full_texts.append(result[0])\n",
    "\n",
    "    return \" \".join(full_texts)\n",
    "\n",
    "def query_expansion(query, num_expansions):\n",
    "    expansion_prompt = f\"\"\"\n",
    "    Given the following search query, generate {num_expansions} additional related queries that could help find more comprehensive information on the topic. The queries should be different from each other and explore various aspects of the main query. Provide only the additional queries, numbered 1-{num_expansions}.\n",
    "\n",
    "    Main query: {query}\n",
    "\n",
    "    Additional queries:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(expansion_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    expanded_queries = [query]\n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.strip() and line[0].isdigit():\n",
    "            expanded_queries.append(line.split('. ', 1)[1].strip())\n",
    "\n",
    "    return expanded_queries[:num_expansions + 1]\n",
    "\n",
    "def create_sentence_windows(text, window_size=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    windows = []\n",
    "    for i in range(len(sentences)):\n",
    "        window = \" \".join(sentences[max(0, i-window_size):min(len(sentences), i+window_size+1)])\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def generate_hypothetical_document(query):\n",
    "    hyde_prompt = f\"\"\"\n",
    "    Given the search query below, generate a hypothetical document that would be a perfect match for this query. The document should be concise, containing only 3 sentences of relevant information that directly addresses the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Document (3 sentences):\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def llm_rerank(query, documents):\n",
    "    rerank_prompt = \"\"\"\n",
    "    Given the following query and a list of document excerpts, rank the documents based on their relevance to the query. Provide the rankings as a list of numbers from 1 to {}, where 1 is the most relevant. Ensure you provide a ranking for every document.\n",
    "\n",
    "    Query: {}\n",
    "\n",
    "    Documents:\n",
    "    {}\n",
    "\n",
    "    Rankings (1 to {}):\n",
    "    \"\"\".format(len(documents), query, \"\\n\".join([f\"{i+1}. {doc.page_content[:200]}...\" for i, doc in enumerate(documents)]), len(documents))\n",
    "\n",
    "    response = llm.invoke(rerank_prompt)\n",
    "    rankings = [int(x) for x in response.content.split() if x.isdigit()]\n",
    "\n",
    "    if len(rankings) < len(documents):\n",
    "        remaining = set(range(1, len(documents) + 1)) - set(rankings)\n",
    "        rankings.extend(remaining)\n",
    "\n",
    "    sorted_docs = sorted(zip(documents, rankings), key=lambda x: x[1])\n",
    "    return sorted_docs\n",
    "\n",
    "def flashrank_rerank(query, documents, ranker):\n",
    "    rerank_request = RerankRequest(\n",
    "        query=query,\n",
    "        passages=[{\"text\": doc.page_content} for doc in documents]\n",
    "    )\n",
    "    reranked = ranker.rerank(rerank_request)\n",
    "    \n",
    "    if isinstance(reranked, list) and isinstance(reranked[0], dict):\n",
    "        sorted_results = sorted(reranked, key=lambda x: x.get('score', 0), reverse=True)\n",
    "        return [(documents[i], result.get('score', 0)) for i, result in enumerate(sorted_results)]\n",
    "    \n",
    "    elif isinstance(reranked, list) and hasattr(reranked[0], 'score'):\n",
    "        sorted_results = sorted(reranked, key=lambda x: x.score, reverse=True)\n",
    "        return [(documents[i], result.score) for i, result in enumerate(sorted_results)]\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unexpected reranked result type. Using original document order.\")\n",
    "        return [(doc, 1.0) for doc in documents]\n",
    "\n",
    "\n",
    "def batch_embed_documents(documents, embeddings, batch_size=512):\n",
    "    batched_embeddings = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        texts = [doc.page_content for doc in batch]\n",
    "        embeddings_batch = embeddings.embed_documents(texts)\n",
    "        batched_embeddings.extend(embeddings_batch)\n",
    "    return batched_embeddings\n",
    "\n",
    "def create_single_vectorstore(index_documents, embeddings):\n",
    "    vectorstore_start = time.time()\n",
    "    \n",
    "    all_documents = []\n",
    "    for doc in index_documents:\n",
    "        all_documents.append(Document(page_content=doc.page_content))\n",
    "    \n",
    "    # Batch process all embeddings\n",
    "    batch_embeddings = batch_embed_documents(all_documents, embeddings)\n",
    "    \n",
    "    # Create a single FAISS vectorstore\n",
    "    texts = [doc.page_content for doc in all_documents]\n",
    "    vectorstore = FAISS.from_embeddings(\n",
    "        embedding=embeddings,\n",
    "        text_embeddings=list(zip(texts, batch_embeddings))\n",
    "    )\n",
    "    \n",
    "    vectorstore_time = time.time() - vectorstore_start\n",
    "    print(f\"-----Vectorstore creation time: {vectorstore_time:.2f} seconds\")\n",
    "    \n",
    "    return vectorstore, all_documents\n",
    "\n",
    "\n",
    "# Update the retriever function to use the single vectorstore:\n",
    "def get_hyde_retriever(vectorstore, hyde_embedding, num_docs, num_rerank, rerank_method):\n",
    "    def retriever(query):\n",
    "        docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs)\n",
    "        \n",
    "        unique_docs = []\n",
    "        seen_content = set()\n",
    "        for doc in docs:\n",
    "            content = doc.page_content\n",
    "            if content not in seen_content:\n",
    "                unique_docs.append(Document(page_content=content))\n",
    "                seen_content.add(content)\n",
    "\n",
    "        try:\n",
    "            if rerank_method == \"none\":\n",
    "                return unique_docs[:num_rerank]\n",
    "            elif rerank_method == \"llm\":\n",
    "                reranked_docs = llm_rerank(query, unique_docs)\n",
    "            elif rerank_method in [\"nano\", \"small\", \"medium_t5\", \"medium_multilang\", \"large\"]:\n",
    "                ranker = globals()[f\"ranker_{rerank_method}\"]\n",
    "                reranked_docs = flashrank_rerank(query, unique_docs, ranker)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown rerank method: {rerank_method}\")\n",
    "\n",
    "            return [doc for doc, _ in reranked_docs[:num_rerank]]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reranking with method {rerank_method}: {str(e)}\")\n",
    "            print(\"Traceback:\", traceback.format_exc())\n",
    "            print(\"Falling back to no reranking.\")\n",
    "            return unique_docs[:num_rerank]\n",
    "\n",
    "    return retriever\n",
    "\n",
    "async def process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model, use_combined_retrieval, use_extended_queries):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        hyde_start = time.time()\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        hyde_time = time.time() - hyde_start\n",
    "        print(f\"hypothetical_doc length: {len(hypothetical_doc)}\")\n",
    "        print(f\"-----HyDE generation time: {hyde_time:.2f} seconds\")\n",
    "\n",
    "        embed_start = time.time()\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "        embed_time = time.time() - embed_start\n",
    "        print(f\"-----Embedding time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        ext_start = time.time()\n",
    "        extended_queries = query_expansion(query, num_expansions) if use_extended_queries else [query]\n",
    "        ext_time = time.time() - ext_start\n",
    "        print(f\"-----Query expansion time: {ext_time:.2f} seconds\")\n",
    "\n",
    "        scrape_start = time.time()\n",
    "        all_texts = await asyncio.gather(*[search_and_scrape(eq, num_urls) for eq in extended_queries])\n",
    "        scrape_time = time.time() - scrape_start\n",
    "        print(f\"-----Web scraping time: {scrape_time:.2f} seconds\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        sentence_windows = create_sentence_windows(combined_text)\n",
    "        print(f\"Number of sentence windows: {len(sentence_windows)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=window) for window in sentence_windows]\n",
    "\n",
    "        vectorstore_start = time.time()\n",
    "        vectorstore, all_documents = create_single_vectorstore(index_documents, embeddings)\n",
    "        \n",
    "        bm25_retriever = None\n",
    "        if use_combined_retrieval:\n",
    "            bm25_retriever = BM25Retriever(all_documents)\n",
    "        \n",
    "        vectorstore_time = time.time() - vectorstore_start\n",
    "        print(f\"-----Vectorstore {'and BM25 ' if use_combined_retrieval else ''}creation time: {vectorstore_time:.2f} seconds\")\n",
    "    \n",
    "        retrieval_start = time.time()\n",
    "        if use_combined_retrieval:\n",
    "            retrieved_docs = combine_retrieval_methods(query, [vectorstore], bm25_retriever, hyde_embedding, num_docs, extended_queries if use_extended_queries else None)\n",
    "        else:\n",
    "            retriever = get_hyde_retriever(vectorstore, hyde_embedding, num_docs, num_rerank, rerank_method)\n",
    "            retrieved_docs = retriever(query)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        print(f\"-----Retrieval and reranking time: {retrieval_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"Number of retrieved and reranked documents: {len(retrieved_docs)}\")\n",
    "\n",
    "        context_docs = [doc.page_content for doc in retrieved_docs]\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "\n",
    "        total_processing_time = hyde_time + embed_time + scrape_time + vectorstore_time + retrieval_time\n",
    "        print(f\"-----Total processing time before answer generation: {total_processing_time:.2f} seconds\")\n",
    "\n",
    "        answer_start = time.time()\n",
    "        prompt_template = \"\"\"\n",
    "        Use the following context to answer the question. Before answering the question generate a reasoning step. then answer.\n",
    "        If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        chosen_llm = llm_70b if use_70b_model else llm_8b\n",
    "\n",
    "        rag_chain = prompt | chosen_llm | StrOutputParser()\n",
    "        answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "        answer_time = time.time() - answer_start\n",
    "        print(f\"-----Answer generation time: {answer_time:.2f} seconds\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*120)\n",
    "        print(\"Final Answer:\\n\", answer)\n",
    "        print(\"-\"*120)\n",
    "\n",
    "        return answer, context_docs, [hyde_time, hyde_embedding, ext_time, scrape_time, vectorstore_time, retrieval_time, total_processing_time, answer_time]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\", []\n",
    "\n",
    "def gradio_interface(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model, use_combined_retrieval, use_extended_queries):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "    answer, context_docs, _ = asyncio.run(process_query(query, num_expansions, num_urls, num_docs, num_rerank, rerank_method, use_70b_model, use_combined_retrieval, use_extended_queries))\n",
    "\n",
    "    sys.stdout = old_stdout\n",
    "    captured_output = buffer.getvalue()\n",
    "\n",
    "    truncated_docs = [f\"Document {i+1}: {doc[:150]}...\" for i, doc in enumerate(context_docs)]\n",
    "    truncated_context = \"\\n\\n\".join(truncated_docs)\n",
    "\n",
    "    captured_output += f\"\\n\\nContext used for answer generation (first 150 characters of each document, {len(context_docs)} documents in total):\\n\" + truncated_context\n",
    "\n",
    "    return captured_output\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your query\"),\n",
    "        gr.Slider(minimum=0, maximum=3, value=1, step=1, label=\"Number of query expansions\"),\n",
    "        gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of URLs to scrape per extended query\"),\n",
    "        gr.Slider(minimum=20, maximum=80, value=80, step=1, label=\"Number of documents to retrieve with HyDE\"),\n",
    "        gr.Slider(minimum=10, maximum=80, value=50, step=1, label=\"Number of documents to keep after retrieval/reranking\"),\n",
    "        gr.Radio([\"none\", \"llm\", \"nano\", \"small\", \"medium_t5\", \"medium_multilang\"], label=\"Reranking method\", value=\"none\"),\n",
    "        gr.Checkbox(label=\"Use 70B model for QA (unchecked uses 8B)\", value=False),\n",
    "        gr.Checkbox(label=\"Use combined BM25 and embedding retrieval\", value=False),\n",
    "        gr.Checkbox(label=\"Use extended queries for BM25\", value=False)\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Advanced RAG Query Processing\",\n",
    "    description=\"Enter a query and adjust parameters to get a detailed answer based on web search and document analysis.\",\n",
    "    examples=[\n",
    "        [\"How can I take care of my eyes?\", 1, 3, 80, 50, \"llm\", False, False, False],\n",
    "        [\"How can I take care of my eyes?\", 1, 3, 80, 50, \"nano\", False, True, True]\n",
    "    ]\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "918a8247-ee12-4ca6-a588-20978b2ac68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "#### evaluation \n",
    "\n",
    "# LLM for generating questions\n",
    "llm_generator = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0.6)\n",
    "\n",
    "# Question generation prompt\n",
    "question_gen_template = \"\"\"Generate exactly {num_questions} diverse and challenging questions that would require complex web searches to answer. The questions should:\n",
    "\n",
    "1. Cover a wide range of topics (e.g., science, history, current events, technology, arts)\n",
    "2. Avoid long questions\n",
    "3. Ensure there is only one question per query. Query should NOT be multiple questions\n",
    "\n",
    "Please provide the questions as a numbered list, starting from 1 and ending at {num_questions}.\n",
    "\n",
    "Generated Questions:\"\"\"\n",
    "\n",
    "question_gen_prompt = PromptTemplate.from_template(question_gen_template)\n",
    "\n",
    "def generate_questions(num_questions, max_attempts=3):\n",
    "    for attempt in range(max_attempts):\n",
    "        question_gen_chain = question_gen_prompt | llm_generator | StrOutputParser()\n",
    "        questions_text = question_gen_chain.invoke({\"num_questions\": num_questions})\n",
    "\n",
    "        questions = []\n",
    "        for line in questions_text.split('\\n'):\n",
    "            match = re.match(r'^\\s*\\d+\\.\\s*(.+)$', line)\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                questions.append(question)\n",
    "\n",
    "        if len(questions) == num_questions:\n",
    "            return questions\n",
    "\n",
    "        print(f\"Attempt {attempt + 1}: Generated {len(questions)} questions instead of {num_questions}. Retrying...\")\n",
    "\n",
    "    raise ValueError(f\"Failed to generate exactly {num_questions} questions after {max_attempts} attempts.\")\n",
    "\n",
    "# Generate questions\n",
    "num_questions = 100\n",
    "\n",
    "evaluation_questions = generate_questions(num_questions)\n",
    "print(f\"Successfully generated {len(evaluation_questions)} questions:\")\n",
    "for i, question in enumerate(evaluation_questions, 1):\n",
    "    print(f\"{i}. {question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbd89660-138d-444c-b4b3-b2bfa2b019e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the average lifespan of a quokka in the wild?',\n",
       " 'Which ancient civilization built the first known suspension bridge?',\n",
       " 'What is the chemical composition of the pigment used in the Mona Lisa?',\n",
       " 'Who is the founder of the first successful cryptocurrency?',\n",
       " 'What is the name of the largest living organism in the world?',\n",
       " 'In what year did the first computer bug occur?',\n",
       " 'What is the name of the ancient city buried under the sands of the Taklamakan Desert?',\n",
       " 'Who is the author of the first science fiction novel?',\n",
       " 'What is the process by which the human brain creates new neurons?',\n",
       " \"What is the name of the world's largest waterfall, by volume of water?\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evaluation_questions)\n",
    "evaluation_questions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1db54502-1c3f-4cc2-88d5-bb1a993bffd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the judge model (405B LLaMA)\n",
    "judge_model = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
    "\n",
    "def evaluate_answer_quality(question: str, answer: str, judge_model: Any) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the answer completely addresses the question.\n",
    "    Returns 1 if yes, 0 if no.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert evaluator. Your task is to determine if the given answer completely addresses the question.\n",
    "    \n",
    "    Question: {question}\n",
    "    Answer: {answer}\n",
    "    \n",
    "    Does the answer completely address the question?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0\n",
    "\n",
    "def evaluate_document_selection(question: str, all_docs: List[str], selected_docs: List[str], judge_model: Any) -> int:\n",
    "    \"\"\"\n",
    "    Evaluate if the selected documents are the best 10 out of the 80 to answer the question.\n",
    "    Returns 1 if yes, 0 if no.\n",
    "    \"\"\"\n",
    "    all_docs_text = \"\\n\".join([f\"{i+1}. {doc}...\" for i, doc in enumerate(all_docs)])\n",
    "    selected_indices = [all_docs.index(doc) + 1 for doc in selected_docs]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert information retrieval system. Your task is to determine if the selected documents are the best 10 out of the given 80 for answering the question completely.\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Here are all 80 retrieved documents:\n",
    "    {all_docs_text}\n",
    "    \n",
    "    The system selected the following documents (by index): {', '.join(map(str, selected_indices))}\n",
    "    \n",
    "    Are these selected documents the best 10 out of the 80 for answering the question completely?\n",
    "    Respond with only 'Yes' or 'No'.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.invoke(prompt)\n",
    "    return 1 if response.content.strip().lower() == 'yes' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b9a25-d87d-4903-bf89-2412a8ab7128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating configuration: vectorstore_only\n",
      "Processing question 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 374\n",
      "-----HyDE generation time: 0.59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.37 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://genomics.senescence.info/species/entry.php?species=Setonix_brachyurus \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://australian.museum/learn/animals/mammals/quokka/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.42 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.83 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.46 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the average lifespan of a quokka in the wild, but the provided context only mentions that quokkas live over 10 years in the wild, without providing a specific average lifespan.\n",
      "\n",
      "Answer: I don't have enough information to answer that question.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 433\n",
      "-----HyDE generation time: 0.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Suspension_bridge \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.quora.com/What-country-invented-the-first-suspension-bridge \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 429 while requesting https://www.quora.com/What-country-invented-the-first-suspension-bridge\n",
      "-----Web scraping time: 1.56 seconds\n",
      "Combined text length: 5000 characters\n",
      "Number of sentence windows: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.60 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 2.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.64 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks about the ancient civilization that built the first known suspension bridge. To answer this question, I need to look for information in the context about the earliest known suspension bridges and the civilization that built them.\n",
      "\n",
      "Answer: The Tibetan siddha and bridge-builder Thangtong Gyalpo originated the use of iron chains in his version of simple suspension bridges. In 1433, Gyalpo built eight bridges in eastern Bhutan. Therefore, the answer is the Tibetan civilization, specifically Thangtong Gyalpo.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 650\n",
      "-----HyDE generation time: 0.70 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.artnews.com/art-news/news/scientists-detect-rare-chemical-compound-mona-lisa-leonardo-1234681965/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.acs.org/pressroom/presspacs/2023/october/mona-lisa-hides-a-surprising-mix-of-toxic-pigments.html \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 1.60 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.03 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 3.57 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.77 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks about the chemical composition of the pigment used in the Mona Lisa. To answer this question, we need to look for information in the text that describes the chemical composition of the pigment used in the Mona Lisa.\n",
      "\n",
      "Answer: According to the text, the oil paint used by Leonardo da Vinci in the base layer of the Mona Lisa has a chemical composition distinct from his other worksâ€”and even those made by his famous contemporaries. The presence of the rare chemical compound, named plumbonacrite, has confirmed a long-held theory among art historians that Leonardo utilized lead oxide powder to thicken and dry the paint layers of the Mona Lisa.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 621\n",
      "-----HyDE generation time: 0.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.31 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Satoshi_Nakamoto \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.investopedia.com/terms/s/satoshi-nakamoto.asp \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://money.usnews.com/investing/articles/the-history-of-bitcoin: \n",
      "-----Web scraping time: 4.05 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 0.95 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.51 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the founder of the first successful cryptocurrency, which is Bitcoin. The context provided mentions Satoshi Nakamoto as the pseudonym for the person or people who developed Bitcoin, authored the Bitcoin whitepaper, and created the first blockchain database.\n",
      "\n",
      "Answer: Satoshi Nakamoto is the founder of the first successful cryptocurrency, Bitcoin.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 336\n",
      "-----HyDE generation time: 0.45 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.scientificamerican.com/article/strange-but-true-largest-organism-is-fungus/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Largest_organisms \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.montrealsciencecentre.com/blog/the-two-largest-living-organisms-on-earth \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 1.80 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.59 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.59 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The text mentions that the Armillaria solidipes is the second largest living organism on Earth, but it does not explicitly state that it is the largest. However, it also mentions that the discovery of this giant Armillaria ostoyae in 1998 heralded a new record holder for the title of the world's largest known organism, which is believed to be the 110-foot- (33.5-meter-) long, 200-ton blue whale. This suggests that the largest living organism is not a fungus, but rather a blue whale.\n",
      "\n",
      "Answer: The blue whale.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 568\n",
      "-----HyDE generation time: 0.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.globalapptesting.com/blog/the-worlds-first-computer-bug-global-app-testing \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://education.nationalgeographic.org/resource/worlds-first-computer-bug/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://lunduke.substack.com/p/the-story-of-the-first-computer-bug \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 1.40 seconds\n",
      "Combined text length: 13563 characters\n",
      "Number of sentence windows: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.71 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.44 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the year in which the first computer bug occurred. To answer this question, I need to find the relevant information in the context provided.\n",
      "\n",
      "Answer: According to the context, the first computer bug occurred on September 9, 1947.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 395\n",
      "-----HyDE generation time: 0.50 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.24 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Taklamakan_Desert \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ancientpages.com/2020/06/01/secret-ancient-world-buried-under-the-vast-takla-makan-desert/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.60 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.16 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 4.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.98 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the name of the ancient city buried under the sands of the Taklamakan Desert. To answer this question, I need to identify the city mentioned in the context as being buried under the sand in the Taklamakan Desert.\n",
      "\n",
      "Answer: The ancient city buried under the sands of the Taklamakan Desert is Loulan.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 555\n",
      "-----HyDE generation time: 0.67 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.reddit.com/r/sciencefiction/comments/1728ki8/who_invented_science_fiction/ \"HTTP/1.1 302 Found\"\n",
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Science_fiction \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 302 while requesting https://www.reddit.com/r/sciencefiction/comments/1728ki8/who_invented_science_fiction/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.theguardian.com/books/2016/may/23/work-from-1616-is-the-first-ever-science-fiction-novel \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 1.68 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.01 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 3.59 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.57 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the author of the first science fiction novel, which is mentioned in the context as Johann Valentin Andreae's work \"The Chemical Wedding\". To answer the question, we need to identify the author of this work.\n",
      "\n",
      "Answer: Johann Valentin Andreae.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 774\n",
      "-----HyDE generation time: 0.77 seconds\n",
      "-----Embedding time: 0.22 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://www.scientificamerican.com/article/the-adult-brain-does-grow-new-neurons-after-all-study-says/ \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://qbi.uq.edu.au/brain-basics/brain-physiology/what-neurogenesis \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while requesting https://www.ninds.nih.gov/health-information/public-education/brain-basics/brain-basics-life-and-death-neuron: \n",
      "-----Web scraping time: 3.86 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.03 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.82 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question is asking about the process by which the human brain creates new neurons, which is a key concept in the provided context. To answer this question, I will look for information in the text that describes the process of creating new neurons in the human brain.\n",
      "\n",
      "Answer: The process by which the human brain creates new neurons is called neurogenesis. According to the text, neurogenesis is the process by which new neurons are formed in the brain, and it is crucial when an embryo is developing, but also continues in certain brain regions after birth and throughout our lifespan.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 447\n",
      "-----HyDE generation time: 0.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/List_of_waterfalls_by_flow_rate \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.worldwaterfalldatabase.com/largest-waterfalls/volume \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://smartwatermagazine.com/q-a/which-largest-waterfall-world \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Web scraping time: 2.95 seconds\n",
      "Combined text length: 15002 characters\n",
      "Number of sentence windows: 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.40 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 5.37 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.71 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: To determine the name of the world's largest waterfall by volume of water, I will look for the waterfall with the highest flow rate or volume of water in the provided text.\n",
      "\n",
      "Answer: According to the text, Inga Falls is the largest waterfall in the world by flow rate, with an estimated flow rate of approximately 25,768.33 cubic metres per second. However, the text also provides a list of waterfalls by average volume, and Inga Falls is listed as having a volume of 910,000 cubic feet per second.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unexpected types in safe_add: <class 'int'> and <class 'list'>\n",
      "Processing question 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothetical_doc length: 594\n",
      "-----HyDE generation time: 0.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Embedding time: 0.23 seconds\n",
      "-----Query expansion time: 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://en.wikipedia.org/wiki/Jonas_Salk \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.who.int/news-room/spotlight/history-of-vaccination/history-of-polio-vaccination \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/ \"HTTP/1.1 403 Forbidden\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response 403 while requesting https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6351694/\n",
      "-----Web scraping time: 1.78 seconds\n",
      "Combined text length: 10001 characters\n",
      "Number of sentence windows: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Vectorstore creation time: 1.11 seconds\n",
      "-----Retrieval and reranking time: 0.00 seconds\n",
      "Number of retrieved and reranked documents: 10\n",
      "-----Total processing time before answer generation: 3.78 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Answer generation time: 0.55 seconds\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Final Answer:\n",
      " Reasoning step: The question asks for the inventor of the first successful polio vaccine, which is a specific piece of information that can be found in the context. To answer this question, I will look for a statement that directly mentions the inventor of the first successful polio vaccine.\n",
      "\n",
      "Answer: According to the context, the first successful vaccine was created by US physician Jonas Salk.\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.fireworks.ai/inference/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "def safe_add(a, b):\n",
    "    if isinstance(a, (int, float)) and isinstance(b, (int, float)):\n",
    "        return a + b\n",
    "    elif isinstance(a, list) and isinstance(b, list):\n",
    "        return [safe_add(x, y) for x, y in zip(a, b)]\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected types in safe_add: {type(a)} and {type(b)}\")\n",
    "        return 0  # or handle this case as appropriate\n",
    "\n",
    "async def run_evaluation(num_questions: int = 100):\n",
    "    questions = evaluation_questions\n",
    "    configurations = [\n",
    "        {\"name\": \"vectorstore_only\", \"use_bm25\": False, \"use_extended_queries\": False},\n",
    "        {\"name\": \"vectorstore_with_bm25\", \"use_bm25\": True, \"use_extended_queries\": False},\n",
    "        {\"name\": \"vectorstore_with_bm25_extended\", \"use_bm25\": True, \"use_extended_queries\": True}\n",
    "    ]\n",
    "    \n",
    "    for config in configurations:\n",
    "        results = []\n",
    "        total_answer_correct = 0\n",
    "        total_docs_correct = 0\n",
    "        total_times = [0] * 8  # For the 8 time measurements\n",
    "        \n",
    "        print(f\"Evaluating configuration: {config['name']}\")\n",
    "        \n",
    "        for i, question in enumerate(questions[:num_questions], 1):\n",
    "            print(f\"Processing question {i}/{num_questions}\")\n",
    "            \n",
    "            try:\n",
    "                answer, context_docs, times = await process_query(\n",
    "                    query=question,\n",
    "                    num_expansions=1,\n",
    "                    num_urls=3,\n",
    "                    num_docs=80,\n",
    "                    num_rerank=10,\n",
    "                    rerank_method=\"none\",\n",
    "                    use_70b_model=False,\n",
    "                    use_combined_retrieval=config['use_bm25'],\n",
    "                    use_extended_queries=config['use_extended_queries']\n",
    "                )\n",
    "                \n",
    "                all_docs = context_docs[:80]\n",
    "                selected_docs = context_docs[:10]\n",
    "                \n",
    "                answer_correct = evaluate_answer_quality(question, answer, judge_model)\n",
    "                docs_correct = evaluate_document_selection(question, all_docs, selected_docs, judge_model)\n",
    "                \n",
    "                total_answer_correct += answer_correct\n",
    "                total_docs_correct += docs_correct\n",
    "                total_times = [safe_add(total, t) for total, t in zip(total_times, times)]\n",
    "                \n",
    "                result = {\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"answer_correctness\": answer_correct,\n",
    "                    \"top_10_docs_correctness\": docs_correct,\n",
    "                    \"all_docs\": all_docs,\n",
    "                    \"selected_docs\": selected_docs,\n",
    "                    \"times\": times\n",
    "                }\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing question {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        avg_answer_correct = total_answer_correct / num_questions\n",
    "        avg_docs_correct = total_docs_correct / num_questions\n",
    "        avg_times = [t / num_questions if isinstance(t, (int, float)) else [x / num_questions for x in t] for t in total_times]\n",
    "        \n",
    "        print(f\"\\nAverage Results for {config['name']} over {num_questions} questions:\")\n",
    "        print(f\"Average Answer Correctness: {avg_answer_correct:.2f}\")\n",
    "        print(f\"Average Top 10 Documents Correctness: {avg_docs_correct:.2f}\")\n",
    "        print(f\"Average HyDE Time: {avg_times[0]:.2f} seconds\")\n",
    "        print(f\"Average Embedding Time: {avg_times[1]:.2f} seconds\")\n",
    "        print(f\"Average Query Expansion Time: {avg_times[2]:.2f} seconds\")\n",
    "        print(f\"Average Web Scraping Time: {avg_times[3]:.2f} seconds\")\n",
    "        print(f\"Average Vectorstore {'and BM25 ' if config['use_bm25'] else ''}Creation Time: {avg_times[4]:.2f} seconds\")\n",
    "        print(f\"Average Retrieval Time: {avg_times[5]:.2f} seconds\")\n",
    "        print(f\"Average Total Processing Time: {avg_times[6]:.2f} seconds\")\n",
    "        print(f\"Average Answer Generation Time: {avg_times[7]:.2f} seconds\")\n",
    "        \n",
    "        output = {\n",
    "            \"results\": results,\n",
    "            \"average_answer_correctness\": avg_answer_correct,\n",
    "            \"average_top_10_docs_correctness\": avg_docs_correct,\n",
    "            \"average_times\": {\n",
    "                \"hyde_time\": avg_times[0],\n",
    "                \"embedding_time\": avg_times[1],\n",
    "                \"query_expansion_time\": avg_times[2],\n",
    "                \"web_scraping_time\": avg_times[3],\n",
    "                \"vectorstore_creation_time\": avg_times[4],\n",
    "                \"retrieval_time\": avg_times[5],\n",
    "                \"total_processing_time\": avg_times[6],\n",
    "                \"answer_generation_time\": avg_times[7]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        filename = f'/home/ubuntu/maziar/12_efficient_ranking/evaluation/{config[\"name\"]}.json'\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nResults have been saved to '{filename}'\")\n",
    "\n",
    "# To run the evaluation, use:\n",
    "await run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e54597-3393-47ce-ac63-a49878c4ef6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (eval_ranking)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
