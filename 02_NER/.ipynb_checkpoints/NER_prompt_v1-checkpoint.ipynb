{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 59238,
     "status": "ok",
     "timestamp": 1722434373153,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "8uS3H8y-DKdu",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5ec23a6f-522d-43bc-cecb-4094b511bd0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.20-py3-none-any.whl.metadata (659 bytes)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain_fireworks\n",
      "  Downloading langchain_fireworks-0.1.6-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
      "Collecting gradio\n",
      "  Downloading gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.23 (from langchain_community)\n",
      "  Downloading langchain_core-0.2.25-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
      "  Downloading langsmith-0.1.94-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
      "Collecting openai<2.0.0,>=1.32.0 (from langchain-openai)\n",
      "  Downloading openai-1.37.1-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.1)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.8.2)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.111.1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.30.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.4)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.3)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx>=0.27.0 (from chromadb)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fireworks-ai>=0.13.0 (from langchain_fireworks)\n",
      "  Downloading fireworks_ai-0.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.1.1 (from gradio)\n",
      "  Downloading gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.5)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.5.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.1.1->gradio) (2024.6.1)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.1->gradio)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting fastapi-cli>=0.0.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting email_validator>=2.0.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse (from fireworks-ai>=0.13.0->langchain_fireworks)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.23->langchain_community)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.7.0)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=8.0.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (71.0.4)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (13.7.1)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi>=0.95.2->chromadb)\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.19.2)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain_community)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
      "Downloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.1.19-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n",
      "Downloading chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_fireworks-0.1.6-py3-none-any.whl (16 kB)\n",
      "Downloading gradio-4.39.0-py3-none-any.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.1.1-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fireworks_ai-0.14.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.25-py3-none-any.whl (377 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.6/377.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.94-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.37.1-py3-none-any.whl (337 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl (29 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl (15 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.47b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading ruff-0.5.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n",
      "Downloading uvicorn-0.30.3-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Building wheels for collected packages: google-search-results, pypika\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=2b84e3620bce7cbc22eff1e6620045a338ab580ad449e036e4023504506a6017\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=48a939031a19721aa50a6701d5d3d8d2b9f7c08246b1c34937218bc9c90bf7d8\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
      "Successfully built google-search-results pypika\n",
      "Installing collected packages: pypika, pydub, monotonic, mmh3, websockets, uvloop, types-requests, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, importlib-metadata, humanfriendly, httpx-sse, httptools, h11, ffmpy, dnspython, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, aiofiles, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, langchainhub, jsonpatch, httpcore, google-search-results, email_validator, coloredlogs, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, httpx, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation-asgi, openai, langchain-core, gradio-client, fireworks-ai, fastapi-cli, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, langchain_fireworks, fastapi, langchain, gradio, chromadb, langchain_community\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.13.0\n",
      "    Uninstalling tomlkit-0.13.0:\n",
      "      Successfully uninstalled tomlkit-0.13.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib_metadata 8.2.0\n",
      "    Uninstalling importlib_metadata-8.2.0:\n",
      "      Successfully uninstalled importlib_metadata-8.2.0\n",
      "Successfully installed aiofiles-23.2.1 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.5 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 ffmpy-0.4.0 fireworks-ai-0.14.0 google-search-results-2.4.2 gradio-4.39.0 gradio-client-1.1.1 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 httpx-sse-0.4.0 humanfriendly-10.0 importlib-metadata-8.0.0 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-30.1.0 langchain-0.2.11 langchain-core-0.2.25 langchain-openai-0.1.19 langchain-text-splitters-0.2.2 langchain_community-0.2.10 langchain_fireworks-0.1.6 langchainhub-0.1.20 langsmith-0.1.94 marshmallow-3.21.3 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.18.1 openai-1.37.1 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-instrumentation-0.47b0 opentelemetry-instrumentation-asgi-0.47b0 opentelemetry-instrumentation-fastapi-0.47b0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-util-http-0.47b0 orjson-3.10.6 overrides-7.7.0 posthog-3.5.0 pydub-0.25.1 pypika-0.48.9 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.5.5 semantic-version-2.10.0 starlette-0.37.2 tiktoken-0.7.0 tomlkit-0.12.0 types-requests-2.32.0.20240712 typing-inspect-0.9.0 uvicorn-0.30.3 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langchain_fireworks google-search-results requests gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZOJPeq3DUgR"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "# os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "# os.environ['LANGCHAIN_API_KEY'] = 'API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pO8Bp6MTDVLf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['FIREWORKS_API_KEY'] = 'API_KEY'\n",
    "os.environ[\"SERPER_API_KEY\"] = 'API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MWNbV0RDQ_EG",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os, bs4, re, time, json\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_fireworks import FireworksEmbeddings, ChatFireworks\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from collections import defaultdict\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haT8ApGDvM7K"
   },
   "outputs": [],
   "source": [
    "prompt_1 = \"\"\"Task: Named Entity Recognition (NER)\n",
    "Description: Perform Named Entity Recognition by analyzing the input text to identify named entities among nouns and classify them into specified categories: LOC (Location), MISC (Miscellaneous), ORG (Organization), and PER (Person). The output should be a dictionary where each key is a noun from the input sentence and the value is the corresponding entity tag.\n",
    "\n",
    "Process:\n",
    "1. Read the input text and identify nouns that potentially represent named entities.\n",
    "2. Classify each noun according to the context it appears in:\n",
    "   - LOC: Geographical locations like cities or countries.\n",
    "   - MISC: Entities that don't fit other categories, such as events or products.\n",
    "   - ORG: Any type of organization, including companies or governmental bodies.\n",
    "   - PER: Names of individuals.\n",
    "3. Output a dictionary with nouns from the sentence as keys and their corresponding tags as values. Nouns that are not named entities should be tagged as \"None\".\n",
    "\n",
    "Example 1:\n",
    "Input: \"Tim Cook visited Apple headquarters.\"\n",
    "Reasoning:\n",
    "- \"Tim Cook\" is identified as a person's name.\n",
    "- \"Apple\" refers to an organization, specifically a company.\n",
    "- \"headquarters\" although a noun, does not represent a named entity in this context.\n",
    "Output: {'Tim Cook': 'PER', 'Apple': 'ORG', 'headquarters': 'None'}\n",
    "\n",
    "Example 2:\n",
    "Input: \"The Eiffel Tower is in Paris.\"\n",
    "Reasoning:\n",
    "- \"Eiffel Tower\" is a location, a famous landmark.\n",
    "- \"Paris\" is also a location, a city.\n",
    "Output: {'Eiffel Tower': 'LOC', 'Paris': 'LOC'}\n",
    "\n",
    "Instructions for Use:\n",
    "1. Input a sentence into the model.\n",
    "2. Follow the reasoning steps to identify and classify each noun that is a potential named entity.\n",
    "3. Construct the output dictionary based on the classifications.\n",
    "\n",
    "Ensure the model processes each noun carefully to determine the most accurate category based on the surrounding context.\n",
    "\n",
    "Input: {input}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt_3 = \"\"\"You are an AI assistant answering questions based solely on the provided context. Follow these steps:\n",
    "\n",
    "1. Analyze the context (search result snippets) and determine if it contains complete information to answer the question. Explain your reasoning in 30 words or less.\n",
    "\n",
    "2. If the context is sufficient:\n",
    "   - Provide a clear, concise answer based strictly on the context.\n",
    "   - End with \"Sufficient: Yes.\"\n",
    "\n",
    "3. If the context is insufficient:\n",
    "   - Briefly explain why you cannot answer the question.\n",
    "   - End with \"Sufficient: No.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1580,
     "status": "ok",
     "timestamp": 1722374872732,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "PthIqww7XbuZ",
    "outputId": "f4ef3f69-a4bd-4712-f534-85e7f2a352bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple announced the M2 chip in June 2022, and it debuted in two new laptops: the 13-inch MacBook Air and MacBook Pro, which launched in the Summer of 2022. However, the exact release date of the M2 chip is not specified in the context. Sufficient: No.\n",
      "0.50 Seconds\n"
     ]
    }
   ],
   "source": [
    "num_search_docs = 10\n",
    "search = GoogleSerperAPIWrapper(k=num_search_docs)\n",
    "\n",
    "def extract_snippets(data):\n",
    "    snippets = []\n",
    "    if 'organic' in data:\n",
    "        for result in data['organic']:\n",
    "            if 'snippet' in result:\n",
    "                snippets.append(result['snippet'])\n",
    "    if 'peopleAlsoAsk' in data:\n",
    "        for item in data['peopleAlsoAsk']:\n",
    "            if 'snippet' in item:\n",
    "                snippets.append(item['snippet'])\n",
    "    return snippets\n",
    "\n",
    "def get_context(question):\n",
    "    results = search.results(question)\n",
    "    snippets = extract_snippets(results)\n",
    "    return \"\\n\\n\".join(snippets[:num_search_docs])\n",
    "\n",
    "def answer_question(question, model_choice, prompt_choice):\n",
    "    context = get_context(question)\n",
    "\n",
    "    # Select the model\n",
    "    if model_choice == \"LLaMA-3.1-8B\":\n",
    "        llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "    elif model_choice == \"Gemma2-9B\":\n",
    "        llm = ChatFireworks(model_name=\"accounts/fireworks/models/gemma2-9b-it\", temperature=0)\n",
    "\n",
    "    # Select the prompt\n",
    "    if prompt_choice == \"Prompt 1\":\n",
    "        system_template = prompt_1\n",
    "    elif prompt_choice == \"Prompt 2\":\n",
    "        system_template = prompt_2\n",
    "    elif prompt_choice == \"Prompt 3\":\n",
    "        system_template = prompt_3\n",
    "    else:\n",
    "        raise ValueError(\"Invalid prompt choice. Choose 'Prompt 1', 'Prompt 2', or 'Prompt 3'.\")\n",
    "\n",
    "    prompt = PromptTemplate.from_template(system_template)\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    input_dict = {\"context\": context, \"question\": question}\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = chain.invoke(input_dict)\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    return {\n",
    "        \"model\": model_choice,\n",
    "        \"prompt\": prompt_choice,\n",
    "        \"context\": context,\n",
    "        \"response\": response,\n",
    "        \"response_time\": response_time\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "question = \"When was Apple M2 released?\"\n",
    "out = answer_question(question, \"LLaMA-3.1-8B\", \"Prompt 3\")\n",
    "\n",
    "print(f\"{out['response']}\")\n",
    "print(f\"{out['response_time']:.2f} Seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2027,
     "status": "ok",
     "timestamp": 1722374877644,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "35kePFyjvoVb",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8e0d3dcf-9cc6-43f8-bb0f-0f89ea78f389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://23e1508193853fbc5a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://23e1508193853fbc5a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_output(result):\n",
    "    return f\"\"\"\n",
    "### Model: {result['model']}\n",
    "### Prompt: {result['prompt']}\n",
    "\n",
    "## Question:\n",
    "{result['question']}\n",
    "\n",
    "## Context:\n",
    "{result['context']}\n",
    "\n",
    "## Answer:\n",
    "{result['response']}\n",
    "\n",
    "**Response Time:** {result['response_time']}\n",
    "    \"\"\"\n",
    "\n",
    "def answer_question_wrapper(question, model_choice, prompt_choice):\n",
    "    result = answer_question(question, model_choice, prompt_choice)\n",
    "    result['question'] = question  # Add the question to the result dictionary\n",
    "    return format_output(result)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=answer_question_wrapper,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Question\"),\n",
    "        gr.Radio([\"LLaMA-3.1-8B\", \"Gemma2-9B\"], label=\"Model Choice\", value=\"LLaMA-3.1-8B\"),\n",
    "        gr.Radio([\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"], label=\"Prompt Choice\", value=\"Prompt 1\")\n",
    "    ],\n",
    "    outputs=gr.Markdown(),\n",
    "    title=\"Question Answering Model\",\n",
    "    description=\"Enter a question, choose a model and a prompt, and the system will provide an answer based on web search, along with the context and response time.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 19367,
     "status": "ok",
     "timestamp": 1722374932174,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "wNyam-OQ88TB",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0142940d-8a0f-4e42-9377-0742cbfa0935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 100 questions:\n",
      "1. What is the average airspeed velocity of an unladen swallow?\n",
      "2. Which ancient civilization built the first known sundial?\n",
      "3. What is the chemical composition of the pigment Tyrian purple?\n",
      "4. Who is the author of the first computer bug?\n",
      "5. What is the name of the largest living organism in the world?\n",
      "6. In what year did the first human settle in North America?\n",
      "7. What is the name of the algorithm used in the first GPS system?\n",
      "8. Who painted the ceiling of the Sistine Chapel?\n",
      "9. What is the deepest part of the ocean?\n",
      "10. What is the name of the first computer virus?\n",
      "11. Who is the founder of the field of psychoanalysis?\n",
      "12. What is the name of the largest star known to science?\n",
      "13. What is the chemical formula for the compound responsible for the smell of skunk spray?\n",
      "14. Who is the author of the book \"The Origin of Species\"?\n",
      "15. What is the name of the first successful polio vaccine?\n",
      "16. In what year did the first human walk on the moon?\n",
      "17. What is the name of the largest known prime number?\n",
      "18. Who is the inventor of the first successful light bulb?\n",
      "19. What is the name of the famous prehistoric monument in England?\n",
      "20. What is the chemical composition of the human nose's olfactory receptors?\n",
      "21. Who is the author of the famous poem \"The Road Not Taken\"?\n",
      "22. What is the name of the largest living species of lizard?\n",
      "23. In what year did the first commercial jet airliner take flight?\n",
      "24. What is the name of the first computer programming language?\n",
      "25. Who is the founder of the field of anthropology?\n",
      "26. What is the name of the largest known black hole?\n",
      "27. What is the chemical formula for the compound responsible for the smell of freshly cut grass?\n",
      "28. Who is the author of the book \"The Interpretation of Dreams\"?\n",
      "29. What is the name of the first successful kidney transplant?\n",
      "30. In what year did the first human settle in Australia?\n",
      "31. What is the name of the largest known crystal cave?\n",
      "32. Who is the inventor of the first successful parachute?\n",
      "33. What is the chemical composition of the human eye's lens?\n",
      "34. Who is the author of the famous play \"Romeo and Juliet\"?\n",
      "35. What is the name of the largest known species of shark?\n",
      "36. In what year did the first commercial computer go on sale?\n",
      "37. What is the name of the first computer network?\n",
      "38. Who is the founder of the field of sociology?\n",
      "39. What is the name of the largest known comet?\n",
      "40. What is the chemical formula for the compound responsible for the smell of coffee?\n",
      "41. Who is the author of the book \"The Wealth of Nations\"?\n",
      "42. What is the name of the largest known species of squid?\n",
      "43. In what year did the first human walk on the bottom of the ocean?\n",
      "44. What is the name of the first computer mouse?\n",
      "45. Who is the inventor of the first successful telephone?\n",
      "46. What is the chemical composition of the human brain's neurons?\n",
      "47. Who is the author of the famous poem \"The Love Song of J. Alfred Prufrock\"?\n",
      "48. What is the name of the largest known species of octopus?\n",
      "49. In what year did the first commercial email service launch?\n",
      "50. What is the name of the first computer virus that affected the internet?\n",
      "51. Who is the founder of the field of psychology?\n",
      "52. What is the name of the largest known species of starfish?\n",
      "53. What is the chemical formula for the compound responsible for the smell of vanilla?\n",
      "54. Who is the author of the book \"The Republic\"?\n",
      "55. What is the name of the largest known species of jellyfish?\n",
      "56. In what year did the first human settle in South America?\n",
      "57. What is the name of the first computer operating system?\n",
      "58. Who is the inventor of the first successful laser?\n",
      "59. What is the chemical composition of the human skin's melanin?\n",
      "60. Who is the author of the famous play \"Hamlet\"?\n",
      "61. What is the name of the largest known species of butterfly?\n",
      "62. In what year did the first commercial satellite launch?\n",
      "63. What is the name of the first computer programming language for the web?\n",
      "64. Who is the founder of the field of economics?\n",
      "65. What is the name of the largest known species of frog?\n",
      "66. What is the chemical formula for the compound responsible for the smell of garlic?\n",
      "67. Who is the author of the book \"The Picture of Dorian Gray\"?\n",
      "68. What is the name of the largest known species of snake?\n",
      "69. In what year did the first human settle in New Zealand?\n",
      "70. What is the name of the first computer graphics software?\n",
      "71. Who is the inventor of the first successful robot?\n",
      "72. What is the chemical composition of the human eye's retina?\n",
      "73. Who is the author of the famous poem \"The Waste Land\"?\n",
      "74. What is the name of the largest known species of turtle?\n",
      "75. In what year did the first commercial video game console launch?\n",
      "76. What is the name of the first computer network protocol?\n",
      "77. Who is the founder of the field of philosophy?\n",
      "78. What is the name of the largest known species of crocodile?\n",
      "79. What is the chemical formula for the compound responsible for the smell of citrus?\n",
      "80. Who is the author of the book \"The Count of Monte Cristo\"?\n",
      "81. What is the name of the largest known species of fish?\n",
      "82. In what year did the first human settle in Antarctica?\n",
      "83. What is the name of the first computer spreadsheet software?\n",
      "84. Who is the inventor of the first successful microwave oven?\n",
      "85. What is the chemical composition of the human brain's synapses?\n",
      "86. Who is the author of the famous play \"Macbeth\"?\n",
      "87. What is the name of the largest known species of spider?\n",
      "88. In what year did the first commercial mobile phone launch?\n",
      "89. What is the name of the first computer graphics hardware?\n",
      "90. Who is the founder of the field of biology?\n",
      "91. What is the name of the largest known species of ant?\n",
      "92. What is the chemical formula for the compound responsible for the smell of honey?\n",
      "93. Who is the author of the book \"The Adventures of Huckleberry Finn\"?\n",
      "94. What is the name of the largest known species of bat?\n",
      "95. In what year did the first human settle in Hawaii?\n",
      "96. What is the name of the first computer antivirus software?\n",
      "97. Who is the inventor of the first successful solar panel?\n",
      "98. What is the chemical composition of the human skin's collagen?\n",
      "99. Who is the author of the famous poem \"The Raven\"?\n",
      "100. What is the name of the largest known species of whale?\n"
     ]
    }
   ],
   "source": [
    "# LLM for generating questions\n",
    "llm_generator = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0.6)\n",
    "\n",
    "# Question generation prompt\n",
    "question_gen_template = \"\"\"Generate exactly {num_questions} diverse and challenging questions that would require complex web searches to answer. The questions should:\n",
    "\n",
    "1. Cover a wide range of topics (e.g., science, history, current events, technology, arts, code)\n",
    "2. Include some questions that are easy to search and find solutions for\n",
    "3. Avoid long questions\n",
    "4. Include some easy factual questions in the list\n",
    "5. Ensure there is only one question per query. Query should NOT be multiple questions\n",
    "\n",
    "Please provide the questions as a numbered list, starting from 1 and ending at {num_questions}.\n",
    "\n",
    "Generated Questions:\"\"\"\n",
    "\n",
    "question_gen_prompt = PromptTemplate.from_template(question_gen_template)\n",
    "\n",
    "def generate_questions(num_questions, max_attempts=3):\n",
    "    for attempt in range(max_attempts):\n",
    "        question_gen_chain = question_gen_prompt | llm_generator | StrOutputParser()\n",
    "        questions_text = question_gen_chain.invoke({\"num_questions\": num_questions})\n",
    "\n",
    "        questions = []\n",
    "        for line in questions_text.split('\\n'):\n",
    "            match = re.match(r'^\\s*\\d+\\.\\s*(.+)$', line)\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                questions.append(question)\n",
    "\n",
    "        if len(questions) == num_questions:\n",
    "            return questions\n",
    "\n",
    "        print(f\"Attempt {attempt + 1}: Generated {len(questions)} questions instead of {num_questions}. Retrying...\")\n",
    "\n",
    "    raise ValueError(f\"Failed to generate exactly {num_questions} questions after {max_attempts} attempts.\")\n",
    "\n",
    "# Generate questions\n",
    "num_questions = 100\n",
    "try:\n",
    "    evaluation_questions = generate_questions(num_questions)\n",
    "    print(f\"Successfully generated {len(evaluation_questions)} questions:\")\n",
    "    for i, question in enumerate(evaluation_questions, 1):\n",
    "        print(f\"{i}. {question}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297866,
     "status": "ok",
     "timestamp": 1722365045347,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "2CJwp5wFP4Hs",
    "outputId": "61ca5d61-e867-477b-a170-4761fb78a54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 100\n",
      "Valid evaluations: 94\n",
      "Correct answers: 87\n",
      "Answer Accuracy: 92.55% (of 94 valid evaluations)\n",
      "Correct sufficiency assessments: 90\n",
      "Sufficiency Assessment Accuracy: 95.74% (of 94 valid evaluations)\n",
      "Averge answer time of the model is: 0.5932064151763916\n"
     ]
    }
   ],
   "source": [
    "# LLM for answering questions\n",
    "llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "\n",
    "# LLM for judging (70B model)\n",
    "judge_llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "# Chain for answering questions\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Judge prompt template\n",
    "judge_template = \"\"\"You are an expert AI evaluator tasked with assessing the performance of a smaller AI model. Your job is to evaluate the model's response based on the given context and question. Focus on two main aspects:\n",
    "\n",
    "1. Answer Correctness: Determine if the model's answer is correct and fully addresses the question based solely on the provided context.\n",
    "2. Sufficiency Assessment: Evaluate if the model's \"Sufficient: Yes/No\" conclusion is correct.\n",
    "\n",
    "Guidelines:\n",
    "- The model should only answer when it can COMPLETELY address the question using the context. It is ok if the model mentions it can not answer based on context.\n",
    "- Partial answers should be considered incorrect.\n",
    "- The model should not use any external knowledge not present in the context.\n",
    "- \"Sufficient: Yes\" should only be used when the context contains ALL necessary information to fully answer the question.\n",
    "- There should be always either \"Sufficient: Yes.\" or \"Sufficient: No.\" at the end.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Model's Response:\n",
    "{response}\n",
    "\n",
    "Provide your evaluation in the following JSON format:\n",
    "{{\n",
    "  \"answer_correctness\": \"Correct\" or \"Incorrect\",\n",
    "  \"sufficiency_assessment\": \"Correct\" or \"Incorrect\",\n",
    "  \"explanation\": \"Brief explanation of your evaluation\"\n",
    "}}\n",
    "\n",
    "Your Evaluation:\"\"\"\n",
    "\n",
    "judge_prompt = PromptTemplate.from_template(judge_template)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_response(context, question, response):\n",
    "    judge_chain = judge_prompt | judge_llm | StrOutputParser()\n",
    "    evaluation = judge_chain.invoke({\"context\": context, \"question\": question, \"response\": response})\n",
    "\n",
    "    try:\n",
    "        # Try to parse the JSON output\n",
    "        eval_dict = json.loads(evaluation)\n",
    "\n",
    "        # Ensure all required keys are present\n",
    "        required_keys = [\"answer_correctness\", \"sufficiency_assessment\", \"explanation\"]\n",
    "        if all(key in eval_dict for key in required_keys):\n",
    "            return eval_dict\n",
    "        else:\n",
    "            raise ValueError(\"Missing required keys in evaluation output\")\n",
    "\n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        # If JSON parsing fails or required keys are missing, return an error dict\n",
    "        return {\n",
    "            \"answer_correctness\": \"Error\",\n",
    "            \"sufficiency_assessment\": \"Error\",\n",
    "            \"explanation\": f\"Failed to parse evaluation: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Run evaluation\n",
    "results = []\n",
    "total_answer_time = 0\n",
    "total_judge_time = 0\n",
    "\n",
    "for question in evaluation_questions:\n",
    "    context = get_context(question)\n",
    "\n",
    "    # Time the smaller model (answering)\n",
    "    start_answer = time.time()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    end_answer = time.time()\n",
    "    answer_time = end_answer - start_answer\n",
    "    total_answer_time += answer_time\n",
    "\n",
    "    # Time the larger model (judging)\n",
    "    start_judge = time.time()\n",
    "    evaluation = evaluate_response(context, question, answer)\n",
    "    end_judge = time.time()\n",
    "    judge_time = end_judge - start_judge\n",
    "    total_judge_time += judge_time\n",
    "\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"context\": context,\n",
    "        \"response\": answer,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"answer_time\": answer_time,\n",
    "        \"judge_time\": judge_time\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"evaluation_results_1.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "def parse_evaluation(evaluation):\n",
    "    lines = evaluation.strip().split('\\n')\n",
    "    answer_correctness = lines[0].split(': ')[1]\n",
    "    sufficiency_assessment = lines[1].split(': ')[1]\n",
    "    return answer_correctness, sufficiency_assessment\n",
    "\n",
    "correct_answers = 0\n",
    "correct_sufficiency = 0\n",
    "total_questions = len(evaluation_questions)\n",
    "valid_evaluations = 0\n",
    "\n",
    "for result in results:\n",
    "    evaluation = result[\"evaluation\"]\n",
    "    if evaluation[\"answer_correctness\"] != \"Error\":\n",
    "        valid_evaluations += 1\n",
    "        if evaluation[\"answer_correctness\"] == \"Correct\":\n",
    "            correct_answers += 1\n",
    "        if evaluation[\"sufficiency_assessment\"] == \"Correct\":\n",
    "            correct_sufficiency += 1\n",
    "\n",
    "print(f\"Total questions: {total_questions}\")\n",
    "print(f\"Valid evaluations: {valid_evaluations}\")\n",
    "print(f\"Correct answers: {correct_answers}\")\n",
    "print(f\"Answer Accuracy: {correct_answers / valid_evaluations * 100:.2f}% (of {valid_evaluations} valid evaluations)\")\n",
    "print(f\"Correct sufficiency assessments: {correct_sufficiency}\")\n",
    "print(f\"Sufficiency Assessment Accuracy: {correct_sufficiency / valid_evaluations * 100:.2f}% (of {valid_evaluations} valid evaluations)\")\n",
    "print(f\"Averge answer time of the model is: {total_answer_time/len(evaluation_questions):.2f} second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 544368,
     "status": "ok",
     "timestamp": 1722365780659,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "OtsopaL0ulWN",
    "outputId": "dc48f721-8216-4273-e70c-8e4cf8bb9b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions: 100\n",
      "Valid evaluations: 100\n",
      "Correct answers: 95\n",
      "Answer Accuracy: 95.00% (of 100 valid evaluations)\n",
      "Correct sufficiency assessments: 99\n",
      "Sufficiency Assessment Accuracy: 99.00% (of 100 valid evaluations)\n",
      "Averge answer time of the model is: 0.6739742994308472\n"
     ]
    }
   ],
   "source": [
    "# LLM for answering questions\n",
    "llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "\n",
    "# LLM for judging (405B model)\n",
    "judge_llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0)\n",
    "\n",
    "# Chain for answering questions\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "judge_prompt = PromptTemplate.from_template(judge_template)\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "results = []\n",
    "total_answer_time = 0\n",
    "total_judge_time = 0\n",
    "\n",
    "for question in evaluation_questions:\n",
    "    context = get_context(question)\n",
    "\n",
    "    # Time the smaller model (answering)\n",
    "    start_answer = time.time()\n",
    "    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "    end_answer = time.time()\n",
    "    answer_time = end_answer - start_answer\n",
    "    total_answer_time += answer_time\n",
    "\n",
    "    # Time the larger model (judging)\n",
    "    start_judge = time.time()\n",
    "    evaluation = evaluate_response(context, question, answer)\n",
    "    end_judge = time.time()\n",
    "    judge_time = end_judge - start_judge\n",
    "    total_judge_time += judge_time\n",
    "\n",
    "    result = {\n",
    "        \"question\": question,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"answer_time\": answer_time,\n",
    "        \"judge_time\": judge_time\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open(\"evaluation_results_2.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Print summary\n",
    "def parse_evaluation(evaluation):\n",
    "    lines = evaluation.strip().split('\\n')\n",
    "    answer_correctness = lines[0].split(': ')[1]\n",
    "    sufficiency_assessment = lines[1].split(': ')[1]\n",
    "    return answer_correctness, sufficiency_assessment\n",
    "\n",
    "correct_answers = 0\n",
    "correct_sufficiency = 0\n",
    "total_questions = len(evaluation_questions)\n",
    "valid_evaluations = 0\n",
    "\n",
    "for result in results:\n",
    "    evaluation = result[\"evaluation\"]\n",
    "    if evaluation[\"answer_correctness\"] != \"Error\":\n",
    "        valid_evaluations += 1\n",
    "        if evaluation[\"answer_correctness\"] == \"Correct\":\n",
    "            correct_answers += 1\n",
    "        if evaluation[\"sufficiency_assessment\"] == \"Correct\":\n",
    "            correct_sufficiency += 1\n",
    "\n",
    "print(f\"Total questions: {total_questions}\")\n",
    "print(f\"Valid evaluations: {valid_evaluations}\")\n",
    "print(f\"Correct answers: {correct_answers}\")\n",
    "print(f\"Answer Accuracy: {correct_answers / valid_evaluations * 100:.2f}% (of {valid_evaluations} valid evaluations)\")\n",
    "print(f\"Correct sufficiency assessments: {correct_sufficiency}\")\n",
    "print(f\"Sufficiency Assessment Accuracy: {correct_sufficiency / valid_evaluations * 100:.2f}% (of {valid_evaluations} valid evaluations)\")\n",
    "print(f\"Averge answer time of the model is: {total_answer_time/len(evaluation_questions):.2f} second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0w5s528Q99H5"
   },
   "outputs": [],
   "source": [
    "def evaluate_models(evaluation_questions, models, prompts, judges):\n",
    "    results = []\n",
    "    response_times = defaultdict(list)\n",
    "\n",
    "    for model in models:\n",
    "        for prompt_name in prompts:\n",
    "            for judge_name in judges:\n",
    "                config_results = []\n",
    "                config_times = []\n",
    "\n",
    "                for question in evaluation_questions:\n",
    "                    context = get_context(question)\n",
    "\n",
    "                    # Select the prompt\n",
    "                    if prompt_name == \"Prompt 1\":\n",
    "                        system_template = prompt_1\n",
    "                    elif prompt_name == \"Prompt 2\":\n",
    "                        system_template = prompt_2\n",
    "                    elif prompt_name == \"Prompt 3\":\n",
    "                        system_template = prompt_3\n",
    "                    else:\n",
    "                        raise ValueError(f\"Invalid prompt choice: {prompt_name}\")\n",
    "\n",
    "                    prompt = PromptTemplate.from_template(system_template)\n",
    "\n",
    "                    # Answer the question\n",
    "                    llm = ChatFireworks(model_name=model_name_map[model], temperature=0)\n",
    "                    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    answer = chain.invoke({\"context\": context, \"question\": question})\n",
    "                    end_time = time.time()\n",
    "                    response_time = end_time - start_time\n",
    "\n",
    "                    config_times.append(response_time)\n",
    "\n",
    "                    judge = ChatFireworks(model_name=model_name_map[judge_name], temperature=0)\n",
    "\n",
    "                    judge_prompt = PromptTemplate.from_template(\"\"\"You are an expert AI evaluator tasked with assessing the performance of a smaller AI model. Your job is to evaluate the model's response based on the given context and question. Focus on two main aspects:\n",
    "\n",
    "                    1. Answer Correctness: Determine if the model's answer is correct and fully addresses the question based solely on the provided context.\n",
    "                    2. Sufficiency Assessment: Evaluate if the model's \"Sufficient: Yes/No\" conclusion is correct.\n",
    "\n",
    "                    Guidelines:\n",
    "                    - The model should only answer when it can COMPLETELY address the question using the context. It is ok if the model mentions it can not answer based on context.\n",
    "                    - Partial answers should be considered incorrect.\n",
    "                    - The model should not use any external knowledge not present in the context.\n",
    "                    - \"Sufficient: Yes\" should only be used when the context contains ALL necessary information to fully answer the question.\n",
    "                    - There should be always either \"Sufficient: Yes.\" or \"Sufficient: No.\" at the end.\n",
    "\n",
    "                    Context:\n",
    "                    {context}\n",
    "\n",
    "                    Question: {question}\n",
    "\n",
    "                    Model's Response:\n",
    "                    {response}\n",
    "\n",
    "                    Provide your evaluation in the following JSON format:\n",
    "                    {{\n",
    "                      \"answer_correctness\": \"Correct\" or \"Incorrect\",\n",
    "                      \"sufficiency_assessment\": \"Correct\" or \"Incorrect\",\n",
    "                      \"explanation\": \"Brief explanation of your evaluation\"\n",
    "                    }}\n",
    "\n",
    "                    Your Evaluation:\"\"\")\n",
    "\n",
    "                    judge_chain = judge_prompt | judge | StrOutputParser()\n",
    "\n",
    "                    evaluation = judge_chain.invoke({\n",
    "                        \"question\": question,\n",
    "                        \"context\": context,\n",
    "                        \"response\": answer\n",
    "                    })\n",
    "\n",
    "                    result = {\n",
    "                        \"question\": question,\n",
    "                        \"model\": model,\n",
    "                        \"prompt\": prompt_name,\n",
    "                        \"judge\": judge_name,\n",
    "                        \"response\": answer,\n",
    "                        \"response_time\": response_time,\n",
    "                        \"evaluation\": evaluation\n",
    "                    }\n",
    "\n",
    "                    config_results.append(result)\n",
    "\n",
    "                # Calculate metrics for this configuration\n",
    "                avg_response_time = sum(config_times) / len(config_times)\n",
    "                correct_answers = sum(1 for r in config_results if '\"answer_correctness\": \"Correct\"' in r['evaluation'])\n",
    "                correct_sufficiency = sum(1 for r in config_results if '\"sufficiency_assessment\": \"Correct\"' in r['evaluation'])\n",
    "\n",
    "                print(f\"\\nMetrics for configuration:\")\n",
    "                print(f\"Model: {model}\")\n",
    "                print(f\"Prompt: {prompt_name}\")\n",
    "                print(f\"Judge: {judge_name}\")\n",
    "                print(f\"Average Response Time: {avg_response_time:.2f} seconds\")\n",
    "                print(f\"Correct Answers: {correct_answers}/{len(evaluation_questions)}\")\n",
    "                print(f\"Correct Sufficiency Assessments: {correct_sufficiency}/{len(evaluation_questions)}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "                results.extend(config_results)\n",
    "                response_times[(model, prompt_name)].extend(config_times)\n",
    "\n",
    "    # Calculate and print overall average response times\n",
    "    print(\"\\nOverall Average Response Times:\")\n",
    "    for (model, prompt), times in response_times.items():\n",
    "        avg_time = sum(times) / len(times)\n",
    "        print(f\"Model: {model}, Prompt: {prompt} - Average Time: {avg_time:.2f} seconds\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6829180,
     "status": "ok",
     "timestamp": 1722381779051,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "IFxsfGWhYPYZ",
    "outputId": "c3acf779-8504-4507-d2bc-0ec6f6b0e994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 1\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.71 seconds\n",
      "Correct Answers: 96/100\n",
      "Correct Sufficiency Assessments: 96/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 1\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 0.85 seconds\n",
      "Correct Answers: 96/100\n",
      "Correct Sufficiency Assessments: 98/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'502 Server Error: Bad Gateway for url: https://api.smith.langchain.com/runs/batch\\', \\'\\\\n<html><head>\\\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\\\n<title>502 Server Error</title>\\\\n</head>\\\\n<body text=#000000 bgcolor=#ffffff>\\\\n<h1>Error: Server Error</h1>\\\\n<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\\\\n<h2></h2>\\\\n</body></html>\\\\n\\')')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 2\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.63 seconds\n",
      "Correct Answers: 95/100\n",
      "Correct Sufficiency Assessments: 96/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 2\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 0.56 seconds\n",
      "Correct Answers: 94/100\n",
      "Correct Sufficiency Assessments: 98/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 3\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.42 seconds\n",
      "Correct Answers: 80/100\n",
      "Correct Sufficiency Assessments: 89/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: LLaMA-3.1-8B\n",
      "Prompt: Prompt 3\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 0.40 seconds\n",
      "Correct Answers: 86/100\n",
      "Correct Sufficiency Assessments: 93/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 1\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.99 seconds\n",
      "Correct Answers: 86/100\n",
      "Correct Sufficiency Assessments: 86/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 1\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 1.04 seconds\n",
      "Correct Answers: 85/100\n",
      "Correct Sufficiency Assessments: 86/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 2\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.83 seconds\n",
      "Correct Answers: 91/100\n",
      "Correct Sufficiency Assessments: 92/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 2\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 0.82 seconds\n",
      "Correct Answers: 89/100\n",
      "Correct Sufficiency Assessments: 90/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 3\n",
      "Judge: LLaMA-70B\n",
      "Average Response Time: 0.61 seconds\n",
      "Correct Answers: 73/100\n",
      "Correct Sufficiency Assessments: 79/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Metrics for configuration:\n",
      "Model: Gemma2-9B\n",
      "Prompt: Prompt 3\n",
      "Judge: LLaMA-405B\n",
      "Average Response Time: 0.62 seconds\n",
      "Correct Answers: 75/100\n",
      "Correct Sufficiency Assessments: 83/100\n",
      "--------------------------------------------------\n",
      "\n",
      "Overall Average Response Times:\n",
      "Model: LLaMA-3.1-8B, Prompt: Prompt 1 - Average Time: 0.78 seconds\n",
      "Model: LLaMA-3.1-8B, Prompt: Prompt 2 - Average Time: 0.60 seconds\n",
      "Model: LLaMA-3.1-8B, Prompt: Prompt 3 - Average Time: 0.41 seconds\n",
      "Model: Gemma2-9B, Prompt: Prompt 1 - Average Time: 1.02 seconds\n",
      "Model: Gemma2-9B, Prompt: Prompt 2 - Average Time: 0.82 seconds\n",
      "Model: Gemma2-9B, Prompt: Prompt 3 - Average Time: 0.61 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping for model names\n",
    "model_name_map = {\n",
    "    \"LLaMA-3.1-8B\": \"accounts/fireworks/models/llama-v3p1-8b-instruct\",\n",
    "    \"Gemma2-9B\": \"accounts/fireworks/models/gemma2-9b-it\",\n",
    "    \"LLaMA-70B\": \"accounts/fireworks/models/llama-v3p1-70b-instruct\",\n",
    "    \"LLaMA-405B\": \"accounts/fireworks/models/llama-v3p1-405b-instruct\"\n",
    "}\n",
    "\n",
    "models = [\"LLaMA-3.1-8B\", \"Gemma2-9B\"]\n",
    "prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\n",
    "judges = [\"LLaMA-70B\", \"LLaMA-405B\"]\n",
    "\n",
    "evaluation_results = evaluate_models(evaluation_questions, models, prompts, judges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AKt07zga7fo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO8X0wppSyPFyg+vNZ25IoP",
   "provenance": [
    {
     "file_id": "16cbyPDOSqUShgvYeiK1jbtYBkzbK15us",
     "timestamp": 1722366099137
    },
    {
     "file_id": "1rjo4NNhfvSdBMPoGROW-lvwT9oLbR-PM",
     "timestamp": 1722357676502
    },
    {
     "file_id": "1pvUJLSQMwmwe-wo6JsZHt-Kh0tHp8JeM",
     "timestamp": 1722294345788
    },
    {
     "file_id": "1xMS4sulBD56oy7VHih-bdwaD3TN2l1tH",
     "timestamp": 1722288913723
    },
    {
     "file_id": "1aRrGzH8tc8b2cibEzz3RobyA4HOVgQLj",
     "timestamp": 1722283659675
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
