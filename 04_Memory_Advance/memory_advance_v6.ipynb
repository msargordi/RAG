{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8uS3H8y-DKdu",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "! pip install langchain_community tiktoken langchainhub chromadb langchain langchain_fireworks google-search-results requests gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 566,
     "status": "ok",
     "timestamp": 1722640257811,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "pO8Bp6MTDVLf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['FIREWORKS_API_KEY'] = 'API_KEY'\n",
    "os.environ[\"SERPER_API_KEY\"] = 'API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "3JveywQqF-dL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "Running on public URL: https://c852b9f7e369881ca9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c852b9f7e369881ca9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import json\n",
    "import gradio as gr\n",
    "\n",
    "# Set up the prompt template\n",
    "system_message = \"\"\"You are a chatbot having a conversation with a human. You might have multiple sessions separated by [NEW CHAT SESSION]. You can use previous chats if necessary.\n",
    "\n",
    "Your primary task is to engage in a natural conversation with the user while maintaining context:\n",
    "\n",
    "1. Maintain a working context of important information about the user (e.g., favorite color, birthday, name) across all chat sessions.\n",
    "2. Update this working context based ONLY on explicit information provided by the user.\n",
    "3. Always use the working context to inform your responses and maintain consistency, even in new chat sessions.\n",
    "\n",
    "For EVERY user message, you MUST:\n",
    "1. Provide a natural, conversational response to the user's input.\n",
    "2. After your response, IF there is new information to add to the working context, include a context update in this format:\n",
    "   [CONTEXT_UPDATE]{\"key1\": \"value1\", \"key2\": \"value2\"}[/CONTEXT_UPDATE]\n",
    "\n",
    "Important guidelines:\n",
    "- Always respond to the user's query or comment first.\n",
    "- Only include a [CONTEXT_UPDATE] section if there is new information to add.\n",
    "- The [CONTEXT_UPDATE] section should contain the full updated context, not just new information.\n",
    "- Never mention the context updates or working context explicitly in your responses to the user.\n",
    "- When a new chat session starts, use the existing working context to inform your responses, but don't repeat all the information unless it's relevant to the current conversation.\n",
    "\n",
    "Remember, your main goal is to have a natural conversation. The context management is a background task to help you maintain consistency across all chat sessions.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=system_message),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatFireworks(model_name=\"accounts/fireworks/models/llama-v3p1-405b-instruct\", temperature=0.3)\n",
    "\n",
    "# Initialize memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Initialize working context\n",
    "working_context = {}\n",
    "\n",
    "# Create the conversation chain\n",
    "chat_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "def extract_context_update(response):\n",
    "    start_tag = \"[CONTEXT_UPDATE]\"\n",
    "    end_tag = \"[/CONTEXT_UPDATE]\"\n",
    "    start = response.find(start_tag)\n",
    "    end = response.find(end_tag)\n",
    "    if start != -1 and end != -1:\n",
    "        context_json = response[start + len(start_tag):end].strip()\n",
    "        try:\n",
    "            return json.loads(context_json)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing context JSON: {context_json}\")\n",
    "    return None\n",
    "\n",
    "def format_working_context():\n",
    "    if not working_context:\n",
    "        return \"No working context available yet.\"\n",
    "    context_str = \"Working Context:\\n\"\n",
    "    for key, value in working_context.items():\n",
    "        context_str += f\"- {key}: {value}\\n\"\n",
    "    return context_str\n",
    "\n",
    "# Function to estimate token count\n",
    "def estimate_token_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Function to summarize chat history\n",
    "def summarize_chat_history(history):\n",
    "    summary_prompt = f\"\"\"Summarize the following conversation, keeping all important details and context:\n",
    "\n",
    "{history}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    summary = llm.predict(summary_prompt)\n",
    "    return summary\n",
    "\n",
    "def calculate_total_tokens():\n",
    "    system_tokens = estimate_token_count(system_message)\n",
    "    context_tokens = estimate_token_count(json.dumps(working_context))\n",
    "    history_tokens = sum(estimate_token_count(msg.content) for msg in memory.chat_memory.messages)\n",
    "    return system_tokens + context_tokens + history_tokens\n",
    "\n",
    "def chat(message, history, max_tokens):\n",
    "    full_input = f\"Human: {message}\\n\\nCurrent working context: {json.dumps(working_context)}\"\n",
    "\n",
    "    # Check if history needs summarization\n",
    "    total_tokens = calculate_total_tokens()\n",
    "    if total_tokens > max_tokens:\n",
    "        summary = summarize_chat_history(str(memory.chat_memory.messages))\n",
    "        memory.clear()\n",
    "        memory.chat_memory.add_message(AIMessage(content=f\"Previous conversation summary: {summary}\"))\n",
    "\n",
    "    full_response = chat_chain.predict(input=full_input)\n",
    "\n",
    "    # Extract and update the working context\n",
    "    context_update = extract_context_update(full_response)\n",
    "    if context_update:\n",
    "        working_context.update(context_update)\n",
    "\n",
    "    # Extract the response part\n",
    "    response_parts = full_response.split(\"[CONTEXT_UPDATE]\")\n",
    "    response = response_parts[0].strip()\n",
    "\n",
    "    if len(response_parts) > 1:\n",
    "        response += ' ' + response_parts[-1].split(\"[/CONTEXT_UPDATE]\")[-1].strip()\n",
    "\n",
    "    history.append((message, response))\n",
    "\n",
    "    # Calculate and return the updated token count and context\n",
    "    updated_token_count = calculate_total_tokens()\n",
    "    updated_context = format_working_context()\n",
    "    return \"\", history, updated_token_count, updated_context\n",
    "\n",
    "def new_chat():\n",
    "    # chat_chain.predict(input=\"[NEW CHAT SESSION] Please acknowledge the start of a new chat session while maintaining the existing working context.\")\n",
    "    memory.clear()\n",
    "    initial_token_count = calculate_total_tokens()\n",
    "    initial_context = format_working_context()\n",
    "    return None, [], 1000, initial_token_count, initial_context\n",
    "\n",
    "def clear_history():\n",
    "    global working_context\n",
    "    working_context = {}\n",
    "    memory.clear()\n",
    "    initial_token_count = calculate_total_tokens()\n",
    "    return None, [], 1000, initial_token_count, \"Working context and chat history cleared.\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    max_tokens = gr.Slider(minimum=500, maximum=100000, value=1000, step=100, label=\"Max Tokens Before Summarization in chat history\")\n",
    "    initial_count = calculate_total_tokens()\n",
    "    token_count = gr.Number(label=\"Current Token Count\", value=initial_count, interactive=False)\n",
    "\n",
    "    with gr.Row():  # This creates a horizontal layout for the buttons\n",
    "        clear = gr.Button(\"New Chat\")\n",
    "        clear_history_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "    context_view = gr.Textbox(label=\"Current Working Context\", interactive=False, value=format_working_context())\n",
    "\n",
    "    msg.submit(chat, [msg, chatbot, max_tokens], [msg, chatbot, token_count, context_view])\n",
    "    clear.click(new_chat, outputs=[msg, chatbot, max_tokens, token_count, context_view])\n",
    "    clear_history_btn.click(clear_history, outputs=[msg, chatbot, max_tokens, token_count, context_view])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KD2GBjYUJkiW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMx8a22A6SbJxxV4JHURVIb",
   "name": "",
   "provenance": [
    {
     "file_id": "1G4HCToN89UMnauTsx-P3PzvPT4v4QZAe",
     "timestamp": 1722552737806
    },
    {
     "file_id": "1-zlz4YIBmTuxn11tUCuyv_xjmV01e-OJ",
     "timestamp": 1722540943483
    },
    {
     "file_id": "1P-UiBQiDIWwHLo4CTwWr2rMPLzQkCXXf",
     "timestamp": 1722538602495
    },
    {
     "file_id": "1qVr6RC4iHWiAwVyqWGGqdLcJNPGIELR-",
     "timestamp": 1722535257600
    },
    {
     "file_id": "1oPZ0lYk9DL1sm-sk4RtGlDf60GMW9_AJ",
     "timestamp": 1722531900585
    },
    {
     "file_id": "1syTT0eVcED4WwnftnpeH-y0vfdvfGVRB",
     "timestamp": 1722459069566
    },
    {
     "file_id": "16cbyPDOSqUShgvYeiK1jbtYBkzbK15us",
     "timestamp": 1722366099137
    },
    {
     "file_id": "1rjo4NNhfvSdBMPoGROW-lvwT9oLbR-PM",
     "timestamp": 1722357676502
    },
    {
     "file_id": "1pvUJLSQMwmwe-wo6JsZHt-Kh0tHp8JeM",
     "timestamp": 1722294345788
    },
    {
     "file_id": "1xMS4sulBD56oy7VHih-bdwaD3TN2l1tH",
     "timestamp": 1722288913723
    },
    {
     "file_id": "1aRrGzH8tc8b2cibEzz3RobyA4HOVgQLj",
     "timestamp": 1722283659675
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
