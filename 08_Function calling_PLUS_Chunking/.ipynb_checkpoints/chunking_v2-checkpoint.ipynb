{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1723068198923,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "9RtawaERNORl"
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_fireworks langchain_community beautifulsoup4 google-search-results chromadb langchainhub sentence-transformers langchain-chroma gradio aiolimiter lxml faiss-cpu flashrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 18907,
     "status": "ok",
     "timestamp": 1723068219318,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "s5FdjAFiNKGx"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_fireworks import FireworksEmbeddings, ChatFireworks\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "from typing import List, Tuple, Any\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import faiss\n",
    "import httpx\n",
    "from urllib.parse import urlparse\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from flashrank import Ranker, RerankRequest\n",
    "import math\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.output_parsers import RegexParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1723068219319,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "9aK_SqnCNZEW",
    "outputId": "d76df4ab-e43d-415e-983b-e9032a07e12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up API clients\n",
    "os.environ['FIREWORKS_API_KEY'] = 'API_KEY'\n",
    "os.environ[\"SERPER_API_KEY\"] = 'API_KEY'\n",
    "\n",
    "\n",
    "# Download NLTK data for sentence tokenization\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "executionInfo": {
     "elapsed": 592685,
     "status": "ok",
     "timestamp": 1723068184809,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "zQ-CXE4vVaVE",
    "outputId": "18347451-e24b-4617-f634-ace2f67791e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "Running on public URL: https://50829bda5c883f5291.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://50829bda5c883f5291.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-16-b2bdc174d304>\", line 211, in process_query\n",
      "    all_texts = await asyncio.gather(*[search_and_scrape(eq, num_urls) for eq in expanded_queries])\n",
      "  File \"<ipython-input-16-b2bdc174d304>\", line 90, in search_and_scrape\n",
      "    search_results = search.results(query)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_community/utilities/google_serper.py\", line 62, in results\n",
      "    return self._google_serper_api_results(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_community/utilities/google_serper.py\", line 164, in _google_serper_api_results\n",
      "    response.raise_for_status()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1021, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://google.serper.dev/search?q=how+to+take+care+of+my+dog%3F&gl=us&hl=en&num=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://50829bda5c883f5291.gradio.live\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "\n",
    "# Initialize components\n",
    "search = GoogleSerperAPIWrapper(k=3)\n",
    "embeddings = FireworksEmbeddings(model=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_8b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_70b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "# Initialize Flashrank Rankers\n",
    "ranker_nano = Ranker()\n",
    "\n",
    "class ChunkIndices(BaseModel):\n",
    "    indices: list[int] = Field(description=\"List of character indices where the content should be split\")\n",
    "\n",
    "    @validator('indices')\n",
    "    def check_indices(cls, indices):\n",
    "        if not all(isinstance(i, int) for i in indices):\n",
    "            raise ValueError(\"All elements must be integers\")\n",
    "        return indices\n",
    "\n",
    "\n",
    "def get_chunk_indices(content, target_chunk_size=250):\n",
    "    template = \"\"\"\n",
    "    Analyze the following content and insert '[chunk_here]' at appropriate points to split it into chunks of approximately {target_chunk_size} characters each, maintaining context and coherence. Follow these guidelines:\n",
    "    1. Keep function definitions and code blocks intact.\n",
    "    2. Don't split in the middle of a sentence or a line of code.\n",
    "    3. Try to split at logical breaks in the content.\n",
    "    4. For prose, prefer splitting at paragraph boundaries.\n",
    "    5. For code, prefer splitting at function or class boundaries.\n",
    "    6. Do not modify the original content other than inserting '[chunk_here]'.\n",
    "\n",
    "    Content length: {content_length} characters\n",
    "\n",
    "    Content:\n",
    "    {content}\n",
    "\n",
    "    Provide the content with '[chunk_here]' insertions:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"target_chunk_size\", \"content_length\", \"content\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    full_prompt = prompt.format(\n",
    "        target_chunk_size=target_chunk_size,\n",
    "        content_length=len(content),\n",
    "        content=content\n",
    "    )\n",
    "\n",
    "    response = llm_8b.invoke(full_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    chunks = response_text.split('[chunk_here]')\n",
    "    indices = [0]\n",
    "    for chunk in chunks[:-1]:\n",
    "        indices.append(indices[-1] + len(chunk))\n",
    "\n",
    "    return indices\n",
    "\n",
    "def chunk_content(content, chunk_indices):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for end in chunk_indices:\n",
    "        chunks.append(content[start:end].strip())\n",
    "        start = end\n",
    "    chunks.append(content[start:].strip())  # Add the last chunk\n",
    "    return chunks\n",
    "\n",
    "async def scrape_webpage(client, url):\n",
    "    try:\n",
    "        response = await client.get(url, timeout=3.0)\n",
    "        response.raise_for_status()\n",
    "        text = response.text\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        content = ' '.join(soup.stripped_strings)\n",
    "        return content[:5000], len(content[:5000])\n",
    "    except (httpx.RequestError, httpx.TimeoutException) as exc:\n",
    "        print(f\"An error occurred while requesting {url}: {exc}\")\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(f\"Error response {exc.response.status_code} while requesting {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return \"\", 0\n",
    "\n",
    "async def search_and_scrape(query, num_urls):\n",
    "    search_results = search.results(query)\n",
    "    scraped_urls = set()\n",
    "    full_texts = []\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=httpx.Timeout(10.0, connect=3.0)) as client:\n",
    "        tasks = []\n",
    "        if 'organic' in search_results:\n",
    "            for result in search_results['organic']:\n",
    "                url = result.get('link')\n",
    "                domain = urlparse(url).netloc if url else None\n",
    "                if url and domain not in scraped_urls and len(tasks) < num_urls:\n",
    "                    tasks.append(scrape_webpage(client, url))\n",
    "                    scraped_urls.add(domain)\n",
    "\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        for result in results:\n",
    "            if isinstance(result, tuple) and result[1] > 0:\n",
    "                full_texts.append(result[0])\n",
    "\n",
    "    return \" \".join(full_texts)\n",
    "\n",
    "def query_expansion(query, num_expansions):\n",
    "    expansion_prompt = f\"\"\"\n",
    "    Given the following search query, generate {num_expansions} additional related queries that could help find more comprehensive information on the topic. The queries should be different from each other and explore various aspects of the main query. Provide only the additional queries, numbered 1-{num_expansions}.\n",
    "\n",
    "    Main query: {query}\n",
    "\n",
    "    Additional queries:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(expansion_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    expanded_queries = [query]\n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.strip() and line[0].isdigit():\n",
    "            expanded_queries.append(line.split('. ', 1)[1].strip())\n",
    "\n",
    "    return expanded_queries[:num_expansions + 1]\n",
    "\n",
    "def generate_hypothetical_document(query):\n",
    "    hyde_prompt = f\"\"\"\n",
    "    Given the search query below, generate a hypothetical document that would be a perfect match for this query. The document should be concise, containing only 3 sentences of relevant information that directly addresses the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Document (3 sentences):\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def batch_rerank(ranker, query, documents, batch_size=32):\n",
    "    all_reranked = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        passages = [{\"id\": j, \"text\": doc.page_content} for j, doc in enumerate(batch, start=i)]\n",
    "        rerank_request = RerankRequest(query=query, passages=passages)\n",
    "        reranked_batch = ranker.rerank(rerank_request)\n",
    "        all_reranked.extend(reranked_batch)\n",
    "    return all_reranked\n",
    "\n",
    "def get_hyde_retriever(vectorstores, hyde_embedding, num_docs_to_rerank, num_docs_to_use, rerank_option):\n",
    "    def retriever(query):\n",
    "        all_docs = []\n",
    "        for vectorstore in vectorstores:\n",
    "            docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs_to_rerank)\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        unique_docs = []\n",
    "        seen_content = set()\n",
    "        for doc in all_docs:\n",
    "            content = doc.page_content\n",
    "            if content not in seen_content:\n",
    "                unique_docs.append(Document(page_content=content))\n",
    "                seen_content.add(content)\n",
    "\n",
    "        if rerank_option != \"No Rerank\":\n",
    "            if rerank_option == \"Nano Cross-Encoder\":\n",
    "                ranker = ranker_nano\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid rerank option: {rerank_option}\")\n",
    "\n",
    "            reranked_results = batch_rerank(ranker, query, unique_docs)\n",
    "            reranked_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "            reranked_docs = [Document(page_content=result[\"text\"]) for result in reranked_results[:num_docs_to_use]]\n",
    "            return reranked_docs\n",
    "        else:\n",
    "            return unique_docs[:num_docs_to_use]\n",
    "    return retriever\n",
    "\n",
    "def batch_embed_documents(documents, batch_size=128):\n",
    "    batched_embeddings = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        texts = [doc.page_content for doc in batch]\n",
    "        embeddings_batch = embeddings.embed_documents(texts)\n",
    "        batched_embeddings.extend(embeddings_batch)\n",
    "    return batched_embeddings\n",
    "\n",
    "async def process_query(query, num_expansions, num_urls, num_docs_to_rerank, num_docs_to_use, rerank_option, use_70b_model):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        hyde_start = time.time()\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        hyde_time = time.time() - hyde_start\n",
    "        print(f\"hypothetical_doc length: {len(hypothetical_doc)}\")\n",
    "        print(f\"-----HyDE generation time: {hyde_time:.2f} seconds\")\n",
    "\n",
    "        embed_start = time.time()\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "        embed_time = time.time() - embed_start\n",
    "        print(f\"-----Embedding time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        ext_start = time.time()\n",
    "        expanded_queries = query_expansion(query, num_expansions)\n",
    "        ext_time = time.time() - embed_start\n",
    "        print(f\"-----Query expansion time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        scrape_start = time.time()\n",
    "        all_texts = await asyncio.gather(*[search_and_scrape(eq, num_urls) for eq in expanded_queries])\n",
    "        scrape_time = time.time() - scrape_start\n",
    "        print(f\"-----Web scraping time: {scrape_time:.2f} seconds\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        chunk_start = time.time()\n",
    "        chunk_indices = get_chunk_indices(combined_text)\n",
    "        chunks = chunk_content(combined_text, chunk_indices)\n",
    "        chunk_time = time.time() - chunk_start\n",
    "        print(f\"-----Chunking time: {chunk_time:.2f} seconds\")\n",
    "        print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "        vectorstore_start = time.time()\n",
    "        vectorstores = []\n",
    "        for i in range(0, len(index_documents), 256):\n",
    "            batch = index_documents[i:i + 256]\n",
    "            batch_embeddings = batch_embed_documents(batch)\n",
    "            texts = [doc.page_content for doc in batch]\n",
    "            vectorstore = FAISS.from_embeddings(\n",
    "                embedding=embeddings,\n",
    "                text_embeddings=list(zip(texts, batch_embeddings))\n",
    "            )\n",
    "            vectorstores.append(vectorstore)\n",
    "\n",
    "        vectorstore_time = time.time() - vectorstore_start\n",
    "        print(f\"-----Vectorstore creation time: {vectorstore_time:.2f} seconds\")\n",
    "\n",
    "        retrieval_start = time.time()\n",
    "        retriever = get_hyde_retriever(vectorstores, hyde_embedding, num_docs_to_rerank, num_docs_to_use, rerank_option)\n",
    "        retrieved_docs = retriever(query)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        print(f\"-----Retrieval{' and reranking' if rerank_option != 'No Rerank' else ''} time: {retrieval_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"Number of retrieved{' and reranked' if rerank_option != 'No Rerank' else ''} documents: {len(retrieved_docs)}\")\n",
    "\n",
    "        context_docs = [doc.page_content for doc in retrieved_docs]\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "\n",
    "        total_processing_time = hyde_time + embed_time + scrape_time + chunk_time + vectorstore_time + retrieval_time\n",
    "        print(f\"-----Total processing time before answer generation: {total_processing_time:.2f} seconds\")\n",
    "\n",
    "        answer_start = time.time()\n",
    "        prompt_template = \"\"\"\n",
    "        Use the following context to answer the question. Before answering the question generate a reasoning step. then answer.\n",
    "        If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        chosen_llm = llm_70b if use_70b_model else llm_8b\n",
    "\n",
    "        rag_chain = prompt | chosen_llm | StrOutputParser()\n",
    "        answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "        answer_time = time.time() - answer_start\n",
    "        print(f\"-----Answer generation time: {answer_time:.2f} seconds\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*120)\n",
    "        print(\"Final Answer:\\n\", answer)\n",
    "        print(\"-\"*120)\n",
    "\n",
    "        return answer, context_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\", []\n",
    "\n",
    "def gradio_interface(query, num_expansions, num_urls, num_docs_to_rerank, num_docs_to_use, rerank_option, use_70b_model):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "    answer, context_docs = asyncio.run(process_query(query, num_expansions, num_urls, num_docs_to_rerank, num_docs_to_use, rerank_option, use_70b_model))\n",
    "\n",
    "    sys.stdout = old_stdout\n",
    "    captured_output = buffer.getvalue()\n",
    "\n",
    "    truncated_docs = [f\"Document {i+1}: {doc[:150]}...\" for i, doc in enumerate(context_docs)]\n",
    "    truncated_context = \"\\n\\n\".join(truncated_docs)\n",
    "\n",
    "    captured_output += f\"\\n\\nContext used for answer generation (first 150 characters of each document, {len(context_docs)} documents in total):\\n\" + truncated_context\n",
    "\n",
    "    return captured_output\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your query\"),\n",
    "        gr.Slider(minimum=0, maximum=3, value=1, step=1, label=\"Number of query expansions\"),\n",
    "        gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of URLs to scrape per extended query\"),\n",
    "        gr.Slider(minimum=20, maximum=100, value=80, step=1, label=\"Number of documents to rerank\"),\n",
    "        gr.Slider(minimum=10, maximum=100, value=50, step=1, label=\"Number of reranked documents to use\"),\n",
    "        gr.Radio([\"No Rerank\", \"Nano Cross-Encoder\"], label=\"Reranking Option\", value=\"Nano Cross-Encoder\"),\n",
    "        gr.Radio([\"LLaMA3.1 8B\", \"LLaMA3.1 70B\"], label=\"Question Answering Option\", value=\"LLaMA3.1 8B\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Advanced RAG Query Processing with Flashrank\",\n",
    "    description=\"Enter a query and adjust parameters to get a detailed answer based on web search and document analysis. Choose from different reranking options.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1723053974203,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "9s2izvJf-CyJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1723069319737,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "D70Qczw_D_vZ"
   },
   "outputs": [],
   "source": [
    "# Define the content (the 5000-character document we created earlier)\n",
    "content = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field at the intersection of computer science, artificial intelligence, and linguistics. It focuses on the interaction between computers and human language, enabling machines to understand, interpret, and generate human-readable text. One of the fundamental tasks in NLP is text classification, which involves categorizing text documents into predefined classes or categories.\n",
    "\n",
    "Let's explore a simple example of text classification using Python and the popular machine learning library scikit-learn. We'll build a basic sentiment analysis model that can classify movie reviews as either positive or negative.\n",
    "\n",
    "First, we need to import the necessary libraries:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "Next, let's define a small dataset of movie reviews with their corresponding sentiments:\n",
    "\n",
    "reviews = [\n",
    "    \"This movie was fantastic! I loved every minute of it.\",\n",
    "    \"Terrible acting and a boring plot. Waste of time.\",\n",
    "    \"Great performances and an engaging story. Highly recommended!\",\n",
    "    \"I fell asleep halfway through. Extremely dull.\",\n",
    "    \"A masterpiece of modern cinema. Absolutely brilliant!\",\n",
    "    \"Poorly written and predictable. Don't bother watching.\",\n",
    "    \"Visually stunning with excellent character development.\",\n",
    "    \"A complete disaster. One of the worst films I've ever seen.\",\n",
    "    \"Captivating from start to finish. A must-watch!\",\n",
    "    \"Disappointing and confusing. I expected much better.\"\n",
    "]\n",
    "\n",
    "sentiments = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
    "\n",
    "Now that we have our data, we need to preprocess it. We'll use the CountVectorizer to convert our text data into numerical features:\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "y = np.array(sentiments)\n",
    "\n",
    "Let's split our data into training and testing sets:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "Now we can train our Naive Bayes classifier:\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "With our model trained, we can make predictions on the test set and evaluate its performance:\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "\n",
    "This simple example demonstrates the basics of text classification using machine learning. However, in real-world applications, we often deal with much larger datasets and more complex models.\n",
    "\n",
    "One popular approach for more advanced NLP tasks is to use pre-trained language models like BERT (Bidirectional Encoder Representations from Transformers). BERT and its variants have revolutionized the field of NLP by achieving state-of-the-art results on various tasks.\n",
    "\n",
    "Let's take a look at how we can use the Transformers library to implement a BERT-based text classification model:\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "def encode_text(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_sentiment(text):\n",
    "    encoding = encode_text(text)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "    return 'Positive' if predicted_class == 1 else 'Negative'\n",
    "\n",
    "# Example usage\n",
    "review = \"This movie exceeded all my expectations. The plot was engaging, and the actors delivered stellar performances.\"\n",
    "sentiment = predict_sentiment(review)\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "\n",
    "This BERT-based approach can provide more accurate results, especially for more nuanced text classification tasks. However, it's important to note that using pre-trained models like BERT requires more computational resources and may not be suitable for all use cases.\n",
    "\n",
    "As you dive deeper into NLP, you'll encounter many other fascinating topics and techniques, such as:\n",
    "\n",
    "1. Named Entity Recognition (NER)\n",
    "2. Part-of-Speech (POS) Tagging\n",
    "3. Text Summarization\n",
    "4. Machine Translation\n",
    "5. Question Answering Systems\n",
    "6. Topic Modeling\n",
    "7. Sentiment Analysis at scale\n",
    "8. Chatbots and Conversational AI\n",
    "\n",
    "Each of these areas presents unique challenges and opportunities for innovation. As the field of NLP continues to evolve, new models and techniques are constantly being developed, pushing the boundaries of what's possible in natural language understanding and generation.\n",
    "\n",
    "In conclusion, NLP is a dynamic and rapidly growing field with numerous practical applications across various industries. From improving search engines and recommendation systems to enabling more natural human-computer interactions, NLP is transforming the way we interact with technology and process information. As you continue to explore this exciting field, remember that the key to success lies in not only understanding the underlying algorithms and models but also in developing a deep appreciation for the nuances and complexities of human language.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7342,
     "status": "ok",
     "timestamp": 1723068226615,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "ZoqLyR3BICdy",
    "outputId": "a5dd70a9-2148-48e8-924a-be653f20b442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 5.46\n",
      "\n",
      "Chunk 1 (Length: 427 characters):\n",
      "Natural Language Processing (NLP) is a fascinating field at the intersection of computer science, artificial intelligence, and linguistics. It focuses on the interaction between computers and human language, enabling machines to understand, interpret, and generate human-readable text. One of the fundamental tasks in NLP is text classification, which involves categorizing text documents into predefined classes or categories.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 2 (Length: 230 characters):\n",
      "Let's explore a simple example of text classification using Python and the popular machine learning library scikit-learn. We'll build a basic sentiment analysis model that can classify movie reviews as either positive or negative.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 3 (Length: 294 characters):\n",
      "First, we need to import the necessary libraries:\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.metrics import accuracy_score, classification_report\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 4 (Length: 790 characters):\n",
      "Next, let's define a small dataset of movie reviews with their corresponding sentiments:\n",
      "\n",
      "reviews = [\n",
      "    \"This movie was fantastic! I loved every minute of it.\",\n",
      "    \"Terrible acting and a boring plot. Waste of time.\",\n",
      "    \"Great performances and an engaging story. Highly recommended!\",\n",
      "    \"I fell asleep halfway through. Extremely dull.\",\n",
      "    \"A masterpiece of modern cinema. Absolutely brilliant!\",\n",
      "    \"Poorly written and predictable. Don't bother watching.\",\n",
      "    \"Visually stunning with excellent character development.\",\n",
      "    \"A complete disaster. One of the worst films I've ever seen.\",\n",
      "    \"Captivating from start to finish. A must-watch!\",\n",
      "    \"Disappointing and confusing. I expected much better.\"\n",
      "]\n",
      "\n",
      "sentiments = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 for positive, 0 for negative\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 5 (Length: 227 characters):\n",
      "Now that we have our data, we need to preprocess it. We'll use the CountVectorizer to convert our text data into numerical features:\n",
      "\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(reviews)\n",
      "y = np.array(sentiments)\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 6 (Length: 143 characters):\n",
      "Let's split our data into training and testing sets:\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 7 (Length: 93 characters):\n",
      "Now we can train our Naive Bayes classifier:\n",
      "\n",
      "clf = MultinomialNB()\n",
      "clf.fit(X_train, y_train)\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 8 (Length: 317 characters):\n",
      "With our model trained, we can make predictions on the test set and evaluate its performance:\n",
      "\n",
      "y_pred = clf.predict(X_test)\n",
      "accuracy = accuracy_score(y_test, y_pred)\n",
      "print(f\"Accuracy: {accuracy:.2f}\")\n",
      "print(\"\n",
      "Classification Report:\")\n",
      "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 9 (Length: 192 characters):\n",
      "This simple example demonstrates the basics of text classification using machine learning. However, in real-world applications, we often deal with much larger datasets and more complex models.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 10 (Length: 270 characters):\n",
      "One popular approach for more advanced NLP tasks is to use pre-trained language models like BERT (Bidirectional Encoder Representations from Transformers). BERT and its variants have revolutionized the field of NLP by achieving state-of-the-art results on various tasks.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 11 (Length: 197 characters):\n",
      "Let's take a look at how we can use the Transformers library to implement a BERT-based text classification model:\n",
      "\n",
      "from transformers import BertTokenizer, BertForSequenceClassification\n",
      "import torch\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 12 (Length: 210 characters):\n",
      "# Load pre-trained BERT model and tokenizer\n",
      "model_name = 'bert-base-uncased'\n",
      "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
      "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 13 (Length: 289 characters):\n",
      "# Tokenize and encode the input text\n",
      "def encode_text(text):\n",
      "    return tokenizer.encode_plus(\n",
      "        text,\n",
      "        add_special_tokens=True,\n",
      "        max_length=128,\n",
      "        padding='max_length',\n",
      "        truncation=True,\n",
      "        return_attention_mask=True,\n",
      "        return_tensors='pt'\n",
      "    )\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 14 (Length: 495 characters):\n",
      "# Function to make predictions\n",
      "def predict_sentiment(text):\n",
      "    encoding = encode_text(text)\n",
      "    input_ids = encoding['input_ids']\n",
      "    attention_mask = encoding['attention_mask']\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        outputs = model(input_ids, attention_mask=attention_mask)\n",
      "        logits = outputs.logits\n",
      "        probabilities = torch.softmax(logits, dim=1)\n",
      "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
      "    \n",
      "    return 'Positive' if predicted_class == 1 else 'Negative'\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 15 (Length: 208 characters):\n",
      "# Example usage\n",
      "review = \"This movie exceeded all my expectations. The plot was engaging, and the actors delivered stellar performances.\"\n",
      "sentiment = predict_sentiment(review)\n",
      "print(f\"Sentiment: {sentiment}\")\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 16 (Length: 267 characters):\n",
      "This BERT-based approach can provide more accurate results, especially for more nuanced text classification tasks. However, it's important to note that using pre-trained models like BERT requires more computational resources and may not be suitable for all use cases.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 17 (Length: 325 characters):\n",
      "As you dive deeper into NLP, you'll encounter many other fascinating topics and techniques, such as:\n",
      "\n",
      "1. Named Entity Recognition (NER)\n",
      "2. Part-of-Speech (POS) Tagging\n",
      "3. Text Summarization\n",
      "4. Machine Translation\n",
      "5. Question Answering Systems\n",
      "6. Topic Modeling\n",
      "7. Sentiment Analysis at scale\n",
      "8. Chatbots and Conversational AI\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 18 (Length: 271 characters):\n",
      "Each of these areas presents unique challenges and opportunities for innovation. As the field of NLP continues to evolve, new models and techniques are constantly being developed, pushing the boundaries of what's possible in natural language understanding and generation.\n",
      "--------------------------------------------------------------------------------\n",
      "Chunk 19 (Length: 557 characters):\n",
      "In conclusion, NLP is a dynamic and rapidly growing field with numerous practical applications across various industries. From improving search engines and recommendation systems to enabling more natural human-computer interactions, NLP is transforming the way we interact with technology and process information. As you continue to explore this exciting field, remember that the key to success lies in not only understanding the underlying algorithms and models but also in developing a deep appreciation for the nuances and complexities of human language.\n",
      "--------------------------------------------------------------------------------\n",
      "Total number of chunks: 19\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Fireworks LLM\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "\n",
    "# Define the chunking template\n",
    "template = \"\"\"\n",
    "Analyze the following content and insert '[CHUNK]' (in all caps) at appropriate points to split it into chunks of\n",
    "approximately {target_chunk_size} characters each, maintaining context and coherence.\n",
    "\n",
    "Follow these guidelines:\n",
    "1. Keep related information together.\n",
    "2. Keep function definitions and code blocks intact as possible.\n",
    "3. Don't split in the middle of a sentence or a line of code.\n",
    "4. Try to split at logical breaks in the content.\n",
    "5. For prose, prefer splitting at paragraph boundaries.\n",
    "6. For code, prefer splitting at function or class boundaries.\n",
    "5. Do not modify the original content other than inserting '[CHUNK]'.\n",
    "6. Do not add any additional text or formatting.\n",
    "7. The first chunk should start immediately, without any preceding text.\n",
    "8. Most importantly you should keep the chunk size around {target_chunk_size} characters.\n",
    "\n",
    "Content length: {content_length} characters\n",
    "\n",
    "Content:\n",
    "{content}\n",
    "\n",
    "Return only the chunked content, with no additional text before or after:\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"target_chunk_size\", \"content_length\", \"content\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# The content variable remains the same as in your original code\n",
    "\n",
    "# Function to chunk the content\n",
    "def chunk_content(content, target_chunk_size):\n",
    "    content_length = len(content)\n",
    "    formatted_prompt = prompt.format(\n",
    "        target_chunk_size=target_chunk_size,\n",
    "        content_length=content_length,\n",
    "        content=content\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    return response.content.strip()\n",
    "\n",
    "# Function to split the content into chunks\n",
    "def split_into_chunks(chunked_content):\n",
    "    chunks = chunked_content.split('[CHUNK]')\n",
    "    return [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "\n",
    "# Chunk the content\n",
    "target_chunk_size =200\n",
    "start_time = time.time()\n",
    "chunked_content = chunk_content(content, target_chunk_size)\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total time: {total_time:.2f}\\n\")\n",
    "\n",
    "# Split the content into chunks\n",
    "chunks = split_into_chunks(chunked_content)\n",
    "\n",
    "# Print the chunks\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i} (Length: {len(chunk)} characters):\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Print the total number of chunks\n",
    "print(f\"Total number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1723068502059,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "-syXYxo2tf_Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfEdmzk86Mmn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdthyEk06MjE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv6KlyL-6MgB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "executionInfo": {
     "elapsed": 2367,
     "status": "ok",
     "timestamp": 1723070391093,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "eUH5C-mp6Mdg",
    "outputId": "fac1804b-a82f-4fff-e68a-12cdd75e3ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://489a4dc38b2caff207.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://489a4dc38b2caff207.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Fireworks LLM\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "\n",
    "# Define the improved chunking template\n",
    "template = \"\"\"\n",
    "Analyze the following content and insert '[CHUNK]' (in all caps) at appropriate points to split it into chunks of\n",
    "EXACTLY {target_chunk_size} characters each (with a maximum deviation of 10%), maintaining context and coherence as much as possible within this strict size constraint.\n",
    "\n",
    "Follow these guidelines in order of priority:\n",
    "1. Maintain chunk size: Each chunk MUST be between {min_chunk_size} and {max_chunk_size} characters. This is the most important rule.\n",
    "2. Keep function definitions and code blocks intact when possible, but split them if necessary to meet the chunk size requirement.\n",
    "3. For prose, prefer splitting at paragraph or sentence boundaries.\n",
    "4. For code, prefer splitting at function, class, or logical block boundaries.\n",
    "5. If necessary to meet the size requirement, split in the middle of a paragraph or code block.\n",
    "6. Do not modify the original content other than inserting '[CHUNK]'.\n",
    "7. Do not add any additional text or formatting.\n",
    "8. The first chunk should start immediately, without any preceding text.\n",
    "\n",
    "Remember: Maintaining the correct chunk size is more important than keeping related information together. If you need to split in a less ideal place to meet the size requirement, do so.\n",
    "\n",
    "Content length: {content_length} characters\n",
    "Target chunk size: {target_chunk_size} characters\n",
    "Minimum chunk size: {min_chunk_size} characters\n",
    "Maximum chunk size: {max_chunk_size} characters\n",
    "\n",
    "Content:\n",
    "{content}\n",
    "\n",
    "Return only the chunked content, with no additional text before or after:\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"target_chunk_size\", \"min_chunk_size\", \"max_chunk_size\", \"content_length\", \"content\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Function to chunk the content\n",
    "def chunk_content(content, target_chunk_size):\n",
    "    content_length = len(content)\n",
    "    min_chunk_size = int(target_chunk_size * 0.5)\n",
    "    max_chunk_size = int(target_chunk_size * 1.5)\n",
    "    formatted_prompt = prompt.format(\n",
    "        target_chunk_size=target_chunk_size,\n",
    "        min_chunk_size=min_chunk_size,\n",
    "        max_chunk_size=max_chunk_size,\n",
    "        content_length=content_length,\n",
    "        content=content\n",
    "    )\n",
    "\n",
    "    llm_start_time = time.time()\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    llm_time = time.time() - llm_start_time\n",
    "    return response.content.strip(), llm_time\n",
    "\n",
    "# The rest of the code remains the same as in the previous version\n",
    "\n",
    "# Function to split the content into chunks\n",
    "def split_into_chunks(chunked_content):\n",
    "    chunks = chunked_content.split('[CHUNK]')\n",
    "    return [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "\n",
    "\n",
    "# Gradio interface function\n",
    "def process_text(input_text, chunk_size):\n",
    "    start_time = time.time()\n",
    "    input_length = len(input_text)\n",
    "    chunked_content, llm_time = chunk_content(input_text, chunk_size)\n",
    "    chunks = split_into_chunks(chunked_content)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    output = f\"Input length: {input_length} characters\\n\"\n",
    "    output += f\"LLM processing time: {llm_time:.2f} seconds\\n\"\n",
    "    output += f\"Total processing time: {total_time:.2f} seconds\\n\\n\"\n",
    "\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        output += f\"Chunk {i} (Length: {len(chunk)} characters):\\n{chunk}\\n{'=' * 80}\\n\\n\"\n",
    "\n",
    "    output += f\"Total number of chunks: {len(chunks)}\"\n",
    "    return output\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_text,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Input Text\", value=content, lines=10),\n",
    "        gr.Slider(minimum=100, maximum=1000, value=200, step=50, label=\"Target Chunk Size\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Chunked Output\", lines=20),\n",
    "    title=\"Text Chunker\",\n",
    "    description=\"Enter your text or use the default content, set the target chunk size, and see the resulting chunks along with input length and processing times.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jyd31ep-6Sp1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBONk5wWBGwkfvDjJ/CyWS",
   "provenance": [
    {
     "file_id": "1KiInGak7ryJqRYp--_M_N8YSqfRGfC9X",
     "timestamp": 1723047089549
    },
    {
     "file_id": "1oULgkdS0WEhL6jvmf3wO4yDIFmOPlfAc",
     "timestamp": 1722978253496
    },
    {
     "file_id": "1OKIuQQXhdZWwqzRDyGsSI0u-vXB9SaZG",
     "timestamp": 1722962538291
    },
    {
     "file_id": "1-pzIxmM0OfVYWvgYNLox_6q6jFxfdAMA",
     "timestamp": 1722950692303
    },
    {
     "file_id": "1pnObW74av-9UCq0S803c5gnbJ6kjmXXV",
     "timestamp": 1722896710025
    },
    {
     "file_id": "1gLEQ7CD3Rqv_H_QqsN7UuyYG29hXwRHN",
     "timestamp": 1722885404765
    },
    {
     "file_id": "18Y0C-1fd6eA0XFM0URvy7tIc5zPrFNS9",
     "timestamp": 1722867557165
    },
    {
     "file_id": "1Cha-ZBnqtRK2LGrWgI2t-cDqhcfrTsPq",
     "timestamp": 1722865370179
    },
    {
     "file_id": "125wkisw8alSzlySbwhQ44VYiuY0CJHf2",
     "timestamp": 1722636201777
    },
    {
     "file_id": "16Ygm33u7K9_YHvuOIHxJYllSJqqAk0SH",
     "timestamp": 1722635210899
    },
    {
     "file_id": "1JZGn8IigITIj7-Eq4OF2zRxGc3v_qrYo",
     "timestamp": 1722629409325
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
