{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RtawaERNORl"
   },
   "outputs": [],
   "source": [
    "!pip install langchain langchain_fireworks langchain_community beautifulsoup4 google-search-results chromadb langchainhub sentence-transformers langchain-chroma gradio aiolimiter lxml faiss-cpu flashrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 21479,
     "status": "ok",
     "timestamp": 1723140594670,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "s5FdjAFiNKGx"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_fireworks import FireworksEmbeddings, ChatFireworks\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import sys\n",
    "import gradio as gr\n",
    "import asyncio\n",
    "from typing import List, Tuple, Any\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import numpy as np\n",
    "from functools import lru_cache\n",
    "import faiss\n",
    "import httpx\n",
    "from urllib.parse import urlparse\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from flashrank import Ranker, RerankRequest\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1725,
     "status": "ok",
     "timestamp": 1723140596390,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "9aK_SqnCNZEW",
    "outputId": "26ded033-38c7-4135-e07e-3e90161d837c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up API clients\n",
    "os.environ['FIREWORKS_API_KEY'] = 'API_KEY'\n",
    "os.environ[\"SERPER_API_KEY\"] = 'API_KEY'\n",
    "\n",
    "\n",
    "# Download NLTK data for sentence tokenization\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "executionInfo": {
     "elapsed": 897686,
     "status": "ok",
     "timestamp": 1723141526402,
     "user": {
      "displayName": "Maziar Sargordi",
      "userId": "18222397832012180721"
     },
     "user_tz": 240
    },
    "id": "VDeUz4riDNUn",
    "outputId": "de07d88e-2f9b-41a8-c1fd-f87f47f3f42f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ms-marco-TinyBERT-L-2-v2.zip: 100%|██████████| 3.26M/3.26M [00:00<00:00, 31.2MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "Running on public URL: https://dc4880b2e8567a4691.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://dc4880b2e8567a4691.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-304ab3ec0494>\", line 164, in process_query\n",
      "    all_texts = await asyncio.gather(*[search_and_scrape(eq, num_urls) for eq in expanded_queries])\n",
      "  File \"<ipython-input-4-304ab3ec0494>\", line 28, in search_and_scrape\n",
      "    search_results = search.results(query)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_community/utilities/google_serper.py\", line 62, in results\n",
      "    return self._google_serper_api_results(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/langchain_community/utilities/google_serper.py\", line 164, in _google_serper_api_results\n",
      "    response.raise_for_status()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://google.serper.dev/search?q=How+canI+take+are+of+my+dog%3F&gl=us&hl=en&num=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://dc4880b2e8567a4691.gradio.live\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "search = GoogleSerperAPIWrapper(k=3)\n",
    "embeddings = FireworksEmbeddings(model=\"nomic-ai/nomic-embed-text-v1.5\")\n",
    "llm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_8b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-8b-instruct\", temperature=0)\n",
    "llm_70b = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\", temperature=0)\n",
    "\n",
    "# Initialize Flashrank Rankers\n",
    "ranker_nano = Ranker()\n",
    "\n",
    "async def scrape_webpage(client, url):\n",
    "    try:\n",
    "        response = await client.get(url, timeout=3.0)\n",
    "        response.raise_for_status()\n",
    "        text = response.text\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        content = ' '.join(soup.stripped_strings)\n",
    "        return content[:5000], len(content[:5000])\n",
    "    except (httpx.RequestError, httpx.TimeoutException) as exc:\n",
    "        print(f\"An error occurred while requesting {url}: {exc}\")\n",
    "    except httpx.HTTPStatusError as exc:\n",
    "        print(f\"Error response {exc.response.status_code} while requesting {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "    return \"\", 0\n",
    "\n",
    "async def search_and_scrape(query, num_urls):\n",
    "    search_results = search.results(query)\n",
    "    scraped_urls = set()\n",
    "    full_texts = []\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=httpx.Timeout(10.0, connect=3.0)) as client:\n",
    "        tasks = []\n",
    "        if 'organic' in search_results:\n",
    "            for result in search_results['organic']:\n",
    "                url = result.get('link')\n",
    "                domain = urlparse(url).netloc if url else None\n",
    "                if url and domain not in scraped_urls and len(tasks) < num_urls:\n",
    "                    tasks.append(scrape_webpage(client, url))\n",
    "                    scraped_urls.add(domain)\n",
    "\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        for result in results:\n",
    "            if isinstance(result, tuple) and result[1] > 0:\n",
    "                full_texts.append(result[0])\n",
    "\n",
    "    return \" \".join(full_texts)\n",
    "\n",
    "def query_expansion(query, num_expansions):\n",
    "    expansion_prompt = f\"\"\"\n",
    "    Given the following search query, generate {num_expansions} additional related queries that could help find more comprehensive information on the topic. The queries should be different from each other and explore various aspects of the main query. Provide only the additional queries, numbered 1-{num_expansions}.\n",
    "\n",
    "    Main query: {query}\n",
    "\n",
    "    Additional queries:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(expansion_prompt)\n",
    "    response_text = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    expanded_queries = [query]\n",
    "    for line in response_text.split('\\n'):\n",
    "        if line.strip() and line[0].isdigit():\n",
    "            expanded_queries.append(line.split('. ', 1)[1].strip())\n",
    "\n",
    "    return expanded_queries[:num_expansions + 1]\n",
    "\n",
    "def create_sentence_windows(text, window_size=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    windows = []\n",
    "    for i in range(len(sentences)):\n",
    "        window = \" \".join(sentences[max(0, i-window_size):min(len(sentences), i+window_size+1)])\n",
    "        windows.append(window)\n",
    "    return windows\n",
    "\n",
    "def generate_hypothetical_document(query):\n",
    "    hyde_prompt = f\"\"\"\n",
    "    Given the search query below, generate a hypothetical document that would be a perfect match for this query. The document should be concise, containing only 3 sentences of relevant information that directly addresses the query.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Document (3 sentences):\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(hyde_prompt)\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def batch_rerank(ranker, query, documents, batch_size=32):\n",
    "    all_reranked = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        passages = [{\"id\": j, \"text\": doc.page_content} for j, doc in enumerate(batch, start=i)]\n",
    "        rerank_request = RerankRequest(query=query, passages=passages)\n",
    "        reranked_batch = ranker.rerank(rerank_request)\n",
    "        all_reranked.extend(reranked_batch)\n",
    "    return all_reranked\n",
    "\n",
    "def get_hyde_retriever(vectorstores, hyde_embedding, num_docs_to_rerank, num_docs_to_use, rerank_option):\n",
    "    def retriever(query):\n",
    "        all_docs = []\n",
    "        for vectorstore in vectorstores:\n",
    "            docs = vectorstore.similarity_search_by_vector(hyde_embedding, k=num_docs_to_rerank)\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        unique_docs = []\n",
    "        seen_content = set()\n",
    "        for doc in all_docs:\n",
    "            content = doc.page_content\n",
    "            if content not in seen_content:\n",
    "                unique_docs.append(Document(page_content=content))\n",
    "                seen_content.add(content)\n",
    "\n",
    "        if rerank_option != \"No Rerank\":\n",
    "            # Choose the appropriate ranker based on the rerank_option\n",
    "            if rerank_option == \"Nano\":\n",
    "                ranker = ranker_nano\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid rerank option: {rerank_option}\")\n",
    "\n",
    "            # Use batch reranking\n",
    "            reranked_results = batch_rerank(ranker, query, unique_docs)\n",
    "\n",
    "            # Sort the results by score in descending order\n",
    "            reranked_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "            # Create new Document objects with reranked content, using only the top num_docs_to_use\n",
    "            reranked_docs = [Document(page_content=result[\"text\"]) for result in reranked_results[:num_docs_to_use]]\n",
    "            return reranked_docs\n",
    "        else:\n",
    "            # If not using reranking, just return the top num_docs_to_use documents\n",
    "            return unique_docs[:num_docs_to_use]\n",
    "    return retriever\n",
    "\n",
    "def batch_embed_documents(documents, batch_size=128):\n",
    "    batched_embeddings = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        texts = [doc.page_content for doc in batch]\n",
    "        embeddings_batch = embeddings.embed_documents(texts)\n",
    "        batched_embeddings.extend(embeddings_batch)\n",
    "    return batched_embeddings\n",
    "\n",
    "async def process_query(query, num_expansions, num_urls, num_docs_to_rerank, num_docs_to_use, rerank_option, use_70b_model):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        hyde_start = time.time()\n",
    "        hypothetical_doc = generate_hypothetical_document(query)\n",
    "        hyde_time = time.time() - hyde_start\n",
    "        print(f\"hypothetical_doc length: {len(hypothetical_doc)}\")\n",
    "        print(f\"-----HyDE generation time: {hyde_time:.2f} seconds\")\n",
    "\n",
    "        embed_start = time.time()\n",
    "        hyde_embedding = embeddings.embed_query(hypothetical_doc)\n",
    "        embed_time = time.time() - embed_start\n",
    "        print(f\"-----Embedding time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        ext_start = time.time()\n",
    "        expanded_queries = query_expansion(query, num_expansions)\n",
    "        ext_time = time.time() - embed_start\n",
    "        print(f\"-----Query expansion time: {embed_time:.2f} seconds\")\n",
    "\n",
    "        scrape_start = time.time()\n",
    "        all_texts = await asyncio.gather(*[search_and_scrape(eq, num_urls) for eq in expanded_queries])\n",
    "        scrape_time = time.time() - scrape_start\n",
    "        print(f\"-----Web scraping time: {scrape_time:.2f} seconds\")\n",
    "\n",
    "        combined_text = \" \".join(all_texts)\n",
    "        print(f\"Combined text length: {len(combined_text)} characters\")\n",
    "\n",
    "        sentence_windows = create_sentence_windows(combined_text)\n",
    "        print(f\"Number of sentence windows: {len(sentence_windows)}\")\n",
    "\n",
    "        index_documents = [Document(page_content=window) for window in sentence_windows]\n",
    "\n",
    "        vectorstore_start = time.time()\n",
    "        vectorstores = []\n",
    "        for i in range(0, len(index_documents), 256):\n",
    "            batch = index_documents[i:i + 256]\n",
    "\n",
    "            # Use batch embedding here\n",
    "            batch_embeddings = batch_embed_documents(batch)\n",
    "\n",
    "            texts = [doc.page_content for doc in batch]\n",
    "\n",
    "            vectorstore = FAISS.from_embeddings(\n",
    "                embedding=embeddings,\n",
    "                text_embeddings=list(zip(texts, batch_embeddings))\n",
    "            )\n",
    "            vectorstores.append(vectorstore)\n",
    "\n",
    "        vectorstore_time = time.time() - vectorstore_start\n",
    "        print(f\"-----Vectorstore creation time: {vectorstore_time:.2f} seconds\")\n",
    "\n",
    "        retrieval_start = time.time()\n",
    "        retriever = get_hyde_retriever(vectorstores, hyde_embedding, num_docs_to_rerank, num_docs_to_use, rerank_option)\n",
    "        retrieved_docs = retriever(query)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        print(f\"-----Retrieval{' and reranking' if rerank_option != 'No Rerank' else ''} time: {retrieval_time:.2f} seconds\")\n",
    "\n",
    "        print(f\"Number of retrieved{' and reranked' if rerank_option != 'No Rerank' else ''} documents: {len(retrieved_docs)}\")\n",
    "\n",
    "        context_docs = [doc.page_content for doc in retrieved_docs]\n",
    "        context = \"\\n\\n\".join(context_docs)\n",
    "\n",
    "        total_processing_time = hyde_time + embed_time + scrape_time + vectorstore_time + retrieval_time\n",
    "        print(f\"-----Total processing time before answer generation: {total_processing_time:.2f} seconds\")\n",
    "\n",
    "        answer_start = time.time()\n",
    "        prompt_template = \"\"\"\n",
    "        Use the following context to answer the question. Before answering the question generate a reasoning step. then answer.\n",
    "        If you cannot answer based on the context, say \"I don't have enough information to answer that question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        # Choose the model based on the use_70b_model parameter\n",
    "        chosen_llm = llm_70b if use_70b_model else llm_8b\n",
    "\n",
    "        rag_chain = prompt | chosen_llm | StrOutputParser()\n",
    "        answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "        answer_time = time.time() - answer_start\n",
    "        print(f\"-----Answer generation time: {answer_time:.2f} seconds\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*120)\n",
    "        print(\"Final Answer:\\n\", answer)\n",
    "        print(\"-\"*120)\n",
    "\n",
    "        return answer, context_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"I'm sorry, but I encountered an error while processing your query. Please try again.\", []\n",
    "\n",
    "def gradio_interface(query, num_expansions, num_urls, num_docs_to_rerank, num_docs_to_use, rerank_option, use_70b_model):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "    answer, context_docs = asyncio.run(process_query(query, num_expansions, num_urls, num_docs_to_rerank, num_docs_to_use, rerank_option, use_70b_model))\n",
    "\n",
    "    sys.stdout = old_stdout\n",
    "    captured_output = buffer.getvalue()\n",
    "\n",
    "    # Process the context to show only 150 characters per document\n",
    "    truncated_docs = [f\"Document {i+1}: {doc[:150]}...\" for i, doc in enumerate(context_docs)]\n",
    "    truncated_context = \"\\n\\n\".join(truncated_docs)\n",
    "\n",
    "    # Add the truncated context to the captured output\n",
    "    captured_output += f\"\\n\\nContext used for answer generation (first 150 characters of each document, {len(context_docs)} documents in total):\\n\" + truncated_context\n",
    "\n",
    "    return captured_output\n",
    "\n",
    "# Create Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter your query\"),\n",
    "        gr.Slider(minimum=0, maximum=3, value=1, step=1, label=\"Number of query expansions\"),\n",
    "        gr.Slider(minimum=1, maximum=10, value=3, step=1, label=\"Number of URLs to scrape per extended query\"),\n",
    "        gr.Slider(minimum=20, maximum=100, value=80, step=1, label=\"Number of documents to rerank\"),\n",
    "        gr.Slider(minimum=10, maximum=100, value=50, step=1, label=\"Number of reranked documents to use\"),\n",
    "        gr.Radio([\"No Rerank\", \"Nano\"], label=\"Reranking Option\", value=\"Nano\"),\n",
    "        gr.Radio([\"LLaMA3.1 8B\", \"LLaMA3.1 70B\"], label=\"Question Answering Option\", value=\"LLaMA3.1 8B\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Advanced RAG Query Processing with Flashrank\",\n",
    "    description=\"Enter a query and adjust parameters to get a detailed answer based on web search and document analysis. Choose from different reranking options.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQ-CXE4vVaVE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPoke32c+Ednzo2BABZx0LJ",
   "provenance": [
    {
     "file_id": "1oULgkdS0WEhL6jvmf3wO4yDIFmOPlfAc",
     "timestamp": 1722978253496
    },
    {
     "file_id": "1OKIuQQXhdZWwqzRDyGsSI0u-vXB9SaZG",
     "timestamp": 1722962538291
    },
    {
     "file_id": "1-pzIxmM0OfVYWvgYNLox_6q6jFxfdAMA",
     "timestamp": 1722950692303
    },
    {
     "file_id": "1pnObW74av-9UCq0S803c5gnbJ6kjmXXV",
     "timestamp": 1722896710025
    },
    {
     "file_id": "1gLEQ7CD3Rqv_H_QqsN7UuyYG29hXwRHN",
     "timestamp": 1722885404765
    },
    {
     "file_id": "18Y0C-1fd6eA0XFM0URvy7tIc5zPrFNS9",
     "timestamp": 1722867557165
    },
    {
     "file_id": "1Cha-ZBnqtRK2LGrWgI2t-cDqhcfrTsPq",
     "timestamp": 1722865370179
    },
    {
     "file_id": "125wkisw8alSzlySbwhQ44VYiuY0CJHf2",
     "timestamp": 1722636201777
    },
    {
     "file_id": "16Ygm33u7K9_YHvuOIHxJYllSJqqAk0SH",
     "timestamp": 1722635210899
    },
    {
     "file_id": "1JZGn8IigITIj7-Eq4OF2zRxGc3v_qrYo",
     "timestamp": 1722629409325
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
